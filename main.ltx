%&main_preamble
\endofdump

\begin{document}

\maketitle

\begin{abstract}
    \noindent
    The aim of my PhD is to develop an improved framework for Profile Guided
    Optimization (PGO) by better understanding what kind, and how many profiles
    are needed to achieve performance greater than best known existing PGO tool
    BOLT. In this preliminary report, I have presented an analysis of Facebook’s
    BOLT, a post-link binary optimizer which is now part of the LLVM project.
    I have conducted an experimental investigation to discern
    how the use of varied types of profiling information influences the
    performance of the optimized binary. The performance metrics gathered in
    this study are derived from a relatively small test set due to resource
    constraints. Expanding the scope of the test set to obtain more
    comprehensive performance data forms part of the future work of this
    research.
\end{abstract}

\section{Introduction}

While Profile Guided Optimization (PGO) has been an active area of research for
some time, it remains a field with space for further exploration. This report
delves into the effects of varying profiling methods on the performance of
newly created binaries. Additionally, it investigates the potential for using a
hybrid \laurie{later in the report you'll need to define `hybrid' more specifically} 
of these profiling methods to develop an enhanced PGO framework.

I start with doing background research on BOLT, the best known existing PGO
tool.  Despite its extensive use by large corporations for complex
applications, even relative simple experiments uncovers surprising interactions
between profiling and performance.

My PhD ultimately aims to determine which profiling methods would prove most
effective and feasible in terms of enhancing PGO tools performance further,
starting with BOLT. As a starting point, I conducted experiments with existing
profiling techniques on Clang binary and compared them to baseline Clang binary
\laurie{to ... /}. Future work will extend the scope of these experiments to
include a broader, more diverse range of files to collect and benchmark the
profiling data.

\subsection*{Methodology}
BOLT is a post-link static binary optimizer that enhances the code layout by
utilizing sample-based profiling data at a binary level. This study investigates
how much performance difference can be achieved by providing different
profiling sampling data into BOLT to optimize the binary. This report discusses
optimizing source-build Clang binary (CLANG 16) using BOLT by feeding three
type of sample profiling data into BOLT. All the experiments are conducted
using \textbf{Intel(R) Xeon(R) CPU E3-1270 v5 @ 3.60GHz}. I used three types of
sample profiles (called as“mode” throughout the report), No Last Branch Record
(NO LBR) which is the default \texttt{perf record} collected sample i.e without
any flags, Last Branch Record (LBR), and Intel-PT (intel-pt or PT) these are
explained in background section. Compared to baseline binary, Clang built with
NO LBR profile (NO LBR mode Clang) performs \% better, LBR mode Clang performs
\% better and PT mode Clang performs \% on single compilation run. Later we
evaluate performance different version of Clang running over 100 times, taking
mean values along with 99\% confidence interval. The baseline binary has been
trained on four c programs (training set), and baseline along with optimized
binaries were benchmarked on the same set of files (testing set).

\section{Background}
Modern compilers have integrated a lot of compiler optimization techniques,
such as using basic block frequencies and branch target information to perform
function inlining.  But they don’t have  runtime information and can’t
differentiate hot code path (executed a lot) from cold code paths (executed
rarely) . This often leads compilers to optimize cold code paths at the
expense of hot ones. Sampling data can help us collect hot code path (at least
most of the times) since show up more often. 

Profile-Guided Optimization (PGO)\footnote{Sometimes known as Feedback-Driven Optimization (FDO): both
terms refer to the same concept.}, optimises a program's runtime
performance with the help of profiling information collected using static
program analysis \laurie{static program analysis?}. PGO helps in performing function splitting which leads to optimize
code locality. Traditional methods such as instrumentation-based profilers have
significant overhead of memory and performance for collecting profiling
information, additionally they might not end up capturing accurate information.
PGO is easily adoptable in production environment in form of continuous feeding
dynamic profiling data to rebuild the binary. This report focuses experiments
using Facebook’s BOLT (now known as BOLT), a post-link FDO tool which uses
sample based profiling information. Although, BOLT supports accepts any profile
which feeds it desired information for rebuilding binary we have used linux’s
tool-chain perf. Not just BOLT provides tool to convert perf data into BOLT
friendly data but perf also let us capture different type of profiling
information very easily.

\subsection{Profiling Techniques}
\subsection*{Perf}
Perf, also commonly known as perf\_event, is a lightweight linux tool-chain that
offers an abstraction over hardware-specific capabilities. It is primarily used for
performance measurements by instrumenting events, these events are unified
interference from different parts of the kernel. Perf uses performance counters
to capture events per-thread, per-cpu and per-cpu or system-wide and collect
sample on top of these events.  

Measurable events includes:

\begin{itemize}
    \item PMU Hardware events: Performance Monitoring Unit or PMU contains
        measurable events such as number of cycles, instruction retired, L1
        cache and so on. They vary depending on processor and model type.
    
    \item Software events: low level events such as page-faults, cpu-clock and
        so, base based on kernel counters.
    
    \item User Statically Defined Tracing: static trace points for user level program.
    
    \item Kernel Tracepoint Events: hardcoded kernel-level instrumentation point.
    
    \item Dynamic Tracing: for dynamically instrumenting software by creating
        event(s) in any location.
    
    \item Timed Profiling: commonly used for capturing snapshot  by using
        custom timer interrupt events and invoked using \texttt{perf record
        -FHz}
\end{itemize}

The perf\_event tool is in the linux-tools -common package and can either be
installed via \texttt{apt install linux-tools-generic} or be built from source. 

perf works can instrument in following modes: a) Counting, using
\texttt{perf stat} which keeps count of running process for supported events,
unless specified \texttt{perf stat} runs in per-thread mode. Finally all
occurrence of events are aggregated and summary is presented on the standard
output b) Sampling mode, using \texttt{perf record/report/annotate}. Event data
is written to the kernel and then read at an asynchronous rate by
\texttt{perf record} to write to the perf.data file. This file is then analyzed
by the perf \texttt{report} or \texttt{script} commands.

Another way to instrument is by using BPF program on events which can execute
custom user-defined programs in kernel space, and perform filters and summaries
of the data. But we won’t be covering that in our experiments.

\subsubsection*{How is a sample event recorded?}

An event sample is recorded when sampling counter overflows, which means
counters go from $2^{64}$ to 0. Since 64-bit hardware counters aren’t
implemented in any PMU, perf emulated it in software. Hence, when using perf on
32-bit system perf silently truncates the period. On every counter overflow,
the sample contains record of program execution with additional information
which depends on what user specified for and an instruction pointer
irrespective of event type.

\subsubsection*{Sample period and rate}

By default perf records sample at \textbf{both} user and kernel levels
using cycles events with average rate of ${1000}$ samples/sec or ${4000}$ events / sec
of thread execution. With Sampling \texttt{perf record} asks PMU to count some
events, for example cpu-cycles and generate an overflow interrupt after every
$nth$ event (e.g a million). On every interrupt perf\_event records such as :
PID/TID (Process/Thread ID), timestamp, command being executed, and instruction
pointer to the ring buffer and reset the counter to a new value.

There is a limit to maximum frequency that can be set but we can adjust both
frequency of collection samples and period i.e collecting sample every $nth$
occurrence of an event by passing \texttt{-F} and \texttt{-c} flag respectively
to \texttt{perf record}. Once the ring buffer is filled, perf will dump the
data to \texttt{perf.data} file which can vary in size depending upon frequency
and period set.

\subsection*{LBR}
By default perf records only records basic performance counter and cannot
follow indirect function call where target is only known at the runtime, this
leads to forming incomplete control flow. Intel has additional support in their
CPUs called Last Branch Record (LBR), a hardware register to record additional 
information about branch instruction such as “from” and “to” address of each
branch with some additional metadata. The retired branches are captured in
rotating ring buffer.The stack depth (8, 16, or 32 frames) varies according to
the Intel CPU. Using LBR can also help in building better control flow graph
with hot code paths, the profile generated can be used for finding basic block
frequency or profile-guided optimization.

\subsection*{PT}
PT is a hardware based tracing technique integrated into Intel’s CPU
hardware. It traces branch execution which theoretically can be used to build
control flow for all executed code.

Since PT captures huge amount of data standard processing such as PMU is
not possible and thus it uses additional auxiliary buffer(AUX) which is
associated with perf’s ring buffer.

We can use PT with perf’s record command as: 

\texttt{perf record -e intel-pt//u ls}

\subsubsection*{Buffer Handling}
PT produces hundreds of megabytes of data per second per CPU and
sometimes it encounters two errors: \textbf{trace data-loss} (generating data at
faster rate than it can be recorded to file) and \textbf{overflow packet}
(unable to record data to memory).

Passing larger auxtrace \textbf{mmap} size seems to help in resolving the trace data
loss, which can be done by passing \texttt{-m} flag.So, passing \texttt{-m,
64M}  means we can set trace buffer size to 64MiB  per CPU. We would need to be
careful with choosing auxtrace mmap size. For instance, if we have 8 CPUs we
are essentially setting total 512MiB of trace buffer size.

To resolve overflow packets error in addition of setting AUX mmap size,
re-running the workload helps. This might capture slightly less data(a couple
of MiB) but you should be able to either see no or fewer “Instruction errors” in captured
data.

BOLT fails to convert data into in a acceptable format if it encounters
trace errors in \texttt{.data} file,  therefore it’s essential to remove the
errors by following above workaround or using some other method.

\subsection*{BOLT}
BOLT is a static post link binary optimizer that is built on top of LLVM
compiler infrastructure.

It uses sample based profiling data and relocations mode (by passing
\texttt{-emit-relocs} flag) to recompile the binary, this helps BOLT reorder
function blocks (or basic blocks) and to split hot/cold code blocks thus
improving code locality. For function discover, BOLT relies on ELF symbol
table and function frame information (to get function boundaries).

To recompile binary BOLT requires a specific format data. The fields that
are required are thread ID (TID) or process ID (PID), instruction pointer (IP),
and either event or branch stack information (brstack). So, it is possible to
collect profile from any method as long as it can be fed to BOLT’s profile
conversion tool. However, for easier workflow using perf is recommended since
a) it’s easy to record different mode of samples b) BOLT has a utility
tool to convert perf data directly into BOLT friendly data.

\section {Optimizing Clang with BOLT}
We decided to replicate tutorial to optimize Clang binary provided as an example
in the BOLT's github repository and followed it with some additional experiments and benchmarking. 

\textbf{Experiment Setup}

For benchmarking, we used  {Intel(R) Xeon(R) CPU E3-1270 v5 @ 3.60GHz, on
Debian OS and baseline CLANG 16 (without any optimization). The files were
executed hundred times using multitime, which is extension of linux times to
run file multiple times and present mean, standard deviation, mins, medians,
and maxs values on standard output. We the used the sample mean and standard
deviation values and later converted standard deviation into 99\% confidence
interval to plot the graphs in this report. 

\textbf{NO LBR vs LBR vs PT}

In this section, we experiment collecting profiles in three modes namely NO
LBR, LBR and PT and using the information to recompile Clang binary. 

Let’s do a small experiment to compare performance of baseline Clang with Clang
build using NO LBR mode profile and LBR mode profiling data. We use two
distinct sets: a training set to generate the profiles for constructing the new
version of Clang, and a testing set to procure then sample mean wall-clock time
over hundred runs and the 99\% confidence interval.

\begin{table}[htbp]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        Testing file & Baseline & NO LBR & LBR \\
        \midrule
        format.c & & 0.735 ± 0.0005 & 0.723 ± 0.0005 \\
        tty.c & & 0.449 ± 0.0005 & 0.442 ± 0.0005 \\
        window-copy.c & & 0.715 ± 0.0008 & 0.704 ± 0.0008  \\
        extsmaild.c & & 0.209 ± 0.0005 & 0.205 ± 0.0005  \\
        \bottomrule
    \end{tabular}
    \caption{The first column presents the testing set files, while the
    succeeding columns represent Clang, each built with profiles collected using the training
    set files and consequently named with the corresponding filename extension.}
\end{table}

By above experiment we know that Clang build with LBR mode profile is
\texttt{2.89 \iti{note to self: change number here}\%} faster than one which uses NO LBR mode profile. Thus, we wanted
PT data to run same optimization which BOLT uses when we feed LBR mode
profiling data into it. However, BOLT doesn’t support processing of PT
data and just deals with NO LBR and LBR type data format. Making some
adjustment to BOLT’s data aggregator source code let us use PT collected
profile information work.

Since BOLT cares only three fields as: process id (pid)/ thread id (tid),
instruction pointer(ip), and branch stack (brstack) input to process binary in
LBR mode, extracting these should be enough. The one trick thing to solve was
getting branch flags which indicates misprediction, this flag is not available
in PT mode thus we couldn’t extract brstack field from perf data. But with
luck, perf team added feature to lets us add configuration option to set
mispred flag on all branches by changing \textit{\~/.perfconfig\iti{note to self: correct the \~}}.

Simply adding following lines to \textit{\~/.perfconfig} lets us get brstack
information out of sampled data collected with Intel PT

\begin{lstlisting}[language=bash]
[intel-pt]
mispred-all = on
\end{lstlisting}

After this adjusting data aggregator source code to use
\texttt{—itrace=i100usle} option in conjunction with perf scripts will lets us
extract brstack field in PT mode to feed into BOLT. We have now a table
comparing baseline, NO LBR mode, LBR mode and PT mode Clang.

\begin{table}[htbp]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        Testing file & Baseline & NO LBR & LBR & Intel PT \\
        \midrule
        format.c & & 0.735 ± 0.0005 & 0.723 ± 0.0005 & 0.701 ± 0.0008 \\
        tty.c & & 0.449 ± 0.0005 & 0.442 ± 0.0005 & 0.428 ± 0.0005 \\
        window-copy.c & & 0.715 ± 0.0008 & 0.704 ± 0.0008 & 0.680 ± 0.0008 \\
        extsmaild.c & & 0.209 ± 0.0005 & 0.205 ± 0.0005 & 0.201 ± 0.0005 \\
        \bottomrule
    \end{tabular}
    \caption{PT mode Clang performs 4.76\% better
    than the NO LBR mode and 2.89\% better than the LBR mode}
\end{table}

To probe into why PT mode Clang outperforms the others, we will examine the
number of functions that BOLT aims to reorder and the actual percentage of
functions reordered in the three modes.

\subsubsection*{Analysis of number of function recorded during compilation,
number of function perf captures in sampling mode and number of functions BOLT
reorders}

We used baseline Clang to capture profiles by compiling extsmaild.c file and
rebuilt the binary in three mode. Table shows us number of functions BOLT
intends to reorder and percentage of clang’s total number of functions it ends
up reordering.

\begin{table}[H]
    \centering
    \begin{tabular}{lccc}
        \toprule
        Mode & Reordering Aim & \% of captured profile function reordered   \\
        \midrule
        NO LBR            & 606                     & 81.68\%   \\
        LBR               & 2638                    & 80.14\%   \\
        PT                & 5202                    & 78.99\%   \\
        \bottomrule
    \end{tabular}
    \caption{Profile functions reordered by BOLT}
\end{table}

Out of 135363 functions in Clang binary, BOLT reorders 495 in NO LBR mode, 2114
functions in LBR mode  and 4109 functions in PT mode i.e 0.37\%,  1.56\%, and
3.04\% of total functions respectively. \iti{note to self:check percentage numbers}

It is clear when collecting profile in LBR  we capture  4.2x more functions
than NO LBR mode and PT capture almost 2x more function than LBR mode.

Before we move onto investigating "Whether percentage of number of functions
that perf successfully captures in different profiling modes affects
performance of newer Clang builds", let's try to verify small hypothesis. Is
there a correlation between a higher number of captured functions in a profile
and a superior Clang build? 

\textbf{Hypothesis I: Increase in lines of code (loc) in testing file is
directly proportional to increased number of functions recorded by perf and
thus captured BOLT}

\begin{table}[H]
    \centering
    \scalebox{0.7}{
    \begin{tabular}{lcccc}
        \toprule
        Profiling mode  & extsmaild.c (1781) & tty.c
        (3026) & format.c (5188) & window-copy (5662) \\
        \midrule
        NO LBR & 606 & 1141 & 1550 & 1635 \\
        LBR  & 2638 & 3962 & 4981 & 4965 \\
        PT & 5202 & 6678 & 7911 & 8045 \\
        \bottomrule
    \end{tabular}}
    \caption{Profiled functions captured by BOLT using baseline Clang
    compilation of the following C programs. The column headers are C program
    we tested on and the number in bracket next to file name represents line of
    code values.}
\end{table}

Looking at values in Table, the number seems to working in the favor of our
hypothesis. This brings us to our next hypothesis:

\subsubsection*{Hypothesis II: An increase in the number of lines of code (LOC)
in testing files contributes to an increase in the number of functions recorded
in the perf data. This, when processed by BOLT, could potentially lead to the
generation of faster Clang builds. }

\begin{table}[ht]
    \centering
    \scalebox{0.7}{
    \begin{tabular}{lccccc}
        \toprule
        Benchmarked on / Clang versions & Baseline Clang & Clang.extsmaild.c (1781) & Clang.tty.c       (3026) & Clang.format.c (5188) & Clang.window-copy.c (5662) \\
        \midrule
        format.c & 0.907 ± 0.0008 & 0.737 ± 0.0061 & 0.739 ± 0.0074 & 0.739 ± 0.0089 & 0.735 ± 0.0005 \\
        tty.c & 0.557 ± 0.0008 & 0.449 ± 0.0005 & 0.451 ± 0.0005 & 0.451 ± 0.0005 & 0.450 ± 0.0005 \\
        window-copy.c & 0.880 ± 0.0008 & 0.715 ± 0.0008 & 0.718 ± 0.0005 & 0.717 ± 0.0008 & 0.717 ± 0.0005 \\
        extsmaild.c & 0.251 ± 0.0005 & 0.209 ± 0.0005 & 0.209 ± 0.0005 & 0.209 ± 0.0005 & 0.209 ± 0.0005 \\
        \bottomrule
    \end{tabular}}
\caption{NO LBR - Benchmarked Clang versions \laurie{what are the numbers in brackets?}}
    \label{table:1}
\end{table}

\begin{table}[ht]
    \centering
    \scalebox{0.7}{
    \begin{tabular}{lccccc}
        \toprule
        Benchmarked on / Clang versions & Baseline Clang & Clang.extsmaild.c (1781) & Clang.tty.c       (3026) & Clang.format.c (5188) & Clang.window-copy.c (5662) \\
        \midrule
        format.c & 0.907 ± 0.0008 & 0.736 ± 0.0082 & 0.731 ± 0.0070 & 0.725 ± 0.0093 & 0.723 ± 0.0005 \\
        tty.c & 0.557 ± 0.0008 & 0.450 ± 0.0005 & 0.444 ± 0.0005 & 0.442 ± 0.0005 & 0.442 ± 0.0005 \\
        window-copy.c & 0.880 ± 0.0008 & 0.716 ± 0.0008 & 0.710 ± 0.0008 & 0.705 ± 0.0008 & 0.704 ± 0.0008 \\
        extsmail.c & 0.251 ± 0.0005 & 0.209 ± 0.0005 & 0.207 ± 0.0005 & 0.205 ± 0.0005 & 0.206 ± 0.0005 \\
        \bottomrule
    \end{tabular}}
    \caption{LBR - Benchmarked Clang versions}
    \label{table:2}
\end{table}

In Figure x Table y , we observe in the NO LBR mode that the range (Mean ± CI) for each
run using Clang binaries exhibits significant overlap. The delta, in this case,
isn't substantial enough to infer a relationship between LOC and performance.
However, in the LBR mode, an increase in LOC profiled function generally
correlates with improved performance, exhibiting either equal or better performance. 
There is trend of diminishing returns over increasing LOC after certain point.

\begin{table}[ht]
    \centering
    \scalebox{0.7}{
    \begin{tabular}{lccccc}
        \toprule
        Benchmarked on / Clang versions & Baseline Clang & Clang.extsmaild.c (1781) & Clang.tty.c       (3026) & Clang.format.c (5188) & Clang.window-copy.c (5662) \\
        \midrule
        format.c & 0.907 ± 0.0008 & 0.723 ± 0.006 & 0.709 ± 0.0009 & 0.701 ± 0.009 & 0.701 ± 0.0008 \\
        tty.c & 0.557 ± 0.0008 & 0.441 ± 0.0005 & 0.430 ± 0.0005 & 0.429 ± 0.0005 & 0.428 ± 0.0005 \\
        window-copy.c & 0.880 ± 0.0008 & 0.705 ± 0.0008 & 0.689 ± 0.0008 & 0.684 ± 0.0008 & 0.680 ± 0.0008 \\
        extsmail.c & 0.251 ± 0.0005 & 0.205 ± 0.0005 & 0.203 ± 0.0005 & 0.201 ± 0.0005 & 0.202 ± 0.0005 \\
        \bottomrule
    \end{tabular}}
    \caption{PT - Benchmarked Clang versions}
    \label{table:3}
\end{table}

In the PT mode, a similar trend to LBR is noticed - an increase in LOC
tends to enhance performance, albeit with notably diminishing returns 
when we compare two files with lesser delta in loc.

Could the observed relative increase in Clang's performance with respect to 
the line of code (LOC) in both LBR and PT modes be attributed to their 
capacity to record more functions during sampling? This 
brings us to our next hypothesis:

\subsubsection*{Hypothesis II: perf in PT mode records more functions
in the samples to built better control flow.}

\texttt{perf record} in LBR and PT mode has some additional information
which is not available in NO LBR mode. We can get more details about branches
such as FROM and TO addresses of each branch with some additional meta-data
such as timing packets. LBR during each sample only logs last 8-32 during each
sample while PT theoretically records every branch. However, both LBR and
PT feature of perf are currently only supported on Intel devices.
Though there seem to be ongoing development to add support for LBR on AMD devices. 

One interesting outcome to see would be aggregating profiles recorded in the 
LBR mode and compared the Clang built from these new profiles to the 
PT mode Clang. So, our next two hypothesis capture this:

\subsubsection*{Hypothesis III : Aggregating multiple profiles over n runs on the 
same training file results in building faster Clang binaries.}

To test this, we aggregate profiles (collected with perf) over n runs on the same 
training file and scrutinize the resulting performance variations. We have four bar 
graphs,each graph represents benchmarking on testing file from our testing set.

\laurie{all these graphs need to be in a vector format. normally the easiest thing
is to output them as pdfs, which pdflatex can then easily read in. svgs are also
ok, as we can easily convert those to pdfs automatically in the makefile.}
\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/extsmaild.png}
        \caption{Image 1}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/format.png}
        \caption{Image 2}
        \label{fig:image2}
    \end{subfigure}

    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/tty.png}
        \caption{Image 3}
        \label{fig:image3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/window-copy.png}
        \caption{Image 4}
        \label{fig:image4}
    \end{subfigure}
    \caption{\small\itshape On the graph, the x-axis represents the Clang build with different
    training files, and the y-axis depicts the mean along with the 99\%
    confidence interval. The color, situated at the top right, indicates
    \texttt{number.mode} - 'number' signifies the counts of profiles that have
    been aggregated and 'mode' pertains to the profiling mode. And the black lines
    on top of colored bar are range(±) with confidence interval}
    \label{fig:images}
\end{figure}

Looking at Figure I graph I, it's quite evident that the performance of Clang versions, 
created with 5, 10, and 15 aggregated LBR profiles, matches or even 
surpasses the performance of the Clang generated with an PT mode profile 
The same trends seems to follow in other three graphs.

\subsubsection*{Hypothesis IV: Aggregating profiles from different training files
results in building faster Clang binary.}

\begin{table}[ht]
    \centering
    \scalebox{0.7}{
    \begin{tabular}{lccccc}
        \toprule
        Clang versions/ benchmarked on & extsmaild.c & tty.c & format.c & window-copy.c \\
        \midrule
        Clang.all-prog-aggr-lbr & 0.220 $\pm$ 0.0005 & 0.473 $\pm$ 0.0005 & 0.773 $\pm$ 0.0007 & 0.754 $\pm$ 0.0007 \\
        Clang.exts.pt & 0.223 $\pm$ 0.0005 & 0.488 $\pm$ 0.0007 & 0.794 $\pm$ 0.0005 & 0.776 $\pm$ 0.0007 \\
        Clang.tty.pt & 0.220 $\pm$ 0.0005 & 0.470 $\pm$ 0.0005 & 0.771 $\pm$ 0.0005 & 0.752 $\pm$ 0.0007 \\
        Clang.format.pt & 0.216 $\pm$ 0.0005 & 0.466 $\pm$ 0.0005 & 0.756 $\pm$ 0.0007 & 0.741 $\pm$ 0.0007 \\
        Clang.window-copy.pt & 0.217 $\pm$ 0.0005 & 0.467 $\pm$ 0.0005 & 0.736 $\pm$ 0.0007 & 0.759 $\pm$ 0.0007 \\
        \bottomrule
    \end{tabular}}
    \caption{Benchmarked Clang versions}
    \label{table:4}
\end{table}

\laurie{use descriptive labels, not `table:4' because that won't make sense
if/when you reorder parts of the report. so for example, this label might be
`Clang\_benchmarks' or similar.}
\cref{table:4} represents Clang trained with two modes LBR and Intel PT, where for LBR mode
we aggregated profile collected from 4 training files and Intel PT mode Clang 
were we used training files individually. We tested them on the same files we trained 
our Clang on.

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.6]{images/all_prof_aggr_ci.png}
    \caption{\textit{Caption: Todo* will be changing images to pdf/svg format later}}
    \label{fig:image}
\end{figure}

{reference img} clearly indicates that aggregating profiles captured from the
testing set doesn't seem to be enough to beat all Clang builds with PT 
profiles using the testing set. Hypothesis II seems to be the reason for 
this (expand on this).

However, we will use another hypothesis to answer why aggregating multiple 
LBR mode profiles helps in building Clang variants that first start converging
to performance values of Clang produced with PT mode profiles and later 
even surpasses it.

\subsubsection*{Hypothesis V: LBR mode collected profiles has finer-grained data than
PT mode collected profiles}

[Reference PT manual about timing packets]

By default, perf when recording in PT mode uses MTC timing packets. 
To use CYC timing packets, we would be using the following command:
\texttt{perf record -e intel\_pt/cyc,cyc\_thres=value,mtc\_period=value/u -- }

\begin{itemize}
    \item \textit{cyc:}
    \item \textit{cyc\_thres:}
    \item \textit{mtc\_period:}
\end{itemize}

Running a comparision with (compare 20.lbr Clang built from training set 
and 1.pt, 5.pt, 15.pt, and 20.pt cyc acc mode)

\section*{Future Work}

\laurie{need to expand what `hybrid' means by quite a bit! readers won't
understand from one sentence what it means. why not give a bit more of an
example? you can even take a couple of parargraphs to do so! explain why,
perhaps, mixing LBR and PT might prove better/more practical than either in
isolation}
I plan to investigate how we can improve PGO tools with hybrid profiling
information. A hybrid profile is a combination of two or more different
types of profiling method: the hope is that this will result in improved
performance \laurie{because of ...}. It would be interesting to combine information from LBR and PT mode
profiles. LBR accurate timing for branch instructions and PT detailed program's
execution profiling can create a more accurate and comprehensive profiling
technique.

The next steps are:

\begin{enumerate}
    \item Expand the experiments from this
report to a larger training and testing set.

    \item Determine the cost of the hybrid.
profiling technique using LBR and PT mode profiles with BOLT

    \item Test the hybrid profiling technique with other PGO tool(s).
\end{enumerate}

\section*{Timeline}
\noindent
\resizebox{\textwidth}{!}{%
    \begin{tabular}{llp{10cm}}
        \toprule
        Year & Month & Goals \\
        \midrule
        2023 & July-December & Expanding the scale of experiment, analyzing the data,
        and modifying BOLT to adopt to hybrid profiling information \\

        2024 & January-June & Literature review on PGO tools. Forming the research
        question. Benchmarking the results to see whether we can work on top of
        BOLT to add additional capabilities or should we build a
        new tool/framework. \\

        2024 & July-December & Complete first version of experimental PGO
        framework. \\

        2025 & January-June & Benchmarking and debugging of the new framework.
        Drafting the initial findings. \\

        2025 & July-December & Identifying areas which can be expanded on the new framework,
        receive feedback and do revisions. \\
        \bottomrule
\end{tabular}
} 

\bibliographystyle{plain}
\bibliography{bib}

\end{document}

