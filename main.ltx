%&main_preamble
\endofdump
\newcommand{\myabstract}[1]{%
  \noindent \textit{\small#1}
  \vspace{2\baselineskip} % Creates some space after the abstract
}

\begin{document}

\maketitle
\iti{\myabstract{%
The aim of my PhD is to develop an efficient framework for Profile Guided Optimization (PGO).
In this preliminary report, I have presented an analysis of Facebook’s BOLT, 
a post-link binary optimizer which is now part of llvm project and called llvm-bolt.
I have conducted an experimental investigation to discern how the use of varied types
of profiling information influences the performance of the optimized binary.
The performance metrics gathered in this study are derived from a relatively 
small test set due to resource constraints. Expanding the scope of the test set to 
obtain more comprehensive performance data forms part of the future 
work of this research.}}

\section{Introduction}
llvm-bolt is a post-link static binary optimizer that enhances the code layout
by utlizing sample-based profiling data at a binary level. This study investigates how much
performance difference we can get by providing different profiling samples. In
this report we discuss building clang binaries using llvm-bolt by providing
llvm-bolt three types of sample data. We used Intel machine \textbf{Intel(R)
Xeon(R) CPU E3-1270 v5 @ 3.60GHz} to run the experiments. The aim is to
understand performance improvement done over baseline clang binary (CLANG 16)
by llvm-bolt by providing fine-grained data, more branch information (tracing
data), and a combination of two. We use three profile types (called as “mode”
throughout the report), No Last Branch Record (NO LBR) which is default
\texttt{perf record} collected sample i.e without any flags, Last Branch Record
(LBR), and Intel PT (intel-pt or PT). Compared to baseline binary, clang built
with NO LBR profile (NO LBR mode clang) performs \% better, LBR mode clang
performs \% better and Intel-PT mode clang performs \% on single compilation
run. Later we evaluate performance different version of clangs running over 100
times, taking mean values along with 99\% confidence interval. The baseline
binary has been trained on four c programs (training set), and baseline along
with optimized binaries were benchmarked on the same set of files (testing
set).

\section{Background}
Modern compilers have integrated a lot of compiler optimization techniques,
such as using basic block frequencies and branch target information to perform
function inlining.  But they don’t have  runtime information and can’t
differentiate \iti{hot code path (executed a lot) fromcold code paths (executed
rarely)} . This often leads compilers to optimize cold code paths at the
expense of hot ones. Sampling data can help us collect hot code path (at least
most of the times) since show up more often. 

Techniques Profile-Guided Optimization (PGO) also know as Feedback-directed
Optimization (FDO) are known for optimizizing runtime performance with the help
of profiling information using static program analysis. PGO / FDO helps in
performing function splitting which helps in optimize code locality.
Traditional methods such as instrumentation-based profilers have significant
overhead of memory and performance for collecting profiling information,
additionally they might not end up capturing accurate information. PGO is
easily adoptable in production environment in form of continuous feeding
dynamic profiling data to rebuild the binary. This report focuses experiments
using Facebook’s BOLT (now known as llvm-bolt), a post-link FDO tool which uses
sample based profiling information. Although, llvm-bolt supports accepts any
profile which feeds it desired information for rebuilding binary we have used
linux’s toolchain perf. Not just llvm-bolt provides tool to convert perf data
into llvm-bolt friendly data but perf aslso let us capture different type of
profiling information very easily.

\subsection{Profiling Techniques}
\subsection*{Perf}
Perf, also commonly known as perf\_event, is a lightweight linux toolchain that
offers an abstraction over hardware-specific capabilities. It is primiarly used for
performance measurements by instrumenting events, these events are unified
interference from different parts of the kernel. Perf uses performance counters
to capture events per-thread, per-cpu and per-cpu or system-wide and collect
sample on top of these events.  

Measurable events includes:

\begin{itemize}
    \item PMU Hardware events: Performance Monitoring Unit or PMU contains
        measurable events such as number of cycles, instruction retired, L1
        cache and so on. They vary depending on processor and model type.
    
    \item Software events: low level events such as page-faults, cpu-clock and
        so, base based on kernel counters.
    
    \item User Statically Defined Tracing: static trace points for user level program.
    
    \item Kernel Tracepoint Events: hardcoded kernel-level instrumentation point.
    
    \item Dynamic Tracing: for dynamically instrumenting software by creating
        event(s) in any location.
    
    \item Timed Profiling: commonly used for capturing snapshot  by using
        custom timer interrupt events and invoked using \texttt{perf record
        -FHz}
\end{itemize}

The perf\_event tool is in the linux-tools -common package and can either be
installed via \texttt{apt install linux-tools-generic} or be built from source. 

perf works can instrument in following modes: a) Counting, using
\texttt{perf stat} which keeps count of running process for supported events,
unless specified \texttt{perf stat} runs in per-thread mode. Finally all
occurence of events are aggregated and summary is presented on the standard
output b) Sampling mode, using \texttt{perf record/report/annotate}. Event data
is written to the kernel and then read at an asynchronous rate by
\texttt{perf record} to write to the perf.data file. This file is then analyzed
by the perf \texttt{report} or \texttt{script} commands.

Another way to instrument is by using BPF program on events which can execute
custom user-defined programs in kernel space, and perform filters and summaries
of the data. But we won’t be covering that in our experiments.

\subsubsection*{How is a sample event recorded?}

An event sample is recorded when sampling counter overflows, which means
counters go from $2^{64}$ to 0. Since 64-bit hardware counters aren’t
implemented in any PMU, perf emulated it in software. Hence, when using perf on
32-bit system perf silently truncates the period. On every counter overflow,
the sample contains record of program execution with additional information
which depends on what user specified for and an instruction pointer
irrespective of event type.

\subsubsection*{Sample period and rate}

By default perf records sample at \textbf{both} user and kernel levels
using cycles events with average rate of ${1000}$ samples/sec or ${4000}$ events / sec
of thread execution. With Sampling \texttt{perf record} asks PMU to count some
events, for example cpu-cycles and generate an overflow interrupt after every
$nth$ event (e.g a million). On every interrupt perf\_event records such as :
PID/TID (Process/Thread ID), timestamp, command being executed, and instruction
pointer to the ring buffer and reset the counter to a new value.

There is a limit to maximum frequency that can be set but we can adjust both
frequency of collection samples and period i.e collecting sample every $nth$
occurance of an event by passing \texttt{-F} and \texttt{-c} flag respectively
to \texttt{perf record}. Once the ring buffer is filled, perf will dump the
data to \texttt{perf.data} file which can vary in size depending upon frequency
and period set.

\subsection*{LBR}
By default perf records only records basic performance counter and cannot
follow indirect function call where target is only known at the runtime, this
leads to forming incomplete control flow. Intel has additional support in their
CPUs called Last Branch Record (LBR), a hardware register to record additonal
information about branch instruction such as “from” and “to” address of each
branch with some additional metadata. The retired branches are captured in
rotating ring buffer.The stack depth (8, 16, or 32 frames) varies according to
the Intel CPU. Using LBR can also help in building better control flow graph
with hot code paths, the profile generated can be used for finding basic block
frequency or profile-guided optimization.

\subsection*{Intel-PT}
Intel-PT is a hardware based tracing technique integrated into Intel’s CPU
hardware. It traces branch execution which theoritically can be used to build
control flow for all executed code.

Since Intel-PT captures huge amount of data standard processing such as PMU is
not possible and thus it uses additional auxiliary buffer(AUX) which is
associated with perf’s ring buffer.

We can use Intel-PT with perf’s record command as: 

\texttt{perf record -e intel-pt//u ls}

\subsubsection*{Buffer Handling}
Intel-PT produces hundreds of megabytes of data per second per CPU and
sometimes it encounters two errors: \textbf{trace data-loss} (generating data at
faster rate than it can be recorded to file) and \textbf{overflow packet}
(unable to record data to memory).

Passing larger auxtrace \textbf{mmap} size seems to help in resolving the trace data
loss, which can be done by passing \texttt{-m} flag.So, passing \texttt{-m,
64M}  means we can set trace buffer size to 64MiB  per CPU. We would need to be
careful with choosing auxtrace mmap size. For instance, if we have 8 CPUs we
are essentially setting total 512MiB of trace buffer size.

To resolve overflow packets error in addition of setting AUX mmap size,
re-running the workload helps. This might capture slightly less data(a couple
of MiB) but you should be able to either see no or fewer “Instruction errors” in captured
data.

llvm-bolt fails to convert data into in a acceptable format if it encounters
trace errors in \texttt{.data} file,  therefore it’s essential to remove the
errors by following above workaround or using some other method.

\subsubsection*{llvm-bolt}
llvm-bolt is a static post link binary optimizer that is built on top of LLVM
compiler infrastructure.

It uses sample based profiling data and relocations mode (by passing
\texttt{-emit-relocs} flag) to recompile the binary, this helps llvm-bolt reorder
function blocks (or basic blocks) and to split hot/cold code blocks thus
improving code locality. For function discover, llvm-bolt relies on ELF symbol
table and function frame information (to get function boundaries).

To recompile binary llvm-bolt requires a specific format data. The fields that
are required are thread ID (TID) or process ID (PID), instruction pointer (IP),
and either event or branch stack information (brstack). So, it is possible to
collect profile from any method as long as it can be fed to llvm-bolt’s profile
conversion tool. However, for easier workflow using perf is recommended since
a) it’s easy to record different mode of samples b) llvm-bolt has a utility
tool to convert perf data directly into bolt friendly data.

\section {Optimizing Clang with llvm-bolt}
We decided to replicate \laurie{replicate whom/what?} optimizing Clang binary with llvm-bolt with some
additional steps.We build clang (version 16.0.3) and bootstrapped it with PGO
and LTO. Finally, we use the bootstraped version to collect profile for a new
clang build in three modes NO LBR, LBR and Intel PT \laurie{i think we can call this just `PT': LBR is also an Intel-only thing, I think}. llvm-bolt only supports
first two, and to make Intel PT work we had to make few tweaks to llvm-bolt’s
source code.

In LBR mode, llvm-bolt uses perf to fetch PID/TID, Instruction Pointer (IP) and
Branch Stack (brstack). llvm-bolt expects branch flags to either predict or
mispredict, however branch flags are not supported by Intel PT (why?)

To make llvm-bolt work we adjusted configuration option in perfconfig file
which sets mispred flag on all branches. Simply adding following lines to
~/.perfconfig lets us get brstack information out of sampled data
collected with Intel PT

\begin{lstlisting}[language=bash]

[intel-pt]

mispred-all = on

\end{lstlisting}


Next adjustment was modifying llvm-bolt’s source code for data aggregation,
adding \texttt{—itrace=i100usle} option in conjuction with perf scripts lets us
extract brstack field in PT mode. Throughout the report we have used multitime
to benchmark runs, this helps us to collect sample mean and standard
deviations.

\subsection*{Single run comparisions}
In this experiment, we established two distinct sets: a training set to
generate the profiles for constructing the new version of clang, and a testing
set to procure the mean wall-clock time and the 99\% confidence interval. The
latter was used to compare the performances of various clangs, each operating
the testing set file(s) one hundred times.

The experiment was conducted in three distinct modes: NO LBR, LBR, and Intel
PT. The term 'mode' here refers to the specific flags employed, if any, during
the profile capture of the training set file compilation.

Within the table, the first column presents the testing set files, while the
succeeding columns represent clangs, each built with the assistance of training
set files and consequently named with the corresponding filename extension.

Moving on to Table  \laurie{use \LaTeX labels, not hardcoded references. I've put the cleverref package to make this even easier for you}, we will analyze the performance of the best clang build
across the NO LBR, LBR, and Intel PT mode profiles. The following table
represents the difference:

\begin{table}[htbp]
  \centering
  \begin{tabular}{ccccc}
    \toprule
    Testing file & Baseline & NO LBR & LBR & Intel PT \\
    \midrule
    format.c & & 0.735 ± 0.0005 & 0.723 ± 0.0005 & 0.701 ± 0.0008 \\
    tty.c & & 0.449 ± 0.0005 & 0.442 ± 0.0005 & 0.428 ± 0.0005 \\
    window-copy.c & & 0.715 ± 0.0008 & 0.704 ± 0.0008 & 0.680 ± 0.0008 \\
    extsmaild.c & & 0.209 ± 0.0005 & 0.205 ± 0.0005 & 0.201 ± 0.0005 \\
    \bottomrule
  \end{tabular}
    \caption{\laurie{needs a caption}}
\end{table}

It can be observed that, on average, the Intel PT mode performs 4.76\% better
than the NO LBR mode and 2.89\% better than the LBR mode. To probe into why the
Intel PT mode clang outperforms the others, we will examine the number of
functions that llvm-bolt aims to reorder and the actual percentage of functions
reordered in the three modes.

\subsubsection*{Analysis of number of function recorded during compilation,
number of function perf captures in sampling mode and number of functions llvm-bolt
reorders}

For training we used clang build with extsmaild.c file in all thre modes, and 
for analysis we ran clang binaries on extsmaild.c to capture perf profiles 
and analysize number of functions recorded duing compilation.

\begin{table}
    \centering
    \begin{tabular}{lccc}
        \toprule
        Method & Perf Captured & llvm-bolt Reorder Aim & llvm-bolt Reordered \% \\
        \midrule
        NO LBR          & x                     & 311                       & 73.95\%                   \\
        LBR             & y                     & 1501                      & 71.95\%                   \\
        Intel PT        & z                     & 4177                      & 69.43\%                   \\
        \bottomrule
    \end{tabular}
    \caption{llvm-bolt Reordering Data}
\end{table}

Out of 175120 functions in Clang binary, llvm-bolt reorders 230
functions in NO LBR mode, 1080 functions in LBR mode and 2900 functions in Intel
PT mode. The unordered functions are ...

Functions captured during compilation (using valgrind) versus function 
captured by perf during sampling.

We will be using two tools, Callgrind, a profiling tool included in Valgrind
suite and perf which we use to capture profile and for profile guided optimization.

Callgrind tracks the callgraph (in simple terms function call hierarchy) of a
program during its execution. It dumps information about number of times each
function is called, number of instructions being executed and number of cycles
consumed by each functions in a callgrind.out.PID file. We are only interested
in number of functions being recorded. We used QCachegrind to study the
dumped data (MacOS version of KCachegrind).

The next question we aim to answer is the percentage of functions that perf
successfully captures. Is there a correlation between a higher number of
captured functions in a profile and a superior clang build (Hypothesis III)?

In order to delve further, we will formulate some hypotheses:

\subsubsection*{Hypothesis I: An increase in the number of lines of code (LOC)
in testing files leads to an increased number of functions captured during perf
sampling, consequently resulting in faster clang builds. }

\begin{table}[ht]
    \centering
    \scalebox{0.7}{
    \begin{tabular}{lccccc}
        \toprule
        Benchmarked on / clang versions & Baseline clang & clang.extsmaild.c (1781) & clang.tty.c       (3026) & clang.format.c (5188) & clang.window-copy.c (5662) \\
        \midrule
        format.c & 0.907 ± 0.0008 & 0.737 ± 0.0061 & 0.739 ± 0.0074 & 0.739 ± 0.0089 & 0.735 ± 0.0005 \\
        tty.c & 0.557 ± 0.0008 & 0.449 ± 0.0005 & 0.451 ± 0.0005 & 0.451 ± 0.0005 & 0.450 ± 0.0005 \\
        window-copy.c & 0.880 ± 0.0008 & 0.715 ± 0.0008 & 0.718 ± 0.0005 & 0.717 ± 0.0008 & 0.717 ± 0.0005 \\
        extsmaild.c & 0.251 ± 0.0005 & 0.209 ± 0.0005 & 0.209 ± 0.0005 & 0.209 ± 0.0005 & 0.209 ± 0.0005 \\
        \bottomrule
    \end{tabular}}
\caption{NO LBR - Benchmarked clang versions \laurie{what are the numbers in brackets?}}
    \label{table:1}
\end{table}

\begin{table}[ht]
    \centering
    \scalebox{0.7}{
    \begin{tabular}{lccccc}
        \toprule
        Benchmarked on / clang versions & Baseline clang & clang.extsmaild.c (1781) & clang.tty.c       (3026) & clang.format.c (5188) & clang.window-copy.c (5662) \\
        \midrule
        format.c & 0.907 ± 0.0008 & 0.736 ± 0.0082 & 0.731 ± 0.0070 & 0.725 ± 0.0093 & 0.723 ± 0.0005 \\
        tty.c & 0.557 ± 0.0008 & 0.450 ± 0.0005 & 0.444 ± 0.0005 & 0.442 ± 0.0005 & 0.442 ± 0.0005 \\
        window-copy.c & 0.880 ± 0.0008 & 0.716 ± 0.0008 & 0.710 ± 0.0008 & 0.705 ± 0.0008 & 0.704 ± 0.0008 \\
        extsmail.c & 0.251 ± 0.0005 & 0.209 ± 0.0005 & 0.207 ± 0.0005 & 0.205 ± 0.0005 & 0.206 ± 0.0005 \\
        \bottomrule
    \end{tabular}}
    \caption{LBR - Benchmarked clang versions}
    \label{table:2}
\end{table}

In Figure x Table y , we observe in the NO LBR mode that the range (Mean ± CI) for each
run using clang binaries exhibits significant overlap. The delta, in this case,
isn't substantial enough to infer a relationship between LOC and performance.
However, in the LBR mode, an increase in LOC profiled function generally
correlates with improved performance, exhibiting either equal or better performance. 
There is trend of deminishing returns over increasing LOC after certain point.

\begin{table}[ht]
    \centering
    \scalebox{0.7}{
    \begin{tabular}{lccccc}
        \toprule
        Benchmarked on / clang versions & Baseline clang & clang.extsmaild.c (1781) & clang.tty.c       (3026) & clang.format.c (5188) & clang.window-copy.c (5662) \\
        \midrule
        format.c & 0.907 ± 0.0008 & 0.723 ± 0.006 & 0.709 ± 0.0009 & 0.701 ± 0.009 & 0.701 ± 0.0008 \\
        tty.c & 0.557 ± 0.0008 & 0.441 ± 0.0005 & 0.430 ± 0.0005 & 0.429 ± 0.0005 & 0.428 ± 0.0005 \\
        window-copy.c & 0.880 ± 0.0008 & 0.705 ± 0.0008 & 0.689 ± 0.0008 & 0.684 ± 0.0008 & 0.680 ± 0.0008 \\
        extsmail.c & 0.251 ± 0.0005 & 0.205 ± 0.0005 & 0.203 ± 0.0005 & 0.201 ± 0.0005 & 0.202 ± 0.0005 \\
        \bottomrule
    \end{tabular}}
    \caption{Intel PT - Benchmarked clang versions}
    \label{table:3}
\end{table}

In the Intel PT mode, a similar trend to LBR is noticed - an increase in LOC
tends to enhance performance, albeit with notably diminishing returns 
when we compare two files with lesser delta in loc.

Could the observed relative increase in clang's performance with respect to 
the line of code (LOC) in both LBR and Intel PT modes be attributed to their 
capacity to record more functions during sampling? This 
brings us to our next hypothesis:

\subsubsection*{Hypothesis II: perf in Intel PT mode records more functions
in the samples to built better control flow.}

\texttt{perf record} in LBR and Intel-PT mode has some additional information
which is not available in NO LBR mode. We can get more details about branches
such as FROM and TO addresses of each branch with some additional meta-data
such as timing packets. LBR during each sample only logs last 8-32 during each
sample while Intel PT theoritically records every branch. However, both LBR and
Intel PT feature of perf are currently only supported on Intel devices.
Though there seem to be ongoing development to add support for LBR on AMD devices. 

One interesting outcome to see would be aggregating profiles recorded in the 
LBR mode and compared the clang built from these new profiles to the 
Intel PT mode clang. So, our next two hypothesis capture this:

\subsubsection*{Hypothesis III : Aggregating multiple profiles over n runs on the 
same training file results in building faster clang binaries.}

To test this, we aggregate profiles (collected with perf) over n runs on the same 
training file and scrutinize the resulting performance variations. We have four bar 
graphs,each graph represents benchmarking on testing file from our testing set.

\laurie{all these graphs need to be in a vector format. normally the easiest thing
is to output them as pdfs, which pdflatex can then easily read in. svgs are also
ok, as we can easily convert those to pdfs automatically in the makefile.}
\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/extsmaild.png}
        \caption{Image 1}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/format.png}
        \caption{Image 2}
        \label{fig:image2}
    \end{subfigure}

    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/tty.png}
        \caption{Image 3}
        \label{fig:image3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/window-copy.png}
        \caption{Image 4}
        \label{fig:image4}
    \end{subfigure}
    \caption{\small\itshape On the graph, the x-axis represents the clang build with different
    training files, and the y-axis depicts the mean along with the 99\%
    confidence interval. The color, situated at the top right, indicates
    \texttt{number.mode} - 'number' signifies the counts of profiles that have
    been aggregated and 'mode' pertains to the profiling mode. And the black lines
    on top of colored bar are range(±) with confidence interval}
    \label{fig:images}
\end{figure}

Looking at Figure I graph I, it's quite evident that the performance of clang versions, 
created with 5, 10, and 15 aggregated LBR profiles, matches or even 
surpasses the performance of the clang generated with an Intel PT mode profile 
The same trends seems to follow in other three graphs.

\subsubsection*{Hypothesis IV: Aggregating profiles from different training files
results in building faster clang binary.}

\begin{table}[ht]
    \centering
    \scalebox{0.7}{
    \begin{tabular}{lccccc}
        \toprule
        Clang versions/ benchmarked on & extsmaild.c & tty.c & format.c & window-copy.c \\
        \midrule
        clang.all-prog-aggr-lbr & 0.220 $\pm$ 0.0005 & 0.473 $\pm$ 0.0005 & 0.773 $\pm$ 0.0007 & 0.754 $\pm$ 0.0007 \\
        clang.exts.pt & 0.223 $\pm$ 0.0005 & 0.488 $\pm$ 0.0007 & 0.794 $\pm$ 0.0005 & 0.776 $\pm$ 0.0007 \\
        clang.tty.pt & 0.220 $\pm$ 0.0005 & 0.470 $\pm$ 0.0005 & 0.771 $\pm$ 0.0005 & 0.752 $\pm$ 0.0007 \\
        clang.format.pt & 0.216 $\pm$ 0.0005 & 0.466 $\pm$ 0.0005 & 0.756 $\pm$ 0.0007 & 0.741 $\pm$ 0.0007 \\
        clang.window-copy.pt & 0.217 $\pm$ 0.0005 & 0.467 $\pm$ 0.0005 & 0.736 $\pm$ 0.0007 & 0.759 $\pm$ 0.0007 \\
        \bottomrule
    \end{tabular}}
    \caption{Benchmarked clang versions}
    \label{table:4}
\end{table}

\cref{table:4} represents clang trained with two modes LBR and Intel PT, where for LBR mode
we aggregated profile collected from 4 training files and Intel PT mode clangs 
were we used training files individually. We tested them on the same files we trained 
our clang on.

\begin{figure}[htbp]
  \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/line_graph.aggr_all.png}
    \caption{Caption for Image 1}
    \label{fig:image1}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/all_prof_aggr_ci.png}
    \caption{Caption for Image 2}
    \label{fig:image2}
  \end{subfigure}
  \caption{\iti{I need to pick one graph in this experiement, need your suggestion}}
  \label{fig:image}
\end{figure}

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=\textwidth]{images/line_graph.aggr_all.png}
%     \caption{Image 4}
%     \label{figure:2}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{images/all_prof_aggr_ci.png}
%     \caption{Image 4}
%     \label{figure:2}
% \end{figure}

\cref{fig:image} clearly indicates that aggregating profiles captured from the
testing set doesn't seem to be enough to beat all clang builds with Intel PT 
profiles using the testing set. Hypothesis II seems to be the reason for 
this (expand on this).

However, we will use another hypothesis to answer why aggregating multiple 
LBR mode profiles helps in building clang variants that first start converging
to performance values of clang produced with Intel PT mode profiles and later 
even surpasses it.

\subsubsection*{Hypothesis V: LBR mode collected profiles has finer-grained data than
Intel PT mode collected profles}

[Reference Intel PT manual about timing packets]

By default, perf when recording in Intel PT mode uses MTC timing packets. 
To use CYC timing packets, we would be using the following command:
\texttt{perf record -e intel\_pt/cyc,cyc\_thres=value,mtc\_period=value/u -- }

\begin{itemize}
    \item \textit{cyc:}
    \item \textit{cyc\_thres:}
    \item \textit{mtc\_period:}
\end{itemize}

Running a comparision with (compare 20.lbr clang built from training set 
and 1.pt, 5.pt, 15.pt, and 20.pt cyc acc mode)

\section*{Future Work}

For future work, I intend to:
\begin{itemize}
    \item Expanding the scope of above experiements to
    bigger training and testing set.
    \item Investing cost of hybrid profiling technique, potentially
    picking up fine-grained details like LBR with increased 
    number of branch information/better control-flow
    like Intel PT.
    \item Exploring and assessing the PGO toolchain for potential application 
    within web browsers, with a specific focus on Chromium or 
    Chrome(?)\iti{Not sure about this but lmk if this sounds 
    feasible/something we would want to do?}
\end{itemize}

\section*{Timeline}

\end{document}

