%&main_preamble
\endofdump
\begin{document}

\maketitle

\begin{abstract}
    \noindent
     JIT compilation gives dynamic languages speed of compiled code with
     flexibility of interpretation. One variant of which is tracing JIT, which
     records instructions of most executed path in a trace and use it instead of
     original code in hot paths. But they are quite hard to implement. In
     Meta-tracing, language interpreters are built on top of a common tracing
     JIT compiler, removing the need to re-implement tracing JIT for every
     language. This separates language implementation concerns from JIT
     compilation. The yk meta-tracing system turns existing C interpreters into
     tracing JIT compilers. However, yk pessimistically turns off all
     optimisations, as not all compiler optimisations are safe to use 
     with yk or meta-tracing in general. This significantly slows down
     the normal interpreter. I am investigating regaining as much performance
     as possible for such interpreters. This reports covers progress for above
     problem till date and plans for the future. 
\end{abstract}

\section{Introduction}
Dynamic languages such as Python and JavaScript are very popular because of
their ease of writing programs and expressiveness. They are used to write small
script as well as large and complex programs such as web-applications. However,
dynamic languages tend to be quite slow to run when compiled\iti{add citation}.
In dynamic languages such as Python, the type information may vary at runtime,
which means compiler cannot rely on one type to transform operations into
machine code and thus will have to generate all possible combinations. This can
be very slow! JIT compilers overcome this problem by recording and using runtime
informations to optimise most executed paths (also known as "hot" paths). JIT
compilation is combination of two traditional method: ahead of time compilation
(AOT) and interpretation. Tracing JIT is a variation of JIT compilation. In
tracing JIT compilation, the VM identifies frequently executed, ‘hot’ program
paths at runtime. Once a hot path is detected, the VM records all instructions
in this path into a trace until execution loops back to the first instruction of
this path. The trace is then compiled and subsequent iterations of this path
execute the compiled trace instead of the original code. But developing custom
tracing JITs is hard undertaking. To solve this, Bolz et al \iti{add cite}
mentions meta-tracing approach in 2009. With meta-tracing system such as yk
language implementers don't need to implement their own tracing JITs instead
they can use yk meta-tracing system and the tracing JIT compilation will
automatically be embedded in the language implementations. yk traces the
execution of the interpreter instead of tracing the program executed by the
interpreter. However, not all compiler optimisations are safe to be used with
meta-tracing systems like yk. Therefore, yk pessimistically turns off all the
optimisations by default. This results in much slower interpreter than the
baseline (i.e~ interpreter compiled without the meta-tracer.) My work focuses on
minimising performance overhead of the yk meta-tracer and thus speeding the C
interpreter to as much extent as possible. In later sections, I'll discuss work
done so far on this and future directions.

Since meta-tracing is retrofitted onto the interpreter and then traces the
interpreter's execution, it involves two interpreters: one that tracing JIT
uses called tracing interpreter and the other that AOT uses to run users
programs called language interpreter. The program that language interpreter
executes is called user program. At first when the program starts everything is
interpreted (i.e~the user program), however when the hot loop is identified the
meta-tracer enters into tracing mode. During tracing, the interpreter records a
history of all the operations it executes. It traces until it has recorded the
execution of one iteration of the hot loop. To decide when this is the case, the
trace is repeatedly checked during tracing as to whether the interpreter is at a
position in the program where it had been earlier. The history recorded by the
tracer is called a trace: it is a sequential list of operations, together with
their actual operands and results. Such a trace can be used to generate
efficient machine code. A trace is sequential path which only follows one of the
many possible directions through the code. To ensure correctness, the trace
contains a guard at every possible point where the path could have followed
another direction, such as at conditions and indirect or virtual calls. When
generating the machine code, every guard is turned into a quick check to
guarantee that the path we are executing is still valid. If a guard fails, the
tracing JIT immediately quit the machine code and continue the execution by
falling back to interpretation (i.e~AOT mode). Further, at JIT time, the tracing
interpreter records the basic blocks (i.e~jumps), when tracing interpreter 
switches back to interpretation (at AOT) it needs to map the information back
at AOT basic block. Meta-tracer such as yk need to do this with hundred percent
accuracy. The JIT compiler relies on the traces to identify hot paths in the
program for optimization. If the mapping from the traced addresses to the LLVM
IR blocks is inaccurate, the JIT compiler might optimize the wrong code paths or
apply optimizations based on incorrect assumptions. This can lead to the
generation of incorrect or inefficient JIT-compiled code, affecting the
program's correctness and performance. However, some llvm optimisations such as branch folding
and block placement passes messes with the mapping. Additionally,
llvm applies some passes at different stages of the pipeline, even after the
final IR has been generated. This again interferes with mapping and creates 
problem for the meta-tracer. To avoid this problem the yk meta-tracing system
turns off all the optimisations by default, which slows down the interpreter 
significantly even without JIT being enabled.

\laurie{need to carefully tease out: what aspect of performance are you looking at? not JIT performance, but the `pure' (non-JIT) interpreter.}
In this report I show that it is possible to minimise the performance overhead
when the yk meta-tracing system is retrofitted in non-JITed C interpreter. Which
means that the performance of non-JITed interpreter compiled with yk subsystem
should be closer to the performance of the same interpreter when compiled with
non-meta-tracing compiler. 
The main contributions of this report are:
\begin{itemize}
    \item{Genetic algorithm to find llvm optimisation passes which doesn't
        break yk's tests and improves performance of yklua, a C interpreter.}
    \item{Identfying and fixing the cause of overhead when using yk to build
        yklua}
\end{itemize}
Through this work, I aim to bridge the gap between the theoretical performance \laurie{theoretical?!} of
meta-tracing JIT compilation and its practical performance for the  C
interpreter.

\subsection{Defining the problem}
For my work done in this report, I lay out two hypothesis:

\textbf{H1} \hspace*{0.5cm} There is a subset of existing compiler passes
which are compatible with meta-tracing and meaningfully improves interpreter's
performance.
\\
To validate this hypothesis, I began by running a divide-and-conquer algorithm
to find a pass sequence compatible with the meta-tracing system. Compatibility
is defined as the successful passing of the meta-tracer system's test cases and the
pass sequence's ability to build and run the interpreter. However, while this
method found a compatible pass sequence, it did not improve the interpreter's
performance and, in some cases, even made it worse. Later, I will present a
better approach using a stochastic genetic algorithm, which not only found
multiple compatible pass sequences but also yielded sequences that began to
improve the interpreter's performance.
\\
\\
\textbf{H2} \hspace*{0.5cm} It is possible to get performance of yk compiled
pure interpreter closer to that of traditionally compiled interpreter.
\\
For this hypothesis, I applied the full O2 pass sequence to the pure
interpreter. "Pure" here refers to a non-JIT-compiled interpreter.
Interestingly, even with full O2 optimisation, the yk-compiled interpreter is
three times slower than a traditionally compiled interpreter (e.g., compiled
using clang or gcc). In this report, I investigate further and show that yk's
pessimistic approach to optimisation, along with some specific yk flags,
contributes to the slowdown. I achieved some initial speedup by optimising one
yk flag that inserts a stack map. I also propose that further performance
improvements are possible by refining other yk flags and making traditional
optimisations compatible with the meta-tracing system.

The report is structured as follows: 
\section{Background}
\label{sec:background}

Modern compilers have huge number of optimisation passes available. These
optimisation passes target unique segments of program, such as transforming a
function, basic block or whole program. Further, these optimisations can be
applied at different stages of compilation pipelines. The sequence of passes try
to retain the semantic of the program while potentially improving program's
performance. There are are two type of passes: Analysis and Transformation. Both
of these passes work together, the analysis passes collect information about the
program and transformation passes are responsible for using the collected
information and change the program. Example of optimisation level provided by
modern compilers are: O0, O1, O2, O3, and Og. The two major compilers gcc and
clang have 200 and 100+ optimisation passes respectively, their sequence is
called as optimisation sequence. By default without optimisations level all of
the passes are turned off, and an expert can choose to turn on and off passes
accordingly.\iti{add reference}

\subsection{LLVM Project}
LLVM is a set of compiler and toolchain technologies. and has its owns
Intermediate Representation (LLVM IR). This language-independent IR helps LLVM
compilation to be target and source independent. The LLVM IR is optimised
through sequence of LLVM passes. Passes are operational unit of the IR which
can:
    \begin{itemize}
        \item{mutate the IR}
        \item{performs some computation about the IR}
        \item{or just print something}
    \end{itemize}

A unit of an IR is scope of the pass, it's what the pass is going 
to operate on. This can be :
\begin{itemize}
    \item{a module: LLVM module is a top-level structure that represents a
        single  unit of code which is processed together. It includes global
        variable, function declaration, and implementations.}
    \item{a function: LLVM function is a self-contained unit of execution within
          a module, it corresponds to function in the source code and contains
          a list of aruguments, a basic block, and a symbol table.}
    \item{a basic block: LLVM basic block is a linear sequence of instructions
          within a function, this has a single entry and exit point. They are
          used to represent straight-line code sequence that makes up function's
          body.}
    \item{an instruction: an LLVM instruction is the smallest unit of execution
         in the LLVM IR, representing a single operation within a basic block.
         Instructions can perform a wide range of operations, including arithmetic,
         memory access, logical operations, and control flow changes. Each instruction
         produces a result and can have zero or more operands, which are either constants
         or the results of other instructions.}
\end{itemize}
These passes are categorised into analysis, transformation and utility passes.
Depending on whether they perform computation on the IR, mutate the IR, or do
some other non-categorised task.

\subsection{LLVM's optimisation pipeline}
Classically, compilers are split into three parts, which is:
frontend, middle-end and the backend. The frontend is responsible
for dealing with programming-language specific analysis, this can be
operations such as parsing, type checking and so. The middle-end is where
optimisations are performed. These optimisations are target independent \iti{add
how}. Next, the backend is responsible for applying target-specific optimisation
and emiting the machine code.

Different languages like C, C++, and Rust (and many more) have their own
frontends but rely on LLVM for middle and backends optimisations.

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, 
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]
Frontend                   LLVM
/---------\               /-----------------------------------\
|  clang   |              |                                   |
|  rustc   | --(LLVM IR)-> | Middle end --(LLVM IR)--> Backend | --> Machine code
|   ...    |              |                                   |
\---------/               \-----------------------------------/
\end{Verbatim}
\end{tcolorbox}

\subsubsection{Type of optimisation pipelines}
For hypothesis I,  we are mostly concerned with LLVM's middle-end optimisation
pipeline. LLVM has three kind of optimisation pipeline: default (non-LTO),
ThinLTO, and FatLTO pipeline.

\subsubsection{Default Pipeline}
The default pipeline separately optimize each module, without any
special knowledge about other modules (apart from function/global declarations).
“Module” here corresponds to “one source file” in C/C++ and one “codegen unit”
in Rust.

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, 
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]
Module 1 --(Optimize)--> Module 1'
Module 2 --(Optimize)--> Module 2'
Module 3 --(Optimize)--> Module 3'
\end{Verbatim}
\end{tcolorbox}

\subsubsection{ThinLTO Pipeline}
QUESTION: what is prelink and postlink? does every optimisation level have
prelink and postlink? (answer yes, O2/O3 is made up of prelink and postlink
union flags)

The LTO pipelines are split into a pre-link and a post-link optimization
pipeline. After the pre-link pipeline, ThinLTO will perform some lightweight
cross-module analysis, and in particular import certain functions from other
modules to make them eligible for inlining. However, the post-link optimization
again works on individual modules.
Pre-link optimisation are module simplication optimisations while contrary to
the name post-link optimisation happen during link-time stage and involves both
module simplication and optimisation passes.

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, 
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]
                              cross-import
Module 1 --(ThinLTO pre-link)------\-/-----(ThinLTO post-link)--> Module 1'
Module 2 --(ThinLTO pre-link)-------X------(ThinLTO post-link)--> Module 2'
Module 3 --(ThinLTO pre-link)------/-\-----(ThinLTO post-link)--> Module 3'
\end{Verbatim}
\end{tcolorbox}

\subsubsection{FatLTO Pipeline}
Finally, FatLTO will simply merge all modules together after the pre-link
pipeline and then run the post-link pipeline on a single, huge module:

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, % Thickness of the box border
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]
             begin{Verbatim}[fontsize=\scriptsize,formatcom=\bfseries]
                                 merge
Module 1 --(FatLTO pre-link)-------\
Module 2 --(FatLTO pre-link)---------------(FatLTO post-link)--> Module'
Module 3 --(FatLTO pre-link)-------/
\end{Verbatim}
\end{tcolorbox}

While LTO stands for “link-time optimization”, it is not necessarily performed
by the linker. For example, rustc will usually perform it as part of the
compiler. The important property of LTO is that is allows cross-module
optimization.

LLVM’s optimization pipeline looks like:

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, % Thickness of the box border
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]

|-------------------------- default ----------------------------|

|------------- pre-link --------------|
|------------------------- post-link ---------------------------|

/-------------------------------------\
|       module simplification         |
|-------------------------------------|   /---------------------\
|                                     |   | module optimization |
|                    cgscc            |   |---------------------|   /---------\
| /-------\ /-----------------------\ |   |                     |   | backend |
| | early | |       inlining        | |   | vectorization       |   \---------/
| |cleanup| |function simplification| |   | runtime unrolling   |
| \-------/ \-----------------------/ |   \---------------------/
|                                     |
\-------------------------------------/
\end{Verbatim}
\end{tcolorbox}

Module simplification in LLVM's LTO pipeline involves pre-link optimizations
like early cleanup, function inlining, and call graph optimizations to prepare
modules for further optimization by simplifying their structure. Module
optimization, occurring post-link, applies global optimizations such as
vectorization and runtime loop unrolling to enhance execution efficiency across
the entire program. The backend phase then generates the final machine code,
incorporating register allocation and architecture-specific optimizations to
produce optimized executable code.

For yk meta-tracer we work with FatLTO because \iti{ask edd/laurie}.
\laurie{why is this a problem for yk but not rpython?}

\laurie{this will need an example of a problematic optimisation, at least
sketched out in pseudo-code, so that readers can understand what the problems
are.}

\iti{add diagrams: tracing, meta-tracing} \\
\iti{add explanation how in hardware tracing we do basic block mapping, while in 
software tracing we do instruction mapping and why} \\
\iti{introduce rpython, explain why wrong mapping of basic blocks creates problem for yk but not
rpython} \\
\iti{give example on how basic block mapping gets disturbed by some
optimisation, (maybe with basic block reordering algorithm?)} \\

\section{Addressing interpreter performance in a meta-tracing system}
As mentioned earlier, meta-tracing systems such as yk cannot enable all the
optimisation passes by default as it messes with basic-blocks mapping when
switching from JIT mode to AOT mode. Therefore, we need to carefully choose the
optimisation pass sequence which doesn't interferes with basic block mapping
while also improving the interpreters performance. Such problem is called
optimising phase ordering problem. Optimising phase ordering is a long standing
problem with tons of work done on it. However, the optimisation pass sequence
which might work for one system may not perform well or even work at all for the
other. Finding right optimisation pass sequence depends on the algorithm and
oracle used. Algorithm can be recursive or stochastic one while oracle means a
test set which sets initial constraint. Unless the pass sequence satisfied all
the constraint it won't be selected in the algorithm. The oracle can contain
constraints such as tests as well as execution time of the program.

Since some optimisation passes depends on others to enable program's
transformation, choosing a right sequence (and order of passes) is important for
program's performance either in respect to execution time or code size. However,
the number of passes are a lot (around 90) which makes our search space huge
(i.e~if we try all sequence irrespective of ordering we would have to try
$2^{90} - 1$ possible combinations!). In this report I initially work with two
algorithm, and finally choose genetic algorithm for the final solution. Genetic
algorithm is well-known stochastic algorithm which is adopted on the basis of
theory of evolution. This algorithm has been used by several researchers to
solve phase-ordering problem. Using genetic algorithm helps us to find an
optimal pass sequence for our oracle in acceptable time. Further, it isn't just
compatible with the meta-tracing system but also improves the C interpreter's
performance.

\section{Experimental Setup}

\section{Experiments}
In order to prove first hypothesis, I conducted experiments by first writing
a divide-and-conquer algorithm, although this algorithm gave a pass sequence
which did show some performance improvement of the interpreter, it however did 
not find a pass sequence which was compatible with the yk meta-tracing system
as well as improved interpreter's performance.
In the experiments done below we use yk as the meta-tracing system and yklua
as the C interpreter being optimised.


% \subsection*{Optimisation passes sequence}
% As we discussed types of pipeline, the next question is how and what passes are
% decided to be sent in prelink and postlink time? how do we know what works 
% for C++ and rust? (https://www.npopov.com/2023/04/07/LLVM-middle-end-pipeline.html)
%
% Why phase-ordering is important? Why can't we just schedule extra pass runs? 
% Question: why splitting the argument was important?
% Question:
%
% Add a section about oracle, describe the oracle.
%
% Question: why can't we use custom pipeline on our oracle?
% why yk or yklua oracle tests fail for some pass sequence?
% Question: how does passes look? (from pass slides)
% Question: how to fetch prelink and postlink passes for particular level?
% example using opt
% Yk
%
% Ykllvm
%
% Yklua
%
% This report focuses on solving phase-ordering problem using genetic algorithm
% for yk and ykluai. yk is a meta-tracing system and yklua is yk-enabled lua
% interpreter. First, I started with a recursive based approach to find pass
% sequence which doesn't violate constraints (where constraint is passing all of
% yk tests and building lua interpreter to run it's tests successfully.) Later, in
% the report I moved to using genetic algorithm due to it's nature to escape local
% optima very efficiently. \iti{add citation} In this report I also address the
% modifications I had to make to make our oracle suitable for the genetic
% algorithm. 
%
% In later sections I show that we can use a parallelised genetic algorithm
% for yk and yk-based interpreter to optimise  yklua by 1.5x-2x times 
% from baseline (where baseline is pipeline with non O2 passes) and identify
% what causes overhead and limit us to achieve theoretical performance for
% the interpreter.\iti{change this line too?} 

\subsection{Methodology}
\laurie{this paragraph, and some of what comes after, feels more like a textbook than a natural part of the flow of the report: it doesn't really help the reader. rather, the reader needs a higher-level overview of what you're doing before you dive into detail.}
Constraint Satisfaction Problems are mathematical problem with a finite set of
variable, each of which has a finite domain (i.e~set of possible values).
Since the variables can be related, this puts a limitation on what values
these variables can be instantiated with. To solve this problem, we have to find
possible values for these variables which lies within the domain and does
not violates any constraints.

Constraint Satisfaction Optimisation Problem (CSOP), is a NP-hard
problem\iti{add citation}. CSOP is a variation of Constraint Satisfaction
Problem which aims to find most optimal solution (i.e~global optima) in the
search space. One way to solve this problem is systematic search algorithm, in
this method we try all possible combinations of variables for the oracle set,
the variables can have any value possible from their domains, the algorithm then
exhausts the search space (if necessary) to find a combination which satisfies
our constraint set for the oracle(s). The problem with this search is that since
the number of combination can be exponentially large, even though it might be
finite to make sure the solution found is optimal or near-optimal the algorithm
will need to exhaust the entire problem (or search) space, this might  take
years.

Another way to solve CSOP is using stochastic search method, stochastic
search method involves randomising the search algorithm. Sometimes search
based problems contains multiple local optima, and deterministic or systematic
algorithm can get stuck in local optima because of combinatorial explosion
problem. 

I perform two algorithm to find best optimisation pass sequence for yk and
yklua, best means the optimisation pass sequence which succeeds all yk test and
improves the performance of yklua interpreter. The first algorithm is recursive
based binary split algorithm, and the second is stochastic based genetic
algorithm. 

But before that let's understand what Constraint Satisfaction Optimisation
Problem or CSOP is.\laurie{?} A finite (i.e~with a countable combination)
constraint satisfaction problem can be explained as a problem with a finite set
of variable, in which each variable can have values from the domain set (which
is possible values or state for that variable). Since variables can be related
to each other, these variables can only be instantiated with those values from
their domain which doesn't violate any constraint from the constraint set.

For easier understanding, CSOP has three variable: $X$, $D$ and $C$:
\begin{itemize}
    \item $X$ is a set of variables, ${X_1, X_2, X_3, ..., X_n}$ 
    \item $D$ is a set of domain for each variable, ${D_1, D_2, ..., D_n}$
    \item $C$ is a set of constraints that dictates allowable combinations.
\end{itemize}

CSOP can usually be solved using different form of search. In this report we
cover three search algorithm two of which are stochastic in nature:
\begin{itemize}
    \item{A devide-and-conquer search which uses binary splitting (or chopping) technique}
    \item{Non-stochastic divide-and-conquer search which uses binary splitting
        technique}
    \item{A global search which uses genetic algorithm}
\end{itemize}

\subsection{Stochastic Binary Splitting Algorithm}
\begin{tcolorbox}
\begin{algorithm}[H]
    \caption{Testing Optimization Passes in Compiler Pipeline}
    \begin{algorithmic}[1] % The [1] argument enables line numbering
    \small
    \State Define structures for \textbf{Pass} and \textbf{PipelineConfig}
    \State Fetch all available optimization passes

    \Function{test\_pipeline}{PipelineConfig}
        \State Execute the pipeline with the specified configuration
        \State Log the results
    \EndFunction

    \Function{main}{}
        \State Fetch all passes
        \State Perform initial testing of passes in isolation (\textbf{attempt1})
        \State Perform combined testing of passes (\textbf{attempt2})
    \EndFunction

    \Function{attempt1}{passes}
        \State Test each pass in isolation
        \State Log which passes succeed and which fail
    \EndFunction

    \Function{attempt2}{passes}
        \State Start with an empty set of okay passes
        \State Recursively test combinations of passes
        \If{a combination is successful}
            \State Add it to the set of okay passes
        \EndIf
        \If{a combination fails with more than one pass}
            \State Split and test subsets
        \EndIf
    \EndFunction
    \end{algorithmic}
\end{algorithm}
\end{tcolorbox}

\subsection{Genetic Algorithm}
\begin{tcolorbox}
\begin{algorithm}[H]
\caption{Testing Optimization Passes in Compiler Pipeline}
\begin{algorithmic}[1] % The [1] argument enables line numbering
\small
\State Define structures for \textbf{Pass} and \textbf{PipelineConfig}
\State Fetch all available optimization passes

\Function{test\_pipeline}{PipelineConfig $pl$}
\State Execute the pipeline with configuration $pl$
\State Log the results
\EndFunction

\Function{encode\_passes\_randomly}{Pass $root\_passes$}
\State Recursively set `on` flag of each pass to 0 or 1 randomly
\State Ensure at least one subpass is enabled if parent is enabled
\State Return modified $root\_passes$
\EndFunction

\Function{decode\_passes\_to\_string}{Pass $passes$}
\State Generate string representation of enabled passes and hierarchy
\State Exclude passes with `on` set to 0
\State Return the string representation
\EndFunction

\Function{evaluate\_fitness}{Pass $passes$, ...}
\State Decode passes string into PipelineConfig $pl$
\State Call $test\_pipeline(pl)$ to get execution time
\State Add fitness score (execution time) to shared list
\EndFunction

\Function{tournament}{list $population$, list $fitness$, float $sp$}
\State Select two individuals randomly
\State Choose fitter one with probability $sp$ as parent
\EndFunction

\Function{crossover}{Pass $parent1$, Pass $parent2$}
\State Create child passes by swapping random passes between parents
\State Return child passes
\EndFunction

\Function{mutate}{Pass $entity$, float $mutation\_rate$}
\State Recursively flip `on` flag of each pass with probability $mutation\_rate$
\State Return mutated pass
\EndFunction

\Function{genetic\_algorithm}{string $parsed\_str$, ...}
\State Create initial population of randomly encoded passes
\For{$generation$ in $1$ to $generations$}
\State Evaluate fitness of each entity in parallel
\State Select elites (good fitness)
\State Apply crossover and mutation to create new generation
\EndFor
\State Return best entity (set of passes)
\EndFunction

\Function{main}{}
\State Set up temporary directories
\State Get available passes using `get\_all\_passes`
\State Run genetic algorithm to find best passes
\State Decode best entity and print final passes
\EndFunction
\end{algorithmic}
\end{algorithm}
\end{tcolorbox}

\subsection{Algorithm's speed up after parallising}

\section{Results}

\subsection{Effects of applying meta-tracing compatible pass sequence on the yklua
interpreter}

\begin{table}[H]
\centering
\label{tab:performance_analysis}
\begin{tabular}{@{}p{7.5cm}ll@{}}
\toprule
\textbf{Yklua binaries built using yk-config} & \textbf{Mean} & \textbf{95\% CI} \\ 
\midrule
NO PRELINK + POSTLINK  & 0.140 & 0.002 \\
using just PRELINK & 0.107 & 0.005 \\
using just POSTLINK & 0.105 & 0.002 \\
using PRELINK + POSTLINK (without handling arguments) & 0.101 & 0.002 \\
using PRELINK (with handling arguments) & 0.097 & 0.003 \\
using PRELINK + POSTLINK (with handling arguments) & 0.096 & 0.003 \\
O2 PRELINK + POSTLINK & 0.097 & 0.004 \\
\bottomrule
\end{tabular}
\caption{The table shows performance of yklua interpreter when it has been built
either using no pass sequence (i.e baseline) and after applying prelink, postlink,
or both prelink and postlink on yklua binary. These numbers are obtained with
95\% confidence interval. The interpreter is built without JIT enabled,
that is only yk is retrofited onto the interpreter but JIT is turned off. 
The first row shows performance when no pass are applied on yklua, and the,
second and third row indicates performance when only pre or postlink passes are
applied on the interpreter. Third row indicates both passes being applied but
arguments inside functions are not handled. In fourth row we handle all
combination to receive compatible prelink and postlink pass sequence in our
genetic algorithm. Finally, last row represents yklua's performance when full
list of O2 (pre and postlink) and applied on the interpreter. This list however
is not compatible with yk meta-tracing system (i.e~ the list fails yk's tests)}
\end{table}

So, for this experiment we consider 2 ways to build lua binary:
\begin{itemize}
    \item{With the help of yk-config, we call this yklua}
    \item{With the help of traditional compilers such as gcc or clang and llvm opt tool.}
\end{itemize}

For 1): We can build yklua with and without optimisations. yk-config has support
to add `cflags` (compiler) and `ldflags` (linker), and we can use the prelink and postlink we got
from the genetic algorithm to append to the cflags and ldflags, this makes sure our
optimisations are applied on the yklua binary.

clang by default adds an `optnone` attribute to all the functions, which makes
any optimisation we add ineffective. For this, yk-config already has
`disable-O0-optnone` (which works with `-O0` flag). 

Table I\iti{add label} (in attached doc) shows performance numbers for 30 runs of `db.lua` when
built with and without optimisations. The first column is with no optimisations
and the rest has some or all optimisations turned on while building yklua
binary.

\iti{add which specific rows are interesting and why} \\
\iti{what result did we expect after applying full O2 passes (i.e~the last
column)}

\subsection{Apply full O2 to yk retrofitted and traditionally compiled
interpreter}

\textbf{Yklua binaries built without using yk-config}

\begin{tabular}{@{}p{7.5cm}ll@{}}
\toprule
\textbf{Yklua binaries built without using yk-config} & \textbf{Mean} & \textbf{95\% CI} \\ 
\midrule
lua binary build with gcc O2 & 0.034 & 0.004 \\
lua binary build with gcc O3 & 0.030 & 0.003 \\
lua binary build using clang and opt (O2 mode) & 0.075 & 0.004 \\
\bottomrule
\end{tabular}

\iti{add why with yk retrofitted into the interpreter there is 3x slowdown}

\subsection{Performance impact of yk-based flags with full O2 optimisation
turned on}

% Flags turned off (O2 passes are set)}

\begin{tabular}{@{}cllr@{}}
\toprule
\# & \multicolumn{1}{c}{Flags turned off (O2 passes are set)} & \multicolumn{1}{c}{Execution Time} & \multicolumn{1}{c}{95\% CI} \\ 
\midrule
1 & baseline (all flags are turned on) & 89.9ms & 0.002 \\
2 & —yk-no-fallthrough & 76.1ms & 0.002 \\
3 & —yk-block-disambiguate & 78.7ms & 0.00189 \\
4 & —yk-patch-control-point & 84.4ms & 0.002 \\
5 & —yk-insert-stackmaps & 85.8ms & 0.002 \\
6 & —yk-split-blocks-after-calls & 82ms & 0.002 \\
7 & —yk-no-fallthrough + —yk-block-disambiguate & 68.4ms & 0.002 \\
8 & —yk-no-fallthrough + —yk-block-disambiguate + —yk-patch-control-point & 63.9ms & 0.002 \\
9 & —yk-no-fallthrough + —yk-block-disambiguate + —yk-insert-stackmaps & 58.4ms & 0.002 \\
10 & —yk-no-fallthrough + —yk-block-disambiguate + —yk-split-blocks-after-calls & 66.8ms & 0.00189 \\
11 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-patch-control-point + —yk-insert-stackmaps\end{tabular} & 55.1ms & 0.002 \\
12 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-patch-control-point + —yk-split-blocks-after-calls\end{tabular} & 64.2ms & 0.002 \\
13 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-insert-stackmaps + —yk-split-blocks-after-calls\end{tabular} & 58.2ms & 0.002 \\
14 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-patch-control-point + —yk-insert-stackmaps +\\ —yk-split-blocks-after-calls\end{tabular} & 54.5ms & 0.003 \\
\bottomrule
\end{tabular}


\subsection{Performance retrieval by improving yk based flags}
\iti{add content about why this was need} \\
\iti{add performance improvement} \\
\iti{add link to the PR} \\

To optimise yk-insert-stackmaps: 1) Currently we insert
stackmaps before every function unless it's an intrinsic. However, we don't need
to insert stackmaps before calls to unmappable functions, as we don't trace
those and thus never have to deopt out of them. This might claw back a few ms
for this flag. 

\iti{1 was solved, add link to the PR}

2) Similarly, we also don't need to insert stackmaps before
functions that we know have no conditions inside them (no condition = no guard =
no deopt). This is a bit more difficult to implement because we will need to
store which function have no condition inside them. This can compromise as a
future work.

As for the yk-split-blocks-after-calls pass. If we adjust jitmodbuilder a bit,
we could also refrain from splitting blocks after calls to unmappable functions.
But that could turn out to be a bit fiddly in jitmodbuilder as the splitting
helps us to know when to start/stop outlining. \iti{Can we work out another way to
solve this?}

% \textbf{Hypothesis I: The sequence order of the optimisation passes is non-critical for
% obtaining successful optimisation pass sequence (for the target oracle).}
%
% The binary split algorithm described in section \iti{add section label} doesn't
% inherently maintain the original order of the optimization pass list during its
% divide-and-conquer process. When it splits the list of passes to test subsets,
% it uses a random shuffle before dividing them into two halves. This shuffling
% means that the original order of the passes is not preserved in the testing
% process.
%
% The random shuffle is intended to explore different combinations of passes in an
% attempt to quickly find a working set of passes. However, this approach does not
% consider the order in which passes are applied, which might be significant
% depending on the context. In compiler optimization, the order in which
% optimization passes are applied can affect the outcome of the optimizations.
% Some optimizations may need to be applied before others to be effective or to
% avoid negative interactions. Parallising the program

% \iti{add the pseudocode here.}
%
% \textbf{Hypothesis II: Elimination of stochastic elements from the divide-and-conquer
% strategy should give us at least a successful pass sequence for the target oracle
% quickly.}

% We start with stochastic binary-split algorithm to find pass list, but soon
% discover we cannot find single pass sequence which is correct for the target
% oracle. Correctness means that the pass sequence should successfully runs yk's
% test  as well as build yklua and run its benchmarks. 
% We discover this was because pass order matters in llvm's optimisation
% pipeline. What that means is as discussed in previous section, some passes
% depends on others to collect information or do some work/computation before they 
% can successfully run their optimisations and/or computations. We could maybe
% solve this by introducing some passes multiple times but that doesn't gaurante
% .... {from edd's comment in the binary split code}.

% \textbf{Hypothesis III: Using stochastic genetic-algorithm can result in better
% optimisation pass sequence for the target oracle compared to the conventional
% methods.}

% What we mean here is the pass list is bigger, which may result in faster
% interpreter. We can tweak the algorithm to mutate and crossover to choose
% better pass sequence while also including randomness to eascape local optima as
% described in section xyz. This is very difficult with some other methods which 
% can not escape local minima like global search as it may not check all the
% combination (as with other method mentioned in guided GA paper). 
%
% Speed of finding successful pass sequence was better, that means in just
% initial generation we were able to find passes 

% \textbf{Hypothesis IV: Running more generations results in finding the optimisation pass
% sequence which converges to global optima easily and making the interpreter faster
% by 3-4x from baseline}
%
% \textbf{Hypothesis V: The integration of high-overhead optimization passes within ykllvm
% contributes to its performance disparity when compared to using standard llvm clang.}
%
% \textbf{Hypothesis VI: Disabling optimisation passes when using yk incurs
% significant performance detriments}
%
% \textbf{Hypothesis: Applying Prelink + Postlink passes should have better performance than either
% just applying Prelink or Postlink pass sequence on the interpreter}
%
% \textbf{Hypothesis VIII: We can improve performance of interpreter when compiled with
% ykllvm by optimising yk-specific optimisation passes}
%

    % parallesing the algorithm for speed-up and faster testing. 
    % Speed up from parallesing
    %
    % Hypothesis III: Deciding upon oracle
    %
    % Hypothesis IV: aiming for performance
    %
    % obstacles: aiming for correctness (yk's tests), why we disabled some tests
    %
    % Obstacles: wall created by ykllvm and yk-config
    %
    %
    %
    % why these passes are high penality?
    %
    % is there any way to solve it?
    %
    % exaplain yk-config in methodology section \iti{IMPORTANT!}
    %
 % Dynamic languages are more expensive to compile than statically
 % typed languages. Since, traditional compilers don't have runtime
 % informations, they need to emit code that can handle 
 % all type of combinations that are possible at the runtime. This
 % makes performance of dynamic language much slower. JIT compilers
 % overcome this problem by recording and using runtime informations 
 % to optimise most executed paths. JIT compilation is 
 % combination of two traditional method : ahead of time compilation
 % (AOT) and interpretation. So
 % However, retrofitting meta-tracing system such as yk into
 % existing C-interpreters brings challenges to their performance. The aim of
 % my PhD is find ways to speed up retrofitting meta-tracing system (i.e~yk)
 % into existing interpreters, In this report, I first find optimisation pass
 % sequence (from tradition passes such as O2) using recursive and stochastic
 % method, the end result, when JIT is turned of is performance improvement by
 % 1.5-2 times for yklua, a C-based interpreter which uses yk. Next, I dig
 % deeper into reasons for runtime performance overhead introduce by yk in
 % C-based interpreters. 

\section{Related Work}

\section{Conclusion and Future Work}

Conclusion: In this report, I prove that retrofitting meta-tracing system such
as yk brings challenges to performance of C based interpreter. I am able to
get back some performance by applying compatible pass sequence obtained with the
help of genetic-algorithm, and by improving one of the costly yk-based flag used
for compilation of the interpreter.

Future work: In future, I intend to fetch more performance by improving yk-based
flag such as yk-insert-stackmaps, yk-split-blocks-after-calls, and
yk-no-fallthrough. \iti{add about yk-no-fallthrough, what it does and how we can
improve this}

Another possible hypothesis, which might be worth testing is whether we can
support optimisations which we have currently disabled because they interfere
with basic block mappings. This would require change in llvm or to write our own
optimisation passes which performs most of the optimisations but don't mess with
the mapping. One way could be to tweak the passes so that only optimisation
within the basic blocks are allowed (and those which work across are not).


% \bibliographystyle{plain}
% \bibliography{bib}

\end{document}
