%&main_preamble
\endofdump
\begin{document}

\maketitle

\begin{abstract}
    \noindent
     JIT compilation gives dynamic languages speed of compiled code with
     flexibility of interpretation. Tracing JIT is a variation that records
     instructions of the most executed path in a trace and uses it instead of
     original code in hot paths. But racing JITs are quite hard to implement. In
     Meta-tracing, language interpreters are built on top of a common tracing
     JIT compiler, removing the need to re-implement tracing JIT for every
     language. This separates language implementation concerns from JIT
     compilation. The yk meta-tracing system turns existing C interpreters into
     tracing JIT compilers. However, yk pessimistically turns off all
     optimisations, as not all compiler optimisations are safe to use with yk or
     meta-tracing in general. This significantly slows down the non-JITed
     interpreter. I am investigating regaining as much performance as possible
     for such interpreters. This reports covers progress for the above problem
     till date and plans for the future. 
\end{abstract}

\section{Introduction}
Dynamic languages such as Python and JavaScript are very popular because of
their ease of writing programs and expressiveness. They are used to write small
scripts as well as large and complex programs such as web-applications. However,
dynamic languages tend to be quite slow to run when compiled\iti{add citation}.
In dynamic languages such as Python, the type information may vary at runtime,
which means the compiler cannot rely on one type to transform operations into
machine code and thus will have to generate all possible combinations. This can
be very slow. JIT compilers overcome this problem by recording and using runtime
information to optimise the most executed paths (also known as ``hot'' paths).
JIT compilation is a combination of two traditional methods: ahead of time
compilation (AOT) and interpretation.

Tracing JIT is a variation of \laurie{``is a kind of''?} JIT compilation. In
tracing JIT compilation, the VM identifies frequently executed, ‘hot’ program
paths at runtime. Once a hot path is detected, the VM records all instructions
in this path into a trace until execution loops back to the first instruction of
this path. The trace is then compiled and subsequent iterations of this path
execute the compiled trace instead of the original code. But developing custom
tracing JITs is a hard undertaking. To solve this, Bolz et al \iti{add cite}
mentions \laurie{mentions?!} meta-tracing approach in 2009 \laurie{meta-tracing has an older history: you can see that in some of the things other papers cite}.

With meta-tracing systems such as yk language implementers don't need to
implement their own tracing JITs; instead they can use yk meta-tracing system
and the tracing JIT compilation will automatically be embedded in the language
implementation(s). yk traces the execution of the interpreter instead of tracing
the program executed by the interpreter. However, not all compiler optimisations
are safe to be used with meta-tracing systems like yk \laurie{because...}.
Therefore, yk pessimistically turns off all optimisations by default. This
results in a much slower interpreter than the baseline (i.e~ interpreter
compiled without the meta-tracer.) My work focuses on minimising performance
overhead of the yk meta-tracer and thus speeding the C interpreter to as much an
extent as possible. In later sections, I'll discuss the work done so far on this
and future directions.

In this report I show that it is possible to minimise the performance overhead
when the yk meta-tracing system is retrofitted in a non-JITed C interpreter. Which
means that the performance of a non-JITed interpreter compiled with yk subsystem
should be closer to the performance of the same interpreter when compiled with
non-meta-tracing compiler.
The main contributions of this report are:
\begin{itemize}
    \item{A genetic algorithm to find llvm optimisation passes that do not
        break yk's tests and improves performance of yklua, a C interpreter.}
    \item{Identfying and fixing the cause of overhead when using yk to build
        yklua}
\end{itemize}
Through this work, I aim to bridge the gap between the achievable performance of
meta-tracing JIT compilation and its current practical performance for the  C
interpreter.

\subsection{Defining the problem}
For my work done in this report, I lay out two hypotheses:

\textbf{H1} \hspace*{0.5cm} There is a subset of existing compiler optimisation passes
that is compatible with meta-tracing and meaningfully improves interpreter's
performance.
\\
To validate this hypothesis, I began by running a divide-and-conquer algorithm
to find a pass sequence compatible with the meta-tracing system. Compatibility
is defined as the successful passing of the meta-tracer system's test cases and the
pass sequence's ability to build and run the interpreter. However, while this
method found a compatible pass sequence, it did not improve the interpreter's
performance and, in some cases, even made it worse. Later, I will present a
better approach using a stochastic genetic algorithm, which not only found
multiple compatible pass sequences but also yielded sequences that began to
improve the interpreter's performance.
\\
\\
\textbf{H2} \hspace*{0.5cm} It is possible to get regain some performance for yk compiled
pure interpreter(i.e~Non-JIted) \\

For this hypothesis, I applied the full O2 pass sequence to the pure
interpreter. ``Pure''' here refers to a non-JIT-compiled interpreter.
Interestingly, even with full O2 optimisation, the yk-compiled interpreter is
three times slower than a traditionally compiled interpreter (e.g. compiled
using clang or gcc). In this report, I investigate further and show that yk's
pessimistic approach to optimisation, along with some specific yk flags
contributes to the slowdown. I achieved some initial speedup by optimising one
yk flag that inserts a stack map. I also propose that further performance
improvements are possible by refining other yk flags and making traditional
optimisations compatible with the meta-tracing system.

The report is structured as follows: 


\section{Background}
\label{sec:background}

Tracing refers to the process of capturing information about execution of a
program. Tracing JITs use the technique where they collect/record information
about most executed (or ``hot'') paths via traces of the program and later at
runtime, use the information to perform optimisations and make inlining decisions. A
trace is a sequential path which follows only one of the many possible directions
through the code. To ensure correctness, the trace contains a guard at every
possible point where the path could have followed another direction, such as at
conditions and indirect or virtual calls. When generating the machine code,
every guard is turned into a quick check to guarantee that the path we are
executing is still valid. If a guard fails, the tracing JIT immediately quits
the machine code and continues the execution by falling back to interpretation
(i.e~AOT compilation).

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[scale=0.3, width=\linewidth]{images/program.pdf} 
    \caption{First Image}
    \label{fig:image1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[scale=0.5, width=\linewidth]{images/trace.pdf} 
    \caption{Second Image}
    \label{fig:image2}
    \end{subfigure}
\end{figure}

Meta-tracing is a system where the tracing JIT is applied at one level down,
which means instead of tracing the execution of user program, the meta-tracer traces the 
execution of the interpreter. 

\iti{this should be caption of the figure}The yk meta-tracing uses ykllvm to
compile the interpreter's source code into a binary, which is called ahead of
time compilation (or AOT compilation). This binary has a built-in tracer, a jit
compiler and LLVM IR. The LLVM IR is used by the jit compiler to use \iti{doube use of use!} the tracing
information. The binary generated using ykllvm is then used to interpret the
user program. If the program has a loop, where a loop means the tracer comes
back to the point it started collecting trace from, then the builtin tacer
records this information. A trace starts getting collected when the meta-tracing
system detects control-point in the interpreter, which can be inserted by the
user initially. This control-point is later converted into a yk specific control
point. After collecting the full trace, the trace is compiled which is called
just-in-time compilation (or JIT compilation) \iti{some word missing here} part of the meta-tracing system.
If, while executing the user program, both loop and complete traces are detected
(which can be a later iteration of the loop) then instead of recording and
compiling the trace again, the already compiled trace is used.

In tracing JIT (which is the core of meta-tracing system), conditions are turned
into guards. The guards are responsible to check whether the conditions are
being met (i.e~are true) or not. If the conditions are true, and a trace of it is
already compiled then instead of interpreting the condition, the compiled trace
is used by the interpreter.

yk uses hardware tracer\footnote{which is Intel PT based} to collect the traces.
Intel PT can only store stream of basic blocks and give their virtual address.
Block mapping is used in hardware tracing to get from a traced virtual address,
to a file offset in the binary, to a LLVM IR block. The yk meta-tracing system 
depends on loader API and a special LLVM section to do this. This section is 
emitted just before backend optimisation.

At JIT time, the tracing interpreter records the basic blocks (i.e~jumps). When
the tracing interpreter switches back to interpretation (i.e~when a guard fails), it
needs to map the information back at AOT basic block; this is where LLVM IR is
used. Meta-tracers such as yk need to do this with a hundred percent accuracy.
The JIT compiler relies on the traces to identify hot paths in the program for
optimization. If the mapping from the traced addresses to the LLVM IR blocks is
inaccurate, the JIT compiler might optimize the wrong code paths or apply
optimizations based on incorrect assumptions. This can lead to the generation of
incorrect or inefficient JIT-compiled code, affecting the program's correctness
and performance.


However, some llvm optimisations passes mess with the basic-block mapping. One
such example is when basic blocks are merged, some blocks may be left
unaccounted in the trace. Additionally, llvm applies some passes at different
stages of the pipeline, even after the final IR has been generated. This again
interferes with mapping and creates problems for the meta-tracer. To avoid this
problem, the yk meta-tracing system turns off all the optimisations by default,
which slows down the interpreter significantly even without the JIT being
enabled.

\iti{add the edd's figure example here}

\begin{tcolorbox}[
    enhanced,
    boxrule=2pt,
    sharp corners,
    colback=white,
    colframe=gray,
    fontupper=\bfseries\footnotesize,
    before={\parindent15pt}, % Apply indentation globally
]
\begin{minipage}[t]{.49\linewidth} % Adjusted for centering
\begin{Verbatim}
+-----+
| bb1 |
+-----+
   |
   v
+---------------+
| bb2           |
| %3 = add %1, %2 |
+---------------+
   |
   v
+---------------+
| bb3           |
| %6 = store %3, %8 |
+---------------+
   |
   v
+-----+
| bb4 |
+-----+
\end{Verbatim}
\end{minipage}%
\hfill % Fill space to ensure the line is centered
{\vrule width 4pt} % Make line thicker
\hfill % Fill space to ensure the line is centered
\begin{minipage}[t]{.49\linewidth} % Adjusted for centering
\begin{Verbatim}
+------+
| mbb1 |
+------+
   |
   v
+----------------------+
| mbb2                 |
| %3 = add %1, %2      |
| %6 = store %3, %8    |
+----------------------+
   |
   v
+-------------------+
| mbb3              |
| (empty/deleted)   |
+-------------------+
   |
   v
+------+
| mbb4 |
+------+
\end{Verbatim}
\end{minipage}
\end{tcolorbox}

\captionof{figure}{High Level IR and Machine IR side by side after backend optimization.}

Modern compilers have a huge number of optimisation passes available. These
optimisation passes target unique segments of program, such as transforming a
function, basic block(s) or whole program. Further, these optimisations can be
applied at different stages of compilation pipelines. The sequence of passes try
to retain the semantics of the program while potentially improving program's
performance. There are are two type of passes: Analysis and Transformation. Both
of these passes work together: the analysis passes collect information about the
program and transformation passes are responsible for using the collected
information and change the program. Example of optimisation level provided by
modern compilers are: O0, O1, O2, O3, and Og. The two major compilers gcc and
clang have 200 and 100+ optimisation passes respectively; their sequence is
called as optimisation sequence. By default without optimisation level all of
the passes are turned off, and an expert can choose to turn on and off passes
accordingly.\iti{add reference}

\subsection{LLVM Project}
LLVM is a set of compiler and toolchain technologies and has its own
Intermediate Representation (LLVM IR). This language-independent IR helps LLVM
compilation to be target and source independent. The LLVM IR is optimised
through a sequence of LLVM passes. Passes are an operational unit of the IR which
can:
    \begin{itemize}
        \item{mutate the IR}
        \item{perform some computation about the IR}
        \item{or just print something}
    \end{itemize}

\iti{sentence might need rewriting, not vert clear}A unit of an IR is scope of the pass, it's what the pass is going 
to operate on. This can be :
\begin{itemize}
    \item{a module: LLVM module is a top-level structure that represents a
        single  unit of code which is processed together. It includes global
        variable(s), function declaration(s), and implementation(s).}
    \item{a function: LLVM function is a self-contained unit of execution within
          a module. It corresponds to a function in the source code and contains
          a list of aruguments, a basic block, and a symbol table.}
    \item{a basic block: LLVM basic block is a linear sequence of instructions
          within a function, and has a single entry and exit point. They are
          used to represent the straight-line code sequence that makes up a function's
          body.}
    \item{an instruction: an LLVM instruction is the smallest unit of execution
         in the LLVM IR, representing a single operation within a basic block.
         Instructions can perform a wide range of operations, including arithmetic,
         memory access, logical operations, and control flow changes. Each instruction
         produces a result and can have zero or more operands, which are either constants
         or the results of other instructions.}
\end{itemize}
These passes are categorised into analysis, transformation and utility passes
depending on whether they perform computation on the IR, mutate the IR, or do
some other non-categorised task.

\subsection{LLVM's optimisation pipeline}
Classically, compilers are split into three parts, which is:
frontend, middle-end and the backend. The frontend is responsible
for dealing with programming-language specific analysis which can be
operations such as parsing, type checking and so. The middle-end is where
optimisations are performed. These optimisations are target independent \iti{add
how}. Next, the backend is responsible for applying target-specific optimisation
and emiting the machine code.

Different languages like C, C++, and Rust (and many more) have their own
frontends but rely on LLVM for middle and backends optimisations.

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, 
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]
Frontend                   LLVM
/---------\               /-----------------------------------\
|  clang   |              |                                   |
|  rustc   | --(LLVM IR)-> | Middle end --(LLVM IR)--> Backend | --> Machine code
|   ...    |              |                                   |
\---------/               \-----------------------------------/
\end{Verbatim}
\end{tcolorbox}

\subsubsection{Type of optimisation pipelines}
For hypothesis I \laurie{`H1'?},  we are mostly concerned with LLVM's middle-end optimisation
pipeline. LLVM has three kinds of optimisation pipelines: default (non-LTO),
ThinLTO, and FatLTO pipeline.

\subsubsection{Default Pipeline}
The default pipeline separately optimises each module, without any
special knowledge about other modules (apart from function/global declarations).
“Module” here corresponds to “one source file” in C/C++ and one “codegen unit”
in Rust.

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, 
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]
Module 1 --(Optimize)--> Module 1'
Module 2 --(Optimize)--> Module 2'
Module 3 --(Optimize)--> Module 3'
\end{Verbatim}
\end{tcolorbox}

\subsubsection{ThinLTO Pipeline}

The LTO pipelines are split into a pre-link and a post-link optimization
pipeline. After the pre-link pipeline, ThinLTO will perform some lightweight
cross-module analysis, and in particular import certain functions from other
modules to make them eligible for inlining. However, the post-link optimization
again works on individual modules.
Pre-link optimisations are module simplication \iti{simplification?} optimisations, while contrary to
the name, post-link optimisations happen during link-time stage and involve both
module simplication \iti{simplification?} and optimisation passes.

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, 
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]
                              cross-import
Module 1 --(ThinLTO pre-link)------\-/-----(ThinLTO post-link)--> Module 1'
Module 2 --(ThinLTO pre-link)-------X------(ThinLTO post-link)--> Module 2'
Module 3 --(ThinLTO pre-link)------/-\-----(ThinLTO post-link)--> Module 3'
\end{Verbatim}
\end{tcolorbox}

\subsubsection{FatLTO Pipeline}
Finally, FatLTO will simply merge all modules together after the pre-link
pipeline and then run the post-link pipeline on a single, huge module:

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, % Thickness of the box border
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]
             begin{Verbatim}[fontsize=\scriptsize,formatcom=\bfseries]
                                 merge
Module 1 --(FatLTO pre-link)-------\
Module 2 --(FatLTO pre-link)---------------(FatLTO post-link)--> Module'
Module 3 --(FatLTO pre-link)-------/
\end{Verbatim}
\end{tcolorbox}

While LTO stands for “link-time optimization”, it is not necessarily performed
by the linker. For example, rustc will usually perform it as part of the
compiler \iti{compilation?}. The important property of LTO is that it allows cross-module
optimization.

LLVM’s optimization pipeline looks like:

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, % Thickness of the box border
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]

|-------------------------- default ----------------------------|

|------------- pre-link --------------|
|------------------------- post-link ---------------------------|

/-------------------------------------\
|       module simplification         |
|-------------------------------------|   /---------------------\
|                                     |   | module optimization |
|                    cgscc            |   |---------------------|   /---------\
| /-------\ /-----------------------\ |   |                     |   | backend |
| | early | |       inlining        | |   | vectorization       |   \---------/
| |cleanup| |function simplification| |   | runtime unrolling   |
| \-------/ \-----------------------/ |   \---------------------/
|                                     |
\-------------------------------------/
\end{Verbatim}
\end{tcolorbox}

Module simplification in LLVM's LTO pipeline involves pre-link optimizations
like early cleanup, function inlining, and call graph optimizations to prepare
modules for further optimization by simplifying their structure. Module
optimization, occurring post-link, applies global optimizations such as
vectorization and runtime loop unrolling to enhance execution efficiency across
the entire program. The backend phase then generates the final machine code,
incorporating register allocation and architecture-specific optimizations to
produce an optimized executable code.

For yk meta-tracer we work with FatLTO because \iti{ask edd/laurie}.
\laurie{why is this a problem for yk but not rpython?}

\laurie{this will need an example of a problematic optimisation, at least
sketched out in pseudo-code, so that readers can understand what the problems
are.}

\iti{add diagrams: tracing, meta-tracing} \\
\iti{add explanation how in hardware tracing we do basic block mapping, while in 
software tracing we do instruction mapping and why} \\
\iti{introduce rpython, explain why wrong mapping of basic blocks creates problem for yk but not
rpython} \\
\iti{give example on how basic block mapping gets disturbed by some
optimisation, (maybe with basic block reordering algorithm?)} \\

\section{Addressing interpreter performance in a meta-tracing system}
As mentioned earlier, meta-tracing systems which use hardware tracing mechanism
such as yk cannot enable all the optimisation passes by default, as they
reorganise the basic-blocks and thus messe the mapping when switching from JIT
mode to AOT mode. Therefore, we need to carefully choose the optimisation pass
sequence which does not interfere with basic block mapping while also improving
the interpreter's performance.

Such a problem is called optimising phase ordering
problem. Optimising phase ordering is a long standing problem with a substantial
work done on it \iti{add citation(s)}. However, the optimisation pass sequence
which might work for one system may not perform well or even work at all for another.
Finding the right optimisation pass sequence depends on the algorithm and the
oracle used. The algorithm can be a recursive or stochastic one, while oracle means \iti{means or is?} a
test set which establishes initial constraint. Unless the pass sequence
satisfies all the constraints, it will not be selected in the algorithm. The oracle
can contain constraints such as tests as well as execution time of the program.

Since some optimisation passes depend on others to enable a program's
transformation, choosing a right sequence (and order of passes) /iti{what is the difference b/w sequence and order of passes?} is important for
a program's performance either in respect to execution time, code size or both.
However, the such high number of passes (around 90) makes our search space large
(i.e~if we try all sequence irrespective of ordering we would have to try
$2^{90} - 1$ possible combinations!).

In this report I start with a
divide-and-conquer algorithm to find a meta-tracing compatible pass sequence, and
later move on to a genetic-algorithm for the same. Genetic algorithm is well-known \iti{genetic algorithms are a class of...?}
stochastic algorithm which is adopted on the basis of theory of evolution. This
algorithm has been used by several researchers to solve phase-ordering
problems\iti{add cite}.

Using genetic algorithm helps us to find an optimal \laurie{it doesn't: it allows us, if we're lucky, to find a ``better than not doing anything'' pass. this might not even be a local optima, let alone a global optima!} pass
sequence for our oracle in acceptable time, where \laurie{this is your definition of `acceptable time'. other people might be willing to run things for weeks or longer. so i would say ``I define `acceptable time' as at most two hours because XYZ''} acceptable time is finding
compatible pass sequence within couple of hours if not minutes. Further, the
genetic-algorithm is not only able to find a pass sequence that is compatible with the
meta-tracing system, but the pass sequence also ends up improving the C
interpreter's performance. This is much better than divide-and-conquer algorithm
which was only able to find a pass sequence which sped up the interpreter \laurie{huh?}.

\section{Experimental Setup}

\section{Experiments}
In order to prove first hypothesis, I conducted experiments\footnote{\iti{add
link to the PR}} by first writing
a divide-and-conquer algorithm. And although this algorithm gave a pass sequence
which did show some performance improvement of the interpreter, it however did 
not find a pass sequence which was compatible with the yk meta-tracing system
whilst achieving lower execution time for the interpreter than the baseline.
In the experiments done below we use yk as the meta-tracing system and yklua
as the C interpreter being optimised.

\subsection{Methodology}

\subsubsection{Divide-and-conquer algorithm for phase ordering problem}
\begin{tcolorbox}
\begin{algorithm}[H]
    \begin{algorithmic}[1] % The [1] argument enables line numbering
    \small
    \caption{First attempt to find a yk compatible pass sequence which may improve
    yklua's performance}
    \State Define structures for \textbf{Pass} and \textbf{PipelineConfig}
    \State Fetch all available optimization passes

    \Function{test\_pipeline}{PipelineConfig}
        \State Execute the pipeline with the specified configuration
        \State Log the results
    \EndFunction

    \Function{attempt1}{passes}
        \State Test each pass in isolation
        \State Log which passes succeed and which fail
    \EndFunction

    \Function{attempt2}{passes}
        \State Start with an empty set of okay passes
        \State Recursively test combinations of passes
        \If{a combination is successful}
            \State Add it to the set of okay passes
        \EndIf
        \If{a combination with more than one pass fails}
            \State Split and test subsets
        \EndIf
    \EndFunction

    \Function{main}{}
        \State Fetch all passes
        \State Perform initial testing of passes in isolation (\textbf{attempt1})
        \State Perform combined testing of passes (\textbf{attempt2})
    \EndFunction
    \end{algorithmic} 
\end{algorithm}
\textbf{Algorithm 1:} this algorithm has two functions, attempt1 tries each pass
individually to see which passes succeed and which ones fail in isolation. It
takes a total of 36 hours to run attempt1. In attempt2, the algorithm starts with
an empty list of ok passes, recursively tests combinations of passes starting with
full list and then splits the list in two if the initial list fails and only
adds the subsets which succeed to the okay pass list. The run time for attempt2
is between 9-10 hours. The initial version of the algorithm used to reshuffle
the list as well to make it stochastic, however, I noticed that we received no
pass sequence which was compatible with yk as well as improved yklua's
performance. Removing stochastic elements helped me to get at least a pass
sequence which was compatible with yk, but failed to provide performance
improvement to yklua. \iti{any reason to be given?}
\end{tcolorbox}

% \subsection*{Optimisation passes sequence}
% As we discussed types of pipeline, the next question is how and what passes are
% decided to be sent in prelink and postlink time? how do we know what works 
% for C++ and rust? (https://www.npopov.com/2023/04/07/LLVM-middle-end-pipeline.html)
%
% Why phase-ordering is important? Why can't we just schedule extra pass runs? 
% Question: why splitting the argument was important?
% Question:
%
% Add a section about oracle, describe the oracle.
%
% Question: why can't we use custom pipeline on our oracle?
% why yk or yklua oracle tests fail for some pass sequence?
% Question: how does passes look? (from pass slides)
% Question: how to fetch prelink and postlink passes for particular level?
% example using opt
% Yk
%
% Ykllvm
%
% Yklua
%
% This report focuses on solving phase-ordering problem using genetic algorithm
% for yk and ykluai. yk is a meta-tracing system and yklua is yk-enabled lua
% interpreter. First, I started with a recursive based approach to find pass
% sequence which doesn't violate constraints (where constraint is passing all of
% yk tests and building lua interpreter to run it's tests successfully.) Later, in
% the report I moved to using genetic algorithm due to it's nature to escape local
% optima very efficiently. \iti{add citation} In this report I also address the
% modifications I had to make to make our oracle suitable for the genetic
% algorithm. 
%
% In later sections I show that we can use a parallelised genetic algorithm
% for yk and yk-based interpreter to optimise  yklua by 1.5x-2x times 
% from baseline (where baseline is pipeline with non O2 passes) and identify
% what causes overhead and limit us to achieve theoretical performance for
% the interpreter.\iti{change this line too?} 

\subsubsection{Genetic Algorithm for phase-ordering problem}
Genetic algorithm is a stochastic search algorithm which borrows the concept of
evolution from nature. This algorithm starts from an initial list (often
represented in a bitwise form) called entities and explores different
combinations. This is done by combining elements from two entities by the way of crossover
and mutation into a third entity, which may represent
a new point in the vast search space, that otherwise would have been left unexplored
by search methods like divide-and-conquer.

Some terminology for the genetic algorithm:

\begin{itemize}
\item{Entity: In genetic algorithms, an entity (often also called an
``individual''' or ``chromosome'') represents a possible solution to the
problem being addressed. An entity is typically encoded as a string or
array, where the structure and content of this encoding depends on the
specific problem. Each entity has a set of properties (or ``genes'')
that can be altered and combined with properties of other entities
during the algorithm's processes. The fitness of each entity, determined
by a fitness function, indicates how good a solution it is with respect
to the problem.}

\item{Crossover: Crossover (also known as ``recombination'') is another genetic operator
used to combine the genetic information of two parent entities to generate new
offspring entities. The process involves swapping sections of the parents'
encodings to produce one or more children. The rationale behind crossover is to
take the good characteristics from each parent and combine them to produce better
offsprings, potentially leading to better solutions in subsequent generations.
There are various crossover techniques, such as single-point crossover,
multi-point crossover, and uniform crossover, each differing in how and which
parts of the parent entities' genes are exchanged.}

\item{Mutation: Mutation is a genetic algorithm operator used to maintain genetic
diversity from one generation of a population of entities to the next. It is
analogous to biological mutation. The process involves making random changes to
individual genes of an entity. This can involve flipping bits in a binary
string, changing numbers in an array, or altering any part of the entity's
encoding. Mutation introduces new genetic structures in the population by
creating variants of existing entities, which helps in exploring new solutions
and prevents the algorithm from getting stuck in local optima.}
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.5]{images/genetic_algo.pdf}
    \caption{\small a parallel genetic algorithm used to find an optimization pass
        sequence compatible with yk and yklua. Initially, it randomly generates
        a population of passes, where each entity is a binary array indicating
        whether a specific pass is active (1) or inactive (0). The main parallel
        component is the fitness function, which is a value associated with
        every entity indicating how ``optimal'' a pass sequence is. Optimal means
        a pass sequence which succeeds all yk's tests while also minimising
        execution time of the interpreter. Each entity's performance is tested
        in parallel, speeding up the process significantly. The algorithm uses
        tournament selection (reproduction operator) to pick parents, which are
        then recombined and mutated to form new entities, iterating through
        generations to improve pass sequence combinations. This evolutionary
        process continues until the optimal set of passes, leading to the best
        interpreter performance, is identified.}
    \label{aggregator-profiles}
\end{figure}


\begin{tcolorbox}
    \begin{algorithm}[H]
        \begin{algorithmic}[1] % The [1] argument enables line numbering
        \small
        \State Define structures for \textbf{Pass} and \textbf{PipelineConfig}
        \State Fetch all available optimization passes

        \Function{test\_pipeline}{PipelineConfig $pl$}
            \State Execute the pipeline with configuration $pl$
            \State Log the results
        \EndFunction

        \Function{encode\_passes\_randomly}{Pass $root\_passes$}
            \State Recursively set `on` flag of each pass to 0 or 1 randomly
            \State Ensure at least one subpass is enabled if parent is enabled
            \State Return modified $root\_passes$
        \EndFunction

        \Function{decode\_passes\_to\_string}{Pass $passes$}
            \State Generate string representation of enabled passes and hierarchy
            \State Exclude passes with `on` set to 0
            \State Return the string representation
        \EndFunction

        \Function{evaluate\_fitness}{Pass $passes$, ...}
            \State Decode passes string into PipelineConfig $pl$
            \State Call $test\_pipeline(pl)$ to get execution time
            \State Add fitness score (execution time) to shared list
        \EndFunction

        \Function{tournament}{list $population$, list $fitness$, float $sp$}
            \State Select two individuals randomly
            \State Choose fitter one with probability $sp$ as parent
        \EndFunction

        \Function{crossover}{Pass $parent1$, Pass $parent2$}
            \State Create child passes by swapping random passes between parents
            \State Return child passes
        \EndFunction

        \Function{mutate}{Pass $entity$, float $mutation\_rate$}
            \State Recursively flip `on` flag of each pass with probability $mutation\_rate$
            \State Return mutated pass
        \EndFunction

        \Function{genetic\_algorithm}{string $parsed\_str$, ...}
            \State Create initial population of randomly encoded passes
            \For{$generation$ in $1$ to $generations$}
            \State Evaluate fitness of each entity in parallel
            \State Select elites (good fitness)
            \State Apply crossover and mutation to create new generation
            \EndFor
            \State Return best entity (set of passes)
        \EndFunction

        \Function{main}{}
            \State Set up temporary directories
            \State Get available passes using `get\_all\_passes`
            \State Run genetic algorithm to find best passes
            \State Decode best entity and print final passes
        \EndFunction
        \end{algorithmic}
    \end{algorithm}
\end{tcolorbox}

\subsection{Algorithm's speed up after parallising}

Since fitness function for each entity is being run in
parallel\footnote{\iti{add link to the PR}}.....

\section{Results}

\subsection{Effects of applying meta-tracing compatible pass sequence on the yklua
interpreter}

\begin{table}[H]
\centering
\label{tab:performance_analysis}
\begin{tabular}{@{}p{7.5cm}ll@{}}
\toprule
\textbf{Yklua binaries built using yk-config} & \textbf{Mean} & \textbf{95\% CI} \\ 
\midrule
NO PRELINK + POSTLINK  & 0.140 & 0.002 \\
using just PRELINK & 0.107 & 0.005 \\
using just POSTLINK & 0.105 & 0.002 \\
using PRELINK + POSTLINK (without handling arguments) & 0.101 & 0.002 \\
using PRELINK (with handling arguments) & 0.097 & 0.003 \\
using PRELINK + POSTLINK (with handling arguments) & 0.096 & 0.003 \\
O2 PRELINK + POSTLINK & 0.097 & 0.004 \\
\bottomrule
\end{tabular}
\caption{\laurie{note that the last 4 rows are all statistically equivalent}The table shows the performance of yklua interpreter when it has been built
either using no pass sequence (i.e baseline) and after applying prelink, postlink,
or both prelink and postlink on yklua binary with 95\% confidence intervals (the rightmost column).
\laurie{For each of the definitions in the caption, be clear which row in the table they relate to, including capitalisation etc}
The interpreter is built without JIT enabled,
that is only yk is retrofited onto the interpreter but JIT is turned off. 
The first row shows performance when no passes are applied on yklua, and the,
second and third rows indicate performance when only pre or postlink passes are
applied on the interpreter. Third row indicates both passes being applied but
arguments inside functions are not handled. In fourth row we handle all
combination to receive compatible prelink and postlink pass sequence in our
genetic algorithm. Finally, last row represents yklua's performance when full
list of O2 (pre and postlink) and applied on the interpreter. This list however
is not compatible with yk meta-tracing system (i.e~ the list fails yk's tests)}
\end{table}

For first experiment we consider two ways to build lua binary:
\begin{itemize}
    \item{With the help of yk-config, we call this yklua}
    \item{With the help of traditional compilers such as gcc or clang and llvm opt tool.}
\end{itemize}

yk-config is a tool to apply yk specific compiler and linker flags on the
interpreter and other systems which use yk's meta-tracing system. \iti{why were
they added?}\laurie{yk-config makes configuring/compiling C interpreters easy. as part of that we decided that ``safe but slow'' was the order of the day, hence turning off most optimisations} For 1): We can build yklua with and without optimisations.
yk-config has support to add `cflags` (compiler) and `ldflags` (linker), and we
can use the prelink and postlink pass sequence we found via the genetic
algorithm, append them to cflags and ldflags, \iti{sentence does not join well} yk's clang (via ykllvm) applies the optimisations to
the yklua binary.

clang by default adds an `optnone` attribute to all the functions \laurie{only in -O0}, which makes
any optimisation we add ineffective. For this, yk-config already has
`disable-O0-optnone` (which works with `-O0` flag). 

Table I\iti{add label} (in attached doc) shows performance numbers for 30 runs of `db.lua` when
built with and without optimisations, with the yk meta-tracing system retrofitted
into the interpreter. The first column is with no optimisations
and the rest has \iti{have? depending on table entries} some or all optimisations turned on while building yklua
binary.

In row 2 - 7, there is clearly some performance improvement (by 1.5-2 times) when applying
the optimisation pass sequence on yklua binary when yk is retrofitted into the
interpreter and JIT is turned off. However, it is interesting that even when
we apply full O2 pass sequence on yklua binary*\footnote{we ignore the
Compatibility of these passes with yk for the time being \laurie{this needs to be in the main text, as really what you're doing here is defining a new hypothesis along the lines of: ``H2: Enabling all optimisation passes, even those which can interfere with correctness, improves performance further over the subset found by the GA``. you invalidate that hypothesis, then in the subsequent subsections start to explain why this has been invalidated.}} the performance
does not improve further. I propose \iti{believe?} applying full pass sequence should have
resulted in 3-4 times performance improvement from the baseline binary. Let's 
dig further why this might have happened.

\laurie{at this point readers are probably going to be confused because we're saying `yklua with -O2 isn't faster, but lua with -O2 is much faster`. it's very easy for them to misread that, so make sure that you've signposted it approrpriately}

\subsection{Apply full O2 to yk retrofitted and traditionally compiled
interpreter}

Next, I ran `db.lua` 30 times when lua binary is compiled with gcc in O2 \& O3
mode, and with lua is compiled with clang and optimisations were applied with
the help of llvm's opt tool. The latter was to verify what is the performance of lua
binary when optimisation pass sequences (pre+postlink) are applied with the help
of another tool instead of using clang's O2. Even with using opt tool to apply
passes, the performance of lua binary remains same as that we get with gcc's O2
pass sequence. This indicates that yk-config might be including some flags
which results in 3 times slowdown of yklua binary from lua binary. 

\begin{table}[H]
\begin{tabular}{@{}p{7.5cm}ll@{}}
\toprule
\textbf{Yklua binaries built without using yk-config} & \textbf{Mean} & \textbf{95\% CI} \\ 
\midrule
lua binary build with gcc O2 & 0.034 & 0.004 \\
lua binary build with gcc O3 & 0.030 & 0.003 \\
lua binary build using clang and opt (O2 mode) & 0.034 & 0.004 \\
\bottomrule
\end{tabular}
\caption{blah blah}
\end{table}

\subsection{Performance impact of yk-based flags with full O2 optimisation
turned on}

To verify if yk-based passes contribute to 3x slowdown to yklua I start to turn
off yk related flags. First, I turn them off individually and then in
combinations. The speedup of the interpreter when the passes are turned off
indicates that the flag (or combination of them) are contributing slowdown by
% ratio of: \scalemath{0.8}{$$\frac{\text{(performance of baseline)}}{\text{(performance of row
% n)}$$} 
The numbers in Table proves H2 to be true. The majority of overhead is caused when the yk flags
are used in combination with row 14 causing maximum slowdown.

\begin{table}[H]
\begin{tabular}{@{}cllr@{}}
\toprule
\# & \multicolumn{1}{c}{Flags turned off (O2 passes are set)} & \multicolumn{1}{c}{Execution Time} & \multicolumn{1}{c}{95\% CI} \\ 
\midrule
1 & baseline (all flags are turned on) & 89.9ms & 0.002 \\
2 & —yk-no-fallthrough & 76.1ms & 0.002 \\
3 & —yk-block-disambiguate & 78.7ms & 0.00189 \\
4 & —yk-patch-control-point & 84.4ms & 0.002 \\
5 & —yk-insert-stackmaps & 85.8ms & 0.002 \\
6 & —yk-split-blocks-after-calls & 82ms & 0.002 \\
7 & —yk-no-fallthrough + —yk-block-disambiguate & 68.4ms & 0.002 \\
8 & —yk-no-fallthrough + —yk-block-disambiguate + —yk-patch-control-point & 63.9ms & 0.002 \\
9 & —yk-no-fallthrough + —yk-block-disambiguate + —yk-insert-stackmaps & 58.4ms & 0.002 \\
10 & —yk-no-fallthrough + —yk-block-disambiguate + —yk-split-blocks-after-calls & 66.8ms & 0.00189 \\
11 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-patch-control-point + —yk-insert-stackmaps\end{tabular} & 55.1ms & 0.002 \\
12 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-patch-control-point + —yk-split-blocks-after-calls\end{tabular} & 64.2ms & 0.002 \\
13 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-insert-stackmaps + —yk-split-blocks-after-calls\end{tabular} & 58.2ms & 0.002 \\
14 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-patch-control-point + —yk-insert-stackmaps +\\ —yk-split-blocks-after-calls\end{tabular} & 54.5ms & 0.003 \\
\bottomrule
\end{tabular}
\caption{blah blah}
\end{table}

\subsection{Performance retrieval by improving yk based flags}

Now that I have shown that yk-based flags contribute upto 1.5 times slowdown in
yklua the next task is to figure out how much performance can be retrieved back
by optimising some (or maybe all) flags above. 
The best targets to fix are yk-insert-stackmaps pass and yk-no-fallthrough,
the latter being a little more difficult to fix because ... \iti{ask Edd}.

Currently in yk-insert-stackmaps stackmaps is inserted before every function
unless it's an intrinsic \iti{some missing word here? Otherwise remove 'an'}. However, stackmaps are not needed before calls to
unmappable functions, as these are not traced and thus never need
deoptimisation. This might claw back a few ms for this flag. 

With code changes and by adding test case in this\iti{add link to the PR}PR, I
show the performance improvement of yklua binary from \iti{add number} to
\iti{add number}.

Similarly, stackmap is not needed for functions which do not have any conditions
inside them, this is because no condition means there are no gaurds which means
the function does not need to be traced or deoptimised. This is a bit more
difficult to implement because we will need to store which functions have no
condition inside them. This can be targetted as a future work.

As for the yk-split-blocks-after-calls pass, if we adjust jitmodbuilder a bit,
we could also refrain from splitting blocks after calls to unmappable functions.
But that could turn out to be a bit fiddly in jitmodbuilder as the splitting
helps us to know when to start/stop outlining. Jitmodbuilder is ...\iti{ask
lukas}
\iti{Can we work out another way to solve this?}

% \textbf{Hypothesis I: The sequence order of the optimisation passes is non-critical for
% obtaining successful optimisation pass sequence (for the target oracle).}
%
% The binary split algorithm described in section \iti{add section label} doesn't
% inherently maintain the original order of the optimization pass list during its
% divide-and-conquer process. When it splits the list of passes to test subsets,
% it uses a random shuffle before dividing them into two halves. This shuffling
% means that the original order of the passes is not preserved in the testing
% process.
%
% The random shuffle is intended to explore different combinations of passes in an
% attempt to quickly find a working set of passes. However, this approach does not
% consider the order in which passes are applied, which might be significant
% depending on the context. In compiler optimization, the order in which
% optimization passes are applied can affect the outcome of the optimizations.
% Some optimizations may need to be applied before others to be effective or to
% avoid negative interactions. Parallising the program

% \iti{add the pseudocode here.}
%
% \textbf{Hypothesis II: Elimination of stochastic elements from the divide-and-conquer
% strategy should give us at least a successful pass sequence for the target oracle
% quickly.}

% We start with stochastic binary-split algorithm to find pass list, but soon
% discover we cannot find single pass sequence which is correct for the target
% oracle. Correctness means that the pass sequence should successfully runs yk's
% test  as well as build yklua and run its benchmarks. 
% We discover this was because pass order matters in llvm's optimisation
% pipeline. What that means is as discussed in previous section, some passes
% depends on others to collect information or do some work/computation before they 
% can successfully run their optimisations and/or computations. We could maybe
% solve this by introducing some passes multiple times but that doesn't gaurante
% .... {from edd's comment in the binary split code}.

% \textbf{Hypothesis III: Using stochastic genetic-algorithm can result in better
% optimisation pass sequence for the target oracle compared to the conventional
% methods.}

% What we mean here is the pass list is bigger, which may result in faster
% interpreter. We can tweak the algorithm to mutate and crossover to choose
% better pass sequence while also including randomness to eascape local optima as
% described in section xyz. This is very difficult with some other methods which 
% can not escape local minima like global search as it may not check all the
% combination (as with other method mentioned in guided GA paper). 
%
% Speed of finding successful pass sequence was better, that means in just
% initial generation we were able to find passes 

% \textbf{Hypothesis IV: Running more generations results in finding the optimisation pass
% sequence which converges to global optima easily and making the interpreter faster
% by 3-4x from baseline}
%
% \textbf{Hypothesis V: The integration of high-overhead optimization passes within ykllvm
% contributes to its performance disparity when compared to using standard llvm clang.}
%
% \textbf{Hypothesis VI: Disabling optimisation passes when using yk incurs
% significant performance detriments}
%
% \textbf{Hypothesis: Applying Prelink + Postlink passes should have better performance than either
% just applying Prelink or Postlink pass sequence on the interpreter}
%
% \textbf{Hypothesis VIII: We can improve performance of interpreter when compiled with
% ykllvm by optimising yk-specific optimisation passes}
%

    % parallesing the algorithm for speed-up and faster testing. 
    % Speed up from parallesing
    %
    % Hypothesis III: Deciding upon oracle
    %
    % Hypothesis IV: aiming for performance
    %
    % obstacles: aiming for correctness (yk's tests), why we disabled some tests
    %
    % Obstacles: wall created by ykllvm and yk-config
    %
    %
    %
    % why these passes are high penality?
    %
    % is there any way to solve it?
    %
    % exaplain yk-config in methodology section \iti{IMPORTANT!}
    %
 % Dynamic languages are more expensive to compile than statically
 % typed languages. Since, traditional compilers don't have runtime
 % informations, they need to emit code that can handle 
 % all type of combinations that are possible at the runtime. This
 % makes performance of dynamic language much slower. JIT compilers
 % overcome this problem by recording and using runtime informations 
 % to optimise most executed paths. JIT compilation is 
 % combination of two traditional method : ahead of time compilation
 % (AOT) and interpretation. So
 % However, retrofitting meta-tracing system such as yk into
 % existing C-interpreters brings challenges to their performance. The aim of
 % my PhD is find ways to speed up retrofitting meta-tracing system (i.e~yk)
 % into existing interpreters, In this report, I first find optimisation pass
 % sequence (from tradition passes such as O2) using recursive and stochastic
 % method, the end result, when JIT is turned of is performance improvement by
 % 1.5-2 times for yklua, a C-based interpreter which uses yk. Next, I dig
 % deeper into reasons for runtime performance overhead introduce by yk in
 % C-based interpreters. 

\section{Related Work}

\section{Conclusion and Future Work}

Conclusion: In this report, I prove that we can win back some of the lost
performance when we retrofit a meta-tracing system such
as yk to a C based interpreter. Although, the experiments above are run only on a
couple of benchmarks, it shows the possibility of improving yk based interpreters. We
can expand the oracle suite of genetic-algorithm to include more comprehensive
benchmarks and extend the above experiments.

Future work: Future work includes improving yk-based flag such as
yk-insert-stackmaps, yk-split-blocks-after-calls, and yk-no-fallthrough.
\iti{add about yk-no-fallthrough, what it does and how we can improve this}

Another possible hypothesis, which might be worth testing is whether we can
create a partial version of the disabled optimisations, which were disabled because they interfere
with basic block mappings. This would require change(s) in llvm or to write our own
optimisation passes which perform most of the optimisations but do not mess with
the mapping. One way could be to tweak the passes so that only optimisations
within the basic blocks are allowed (and those which work across the basic
blocks are not allowed).


% \bibliographystyle{plain}
% \bibliography{bib}

\end{document}
