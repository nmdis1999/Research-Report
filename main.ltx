%&main_preamble
\endofdump
\begin{document}

\maketitle

\begin{abstract}
    \noindent
     JIT compilation gives dynamic languages speed of compiled code with
     flexibility of interpretation. Tracing JIT is a variation that records
     instructions of the most executed path in a trace and uses it instead of the
     original code in hot paths. But tracing JITs are fairly hard to implement. In
     meta-tracing, language interpreters are built on top of a common tracing
     JIT compiler, removing the need to re-implement tracing JIT for every
     language. This separates language implementation concerns from JIT
     compilation. The yk meta-tracing system turns existing C interpreters into
     tracing JIT compilers. However, yk pessimistically turns off all
     optimisations, as not all compiler optimisations are safe to use with yk or
     meta-tracing in general. This significantly slows down the non-JITed
     interpreter. I am investigating regaining as much performance as possible
     for such interpreters. This report covers progress for the above problem
     to date and plans for the future. 
\end{abstract}

\section{Introduction}
Dynamic languages such as Python and JavaScript are very popular because of
their ease of writing programs and expressiveness. They are used to write small
scripts as well as large and complex programs such as web applications. However,
dynamic languages tend to be quite slow to run when compiled~\cite{pypy}.
In dynamic languages such as Python, the type information may vary at runtime,
which means the compiler cannot rely on one type to transform operations into
machine code and thus will have to generate all possible combinations. This can
be very slow. JIT compilers overcome this problem by recording and using runtime
information to optimise the most executed paths (also known as ``hot'' paths).
JIT compilation is a combination of two traditional methods: ahead of time
compilation (AOT) and interpretation.

Tracing JIT is a kind of JIT compilation. In tracing JIT compilation, the VM
identifies frequently executed, ``hot'' program paths at runtime. Once a hot
path is detected, the VM records all instructions in this path into a trace
until execution loops back to the first instruction of this path. The trace is
then compiled and the subsequent iterations of this path are executed using the
compiled trace instead of the original code. But developing custom tracing JITs
is a hard undertaking. The concept of tracing optimisations was initially
explored by Dynamo Project~\cite{dynamo}, the project was successful in using
tracing information at runtime to optimise most executed paths. This technique
is also used in Mozilla's TraceMonkey JavaScript VM~\cite{tracemonkey}and in
Chrome's V8 through their Chromium tracing system~\cite{v8jit}.
 

With meta-tracing systems such as yk, language implementers do not need to
implement their own tracing JITs; instead, they can use yk meta-tracing system
and the tracing JIT compilation will automatically be embedded in the language
implementation(s). yk traces the execution of the interpreter instead of tracing
the program executed by the interpreter. However, not all compiler optimisations
are safe to use with meta-tracing systems like yk because they use a hardware
tracing mechanism to collect the program's traces. This creates a problem for
hardware meta-tracing systems because some backend optimisations interfere with
basic block positions and thus mapping their address from AOT to JIT compilation
and vice-versa becomes challenging. Therefore, yk pessimistically turns off all
optimisations by default. This results in a much slower interpreter than the
baseline (i.e.~interpreter compiled without the meta-tracer.) My work focuses on
minimising the performance overhead of the yk meta-tracer and thus speeding the C
interpreter to as much an extent as possible. In later sections, I'll discuss
the work done so far on this and future directions.

\begin{figure}[ht!]
    \label{figure:example}
    \centering
    \scalebox{0.8}{
    \includegraphics[scale=0.5, width=\linewidth]{images/multilang.pdf} 
    }
    \caption{Example of how dynamic languages need individual JITs without
    meta-tracing system, while with meta-tracing system multiple languages can
    use the same JIT without having to write their own tracing JITs}
\end{figure}

In this report, I show that it is possible to minimise the performance overhead
when the yk meta-tracing system is retrofitted in a non-JITed C interpreter. Which
means that the performance of a non-JITed interpreter compiled with the yk subsystem
should be closer to the performance of the same interpreter when compiled with
non-meta-tracing compiler.
The main contributions of this report are:
\begin{itemize}
    \item{A genetic algorithm to find llvm optimisation passes that do not
        break yk's tests and improve the performance of yklua, a C interpreter.}
    \item{Identfying and fixing the cause of overhead when using yk to build
        yklua}
\end{itemize}
Through this work, I aim to bridge the gap between the achievable
performance\footnote{Achievable performance is defined as convergence of
execution time of meta-tracer compiled interpreter to that of traditionally
compiled interpreter}  of meta-tracing JIT compilation, and its current
practical performance for the  C interpreter.

\subsection{Defining the problem}
For my work done in this report, I lay out the hypothesis:

\textbf{H1} \hspace*{0.5cm} There is a subset of existing compiler optimisation passes
that is compatible with meta-tracing system and meaningfully improves the interpreter's
performance.
\\
To validate this hypothesis, I began by running a divide-and-conquer algorithm
to find a pass sequence compatible with the meta-tracing system. Compatibility
is defined as the successful passing of the meta-tracer system's test cases and the
pass sequence's ability to build and run the interpreter. However, while this
method found a compatible pass sequence, it did not improve the interpreter's
performance and in some cases, even made it worse. Later, I will present a
better approach using a stochastic genetic algorithm, which not only found
multiple compatible pass sequences but also yielded sequences that began to
improve the interpreter's performance.

The report is structured as follows: Section \ref{sec:background} is the background,
the next section i.e~Section \ref{section:3} addresses interpreter's performance in
meta-tracing system. This is followed by Section \ref{section:4} and
\ref{section:5} describing the experimental setup and what methodology is used to
conduct the experiments. Section \ref{section:6} discusses results, and finally I
present Section \ref{section:7} for addressing related works and wrapping the
report with Section \ref{section:8} i.e~conclusions and future Work.

\section{Background}
\label{sec:background}

Tracing refers to the process of capturing information about the execution of a
program. Tracing JITs use the technique where they collect/record information
about most executed (or ``hot'') paths via traces of the program and later at
runtime, uses the information to perform optimisations and make inlining
decisions\cite{tracing-jit}. A trace is a sequential path which follows only one
of the many possible directions through the code. To ensure correctness, the
trace contains a guard at every possible point where the path could have
followed another direction, such as at conditions and indirect or virtual calls.
When generating the machine code, every guard is turned into a quick check to
guarantee that the path we are executing is still valid. If a guard fails, the
tracing JIT immediately quits the machine code and continues the execution by
falling back to interpretation (i.e~AOT compilation).

\begin{figure}[ht!]
    \label{figure:example}
    \centering
    \scalebox{0.8}{
    \includegraphics[scale=0.5, width=\linewidth]{images/program_and_trace.pdf} 
    }
    \caption{Example of how trace of a program looks like(inspired from
    Wikipedia). Left hand side is a python program and right hand side is image
    of the program's trace}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{images/trace.pdf}
    \caption{\small In tracing JIT (which is the core of meta-tracing system), conditions are turned
    into guards. The guards are responsible for checking whether the conditions are
    being met (i.e~they are true) or not. If the conditions are true, and a trace of it is
    already compiled then instead of interpreting the condition again, the compiled trace
    is used by the interpreter.}
    \label{traces}
\end{figure}

Meta-tracing is a system where the tracing JIT is applied at one level down,
which means instead of tracing the execution of user program, the meta-tracer traces the 
execution of the interpreter\cite{meta-tracing}. 

\begin{figure}[htbp]
    \centering
    \scalebox{0.5}{
        \includegraphics[scale=0.01, width=\linewidth]{images/yk_metatracer.pdf} 
    }
    \caption{Figure shows how ykllvm introduces control-point and compile
    (yk)lua binary. The control-point is necessary so that the meta-tracing
    system can identify at what point to start recording tracing. ykllvm also add
    some additional data such as builtin tracer and LLVM IR into the yklua binary}
    \label{traces}
\end{figure}

\begin{figure}[htbp]
    \centering
    \scalebox{0.8}{
        \includegraphics[scale=0.5, width=\linewidth]{images/aot_and_jit.pdf} 
    }
    \caption{\small The yk meta-tracing system uses ykllvm
    to compile the interpreter's source code into a binary, which is called ahead of
    time compilation (or AOT compilation). This binary has a built-in tracer, a jit
    compiler and LLVM IR. The LLVM IR is then used to get the tracing information
    from the binary. The binary is generated via ykllvm and is used to interpret the
    user program. If the program has a loop, where a loop means the tracer comes
    back to the point it started collecting trace from, then the builtin tacer
    records this information. A trace starts getting collected when the meta-tracing
    system detects control-point in the interpreter, which can be inserted by the
    user initially. This control-point is later converted into a yk specific control
    point. After collecting the full trace, the trace is compiled which is called
    just-in-time compilation (or JIT compilation) and then executed as a part of the
    meta-tracing system. If, while executing the user program, both loop and
    complete traces are detected (which can be a later iteration of the loop) then
    instead of recording and compiling the trace again, the already compiled trace
    is used for execution.
    }
\label{traces}
\end{figure}

\begin{samepage}
yk uses hardware tracer\footnote{The hardware tracer is Intel PT based, Intel
Processor Trace (Intel PT) is a feature available in some Intel processors that
provides a hardware-based tracing mechanism, enabling low overhead collection of
program execution traces.}.And Intel PT can only store a stream of basic blocks
and give their virtual addresses~\cite{intel-pt}. So, block mapping is used in
hardware tracing to get from a traced virtual address to a file offset in the
binary, to a LLVM IR block. The yk meta-tracing system depends on loader API and
a special LLVM section to do this. This section is emitted just before backend
optimisation.
\end{samepage}

At JIT time, the tracing interpreter records the basic blocks (jumps). When
the tracing interpreter switches back to interpretation (when a guard fails), it
needs to map the information back at the AOT basic block; this is where LLVM IR is
used. Meta-tracers such as yk need to do this with a hundred per cent accuracy.
The JIT compiler relies on the traces to identify hot paths in the program for
optimisation\cite{hot-trace}. If the mapping from the traced addresses to the LLVM IR blocks is
inaccurate, the JIT compiler might optimise the wrong code paths or apply
optimisations based on incorrect assumptions. This can lead to the generation of
incorrect or inefficient JIT-compiled code, affecting the program's correctness
and performance.

However, some llvm optimisation passes mess with the basic-block mapping. One
such example is when basic blocks are merged, some blocks may be left
unaccounted for in the trace. Additionally, llvm applies some passes at different
stages of the pipeline, even after the final IR has been generated. This again
interferes with mapping and creates problems for the meta-tracer. To avoid this
problem, the yk meta-tracing system turns off all the optimisations by default,
which slows down the interpreter significantly even without the JIT being
enabled.


\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{images/block-merge.pdf}
    \caption{\small High Level IR and Machine IR side by side after backend
optimisation. After some backend optimisations some basic blocks get merged into
one. And the previous basic block which contained some operations are either
empty or deleted. This creates mapping issue in meta-tracing systems.}

    \label{traces}
\end{figure}

Modern compilers have a huge number of optimisation passes available. These
optimisation passes target unique segments of a program, such as transforming a
function, basic block(s) or whole program\cite{llvm-pass}. Further, these optimisations can be
applied at different stages of compilation pipelines. The sequence of passes tries
to retain the semantics of the program while potentially improving the program's
performance. There are two types of passes: Analysis and Transformation. Both
of these passes work together: the analysis passes collect information about the
program and transformation passes are responsible for using the collected
information and changing the program. Examples of optimisation levels provided by
modern compilers are: -O0, -O1, -O2, -O3, and -Og. The two major compilers gcc and
Clang have 200 and 100+ optimisation passes respectively; their sequence is
called an optimisation sequence. By default, without an optimisation level, all of
the passes are turned off and an expert can choose to turn on and off passes
accordingly~\cite{kulkarni}.

\subsection{LLVM Project}
LLVM is a set of compiler and toolchain technologies and has its own
Intermediate Representation (LLVM IR). This language-independent IR helps LLVM
compilation to be target and source independent. The LLVM IR is optimised
through a sequence of LLVM passes. Passes are an operational unit of the IR which
can:
    \begin{itemize}
        \item{mutate the IR}
        \item{perform some computation about the IR}
        \item{or just print something}
    \end{itemize}

The scope of each pass is a unit of an IR; it is what the pass is going to operate on. This
can be :
\begin{itemize}
    \item{a module: LLVM module is a top-level structure that represents a
        single  unit of code which is processed together. It includes global
        variable(s), function declaration(s), and implementation(s).}
    \item{a function: LLVM function is a self-contained unit of execution within
          a module. It corresponds to a function in the source code and contains
          a list of aruguments, a basic block, and a symbol table.}
    \item{a basic block: LLVM basic block is a linear sequence of instructions
          within a function, and has a single entry and exit point. They are
          used to represent the straight-line code sequence that makes up a function's
          body.}
    \item{an instruction: an LLVM instruction is the smallest unit of execution
         in the LLVM IR, representing a single operation within a basic block.
         Instructions can perform a wide range of operations, including arithmetic,
         memory access, logical operations, and control flow changes. Each instruction
         produces a result and can have zero or more operands, which are either constants
         or the results of other instructions.}
\end{itemize}
These passes are categorised into analysis, transformation and utility passes
depending on whether they perform computation on the IR, mutate the IR, or do
some other non-categorised task.

\subsection{LLVM's optimisation pipeline}
Classically, compilers are split into three parts: frontend, middle-end and the
backend. The frontend deals with language-specific analysis i.e~operations such as
parsing, type checking and so on. The target-independent optimisations are
performed in the middle-end. And finally, the backend applies the target-specific
optimisations and emits the machine code.

Although languages such as C, C++, and Rust etc. have their own frontends,
but they rely on LLVM for middle and backend optimisations.

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, 
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]
Frontend                   LLVM
/---------\               /-----------------------------------\
|  clang   |              |                                   |
|  rustc   | --(LLVM IR)->| Middle end --(LLVM IR)--> Backend | --> Machine code
|   ...    |              |                                   |
\---------/               \-----------------------------------/
\end{Verbatim}
\end{tcolorbox}

\subsubsection{Type of optimisation pipelines}
For H1, we are mostly concerned with LLVM's middle-end optimisation
pipeline. LLVM has three kinds of optimisation pipelines: the default (non-LTO)
pipeline, ThinLTO pipeline, and FatLTO pipeline.

\subsubsection{Default Pipeline}
The default pipeline optimises each module separately, without any special
knowledge about other modules. ``Module'' here refers to ``single source file''
in C/C++ and a single ``codegen unit'''in Rust language.

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, 
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]
Module 1 --(Optimise)--> Module 1'
Module 2 --(Optimise)--> Module 2'
Module 3 --(Optimise)--> Module 3'
\end{Verbatim}
\end{tcolorbox}

\subsubsection{ThinLTO Pipeline}

The LTO pipelines are split into a pre-link and a post-link optimisation
pipeline\cite{lto-prepost}. After the pre-link pipeline, ThinLTO performs some
lightweight cross-module analysis, such as importing certain functions from
other modules to make them ready for inlining. The post-link optimisation
however, works only on individual modules. Pre-link optimisations simplify the
modules for optimisations, while contrary to the name, post-link optimisations
happen during the link-time stage and perform both module simplification and
optimisations through passes.

\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, 
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]
                              cross-import
Module 1 --(ThinLTO pre-link)------\-/-----(ThinLTO post-link)--> Module 1'
Module 2 --(ThinLTO pre-link)-------X------(ThinLTO post-link)--> Module 2'
Module 3 --(ThinLTO pre-link)------/-\-----(ThinLTO post-link)--> Module 3'
\end{Verbatim}
\end{tcolorbox}

\subsubsection{FatLTO Pipeline}
Finally, after the pre-link step, FatLTO merges all modules together
and then runs the post-link optimisations on a single large module:

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, % Thickness of the box border
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]
             begin{Verbatim}[fontsize=\scriptsize,formatcom=\bfseries]
                                 merge
Module 1 --(FatLTO pre-link)-------\
Module 2 --(FatLTO pre-link)---------------(FatLTO post-link)--> Module'
Module 3 --(FatLTO pre-link)-------/
\end{Verbatim}
\end{tcolorbox}

While LTO stands for ``link-time optimisation'', it is not necessarily performed
by the linker. For example, rustc performs LTO as part of its
compilation process. 
LLVM’s optimisation pipeline looks like:

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, % Thickness of the box border
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]

|-------------------------- default ----------------------------|

|------------- pre-link --------------|
|------------------------- post-link ---------------------------|

/-------------------------------------\
|       module simplification         |
|-------------------------------------|   /---------------------\
|                                     |   | module optimisation |
|                    cgscc            |   |---------------------|   /---------\
| /-------\ /-----------------------\ |   |                     |   | backend |
| | early | |       inlining        | |   | vectorization       |   \---------/
| |cleanup| |function simplification| |   | runtime unrolling   |
| \-------/ \-----------------------/ |   \---------------------/
|                                     |
\-------------------------------------/
\end{Verbatim}
\end{tcolorbox}


In the LLVM LTO pipeline, module simplification includes pre-link optimizations
such as initial cleanup, inlining of functions, and call graph optimisation to
refine the modules for additional optimization processes. Following
this, module optimization during the post-link phase implements optimizations
like vectorization and dynamic loop unrolling to improve the program's overall
performance. Finally, the backend phase compiles the machine code,
integrating register allocation and optimizations tailored to the specific
architecture, resulting in a faster executable code.

For the yk meta-tracer, we work with FatLTO because it merges multiple modules into
one giving us full IR. yk can potentially work with ThinLTO and the choice of
going with FatLTO is just for the sake of convenience.

\section{Addressing interpreter performance in a meta-tracing system}
\label{section:3}
As mentioned earlier, meta-tracing systems which use hardware tracing mechanism
such as yk cannot enable all the optimisation passes by default, as they
reorganise the basic-blocks and thus mess the mapping when switching from JIT
mode to AOT mode. Therefore, we need to carefully choose an optimisation pass
sequence which does not interfere with basic block mapping while also improving
the interpreter's performance.

Such a problem is called an optimising phase ordering problem. Optimising phase
ordering is a long-standing problem with substantial work done on
it~\cite{guidedGA, kulkarni, genetic-algo}. However, the optimisation pass sequence which might work for one
system may not perform well or even work at all for another. Finding the right
optimisation pass sequence depends on the algorithm and the oracle used. The
algorithm can be a recursive or stochastic one, while an oracle is a test set which
establishes an initial constraint. Unless the pass sequence satisfies all the
constraints, it will not be selected in the algorithm. The oracle can contain
constraints such as tests as well as the execution time of the program.

Since some optimisation passes depend on others to enable a program's
transformation, choosing the right sequence (and order of passes) is important for
a program's performance either with respect to execution time, code size or both.
The pass sequence is a subset of passes from the larger set, while the order of
passes means in which position the passes are placed with respect to
one another. However, such a high number of passes (around 90) makes our
search space large (if we try all sequences irrespective of ordering we would
have to try $2^{90} - 1$ possible combinations!).

In this report, I start with a divide-and-conquer algorithm to find a
meta-tracing compatible pass sequence and later move on to a genetic algorithm
for the same. Genetic algorithm is a well-known class of stochastic algorithms
which is adopted based on the theory of evolution. This algorithm has been
used by several researchers to solve phase-ordering problems~\cite{guidedGA,
kulkarni, genetic-algo}.

Using a genetic algorithm allows us to find a pass sequence for our oracle in
acceptable time, where acceptable time is finding a compatible pass sequence
within a couple of hours if not minutes because of the vast search space. Further,
the genetic algorithm is not only able to find a pass sequence that is
compatible with the meta-tracing system, but the pass sequence also ends up
improving the C interpreter's performance. 


\section{Experimental Setup}
\label{section:4}
The experiments were conducted on a dual-socket Intel Xeon Platinum 8452Y
system, featuring an x86\_64 architecture with 72 total cores distributed across
two NUMA nodes, operating between 800 MHz and 3200 MHz.

\section{Experiments}
\label{section:5}
In order to test H1, I conducted experiments\footnote{Pull request with my
divide-and-conquer algorithm: https://github.com/ykjit/yk/pull/829 and
genetic-algorithm: https://github.com/ykjit/yk/pull/900} by first writing a
divide-and-conquer algorithm. And although this algorithm gave a pass sequence
which did show some performance improvement of the interpreter, it did
not find a pass sequence which was compatible with the yk meta-tracing system.
In the experiments done below, we use yk as the meta-tracing system and yklua as
the C interpreter being optimised.

\subsection{Methodology}

\subsubsection{Divide-and-conquer algorithm for phase ordering problem}
\scalebox{0.65}{
\label{dac-algo}
\begin{tcolorbox}
    \begin{algorithmic}[1] % The [1] argument enables line numbering
    \small
    \State Define structures for \textbf{Pass} and \textbf{PipelineConfig}
    \State Fetch all available optimisation passes

    \Function{test\_pipeline}{PipelineConfig}
        \State Execute the pipeline with the specified configuration
        \State Log the results
    \EndFunction

    \Function{attempt1}{passes}
        \State Test each pass in isolation
        \State Log which passes succeed and which fail
    \EndFunction

    \Function{attempt2}{passes}
        \State Start with an empty set of okay passes
        \State Recursively test combinations of passes
        \If{a combination is successful}
            \State Add it to the set of okay passes
        \EndIf
        \If{a combination with more than one pass fails}
            \State Split and test subsets
        \EndIf
    \EndFunction

    \Function{main}{}
        \State Fetch all passes
        \State Perform initial testing of passes in isolation (\textbf{attempt1})
        \State Perform combined testing of passes (\textbf{attempt2})
    \EndFunction
    \end{algorithmic} 
\textbf{Algorithm 1:} this algorithm has two functions, attempt1 tries each pass
individually to see which passes succeeds and which ones fail in isolation. It
takes a total of 36 hours to run attempt1. In attempt2, the algorithm starts with
an empty list of ok passes, recursively tests combinations of passes starting with
a full list and then splits the list in two if the initial list fails and only
adds the subsets which succeed to the okay pass list. The run time for attempt2
is between 9-10 hours. The initial version of the algorithm used to reshuffle
the list as well to make it stochastic, however, I noticed that we received no
pass sequence which was compatible with yk as well as improved yklua's
performance. Removing stochastic elements helped me to get at least a pass
sequence which was compatible with yk, but failed to provide performance
improvement to yklua. This is because some LLVM optimisations depend on others
to give them information. So if the order of pass sequence is not right some 
optimisation might not work because their dependencies are not present before
they are processed.
\end{tcolorbox}}

\subsubsection{Genetic Algorithm for phase-ordering problem}
A genetic algorithm is a stochastic search algorithm which borrows the concept of
evolution from nature. This algorithm starts from an initial list (often
represented in a bitwise form) called entities and explores different
combinations. This is done by combining elements from two entities by way of
crossover and mutation into a third entity, which may represent a new point in
the vast search space, that otherwise would have been left unexplored by search
methods like divide-and-conquer.

Some terminology for the genetic algorithm:

\begin{itemize}
\item{Entity: In genetic algorithms, an entity (often also called an
``individual''' or ``chromosome'') represents a possible solution to the
problem being addressed. An entity is typically encoded as a string or
array, where the structure and content of this encoding depends on the
specific problem. Each entity has a set of properties (or ``genes'')
that can be altered and combined with properties of other entities
during the algorithm's processes. The fitness of each entity, determined
by a fitness function, indicates how good a solution it is with respect
to the problem.}

\item{Crossover: Crossover (also known as ``recombination'') is another genetic operator
used to combine the genetic information of two parent entities to generate new
offspring entities. The process involves swapping sections of the parents'
encoding to produce one or more children. The rationale behind crossover is to
take the good characteristics of each parent and combine them to produce better
offsprings, potentially leading to better solutions in subsequent generations.
There are various crossover techniques, such as single-point crossover,
multi-point crossover, and uniform crossover, each differing in how and which
parts of the parent entities' genes are exchanged.}

\item{Mutation: Mutation is a genetic algorithm operator used to maintain genetic
diversity from one generation of a population of entities to the next. It is
analogous to biological mutation. The process involves making random changes to
individual genes of an entity. This can involve flipping bits in a binary
string, changing numbers in an array, or altering any part of the entity's
encoding. Mutation introduces new genetic structures in the population by
creating variants of existing entities, which helps in exploring new solutions
and prevents the algorithm from getting stuck in local optima.}
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.5]{images/genetic_algo.pdf}
    \caption{\small Figure retrieved from~\cite{guidedGA}, a parallel genetic algorithm used to find an optimisation pass
        sequence compatible with yk and yklua. Initially, it randomly generates
        a population of passes, where each entity is a binary array indicating
        whether a specific pass is active (1) or inactive (0). The main parallel
        component is the fitness function, which is a value associated with
        every entity indicating how ``optimal'' a pass sequence is. Optimal means
        a pass sequence which succeeds all yk's tests while also minimising
        execution time of the interpreter. Each entity's performance is tested
        in parallel, speeding up the process significantly. The algorithm uses
        tournament selection (reproduction operator) to pick parents, which are
        then recombined and mutated to form new entities, iterating through
        generations to improve pass sequence combinations. This evolutionary
        process continues until the optimal set of passes, leading to the best
        interpreter performance, is identified.}
    \label{genetic-algo}
\end{figure}


\scalebox{0.80}{
\label{algorithm:}
\begin{tcolorbox}
    \begin{algorithm}[H]
        \begin{algorithmic}[1] % The [1] argument enables line numbering
        \small
        \State Define structures for \textbf{Pass} and \textbf{PipelineConfig}
        \State Fetch all available optimisation passes

        \Function{test\_pipeline}{PipelineConfig $pl$}
            \State Execute the pipeline with configuration $pl$
            \State Log the results
        \EndFunction

        \Function{encode\_passes\_randomly}{Pass $root\_passes$}
            \State Recursively set `on` flag of each pass to 0 or 1 randomly
            \State Ensure at least one subpass is enabled if parent is enabled
            \State Return modified $root\_passes$
        \EndFunction

        \Function{decode\_passes\_to\_string}{Pass $passes$}
            \State Generate string representation of enabled passes and hierarchy
            \State Exclude passes with `on` set to 0
            \State Return the string representation
        \EndFunction

        \Function{evaluate\_fitness}{Pass $passes$, ...}
            \State Decode passes string into PipelineConfig $pl$
            \State Call $test\_pipeline(pl)$ to get execution time
            \State Add fitness score (execution time) to shared list
        \EndFunction

        \Function{tournament}{list $population$, list $fitness$, float $sp$}
            \State Select two individuals randomly
            \State Choose fitter one with probability $sp$ as parent
        \EndFunction

        \Function{crossover}{Pass $parent1$, Pass $parent2$}
            \State Create child passes by swapping random passes between parents
            \State Return child passes
        \EndFunction

        \Function{mutate}{Pass $entity$, float $mutation\_rate$}
            \State Recursively flip `on` flag of each pass with probability $mutation\_rate$
            \State Return mutated pass
        \EndFunction

        \Function{genetic\_algorithm}{string $parsed\_str$, ...}
            \State Create initial population of randomly encoded passes
            \For{$generation$ in $1$ to $generations$}
            \State Evaluate fitness of each entity in parallel
            \State Select elites (good fitness)
            \State Apply crossover and mutation to create new generation
            \EndFor
            \State Return best entity (set of passes)
        \EndFunction

        \Function{main}{}
            \State Set up temporary directories
            \State Get available passes using `get\_all\_passes`
            \State Run genetic algorithm to find best passes
            \State Decode best entity and print final passes
        \EndFunction
        \end{algorithmic}
    \end{algorithm}
\end{tcolorbox}}

\subsection{Algorithm's speed up after parallising}

The first version of the genetic-algorithm was not parallised and it took 4-5
hours to run 10 generations. I optimised the algorithm to calculate the fitness function for each entity 
in parallel which reduced the run-time for 10 generation from 4-5 hours to 15
minutes. This improved my efficiency in running experiments and finding patterns in
successful and non-successful pass sequence. \footnote{commit with this change: https://github.com/ykjit/yk/pull/900/commits/e597774190c2856cd19328c94401f44ea8bf3a8b}

\section{Results}
\label{section:6}
\subsection{Effects of applying meta-tracing compatible pass sequence on the yklua
interpreter}

Table \ref{table:preandpostlink} shows performance numbers for baseline as well as yklua binaries
when we apply prelink, postlink or both flags on the binary. Row I is when no
pass sequence is applied, that is the baseline binary. Rows II and III are when
either just prelink or postlink passes are applied. In Row IV, I applied both
prelink and postlink passes and so on. Handling arguments mean handling the
function argument as a separate pass.
Example: Some passes are in form of a function such as $function(a,b,c,..)$. For
rows V and VI, I treat every argument as a seperare pass like $function(a),
function(b), function(c), function(a,b), function(a,c)...$. In the last row,
that is Row VII, I apply full pass sequence for -O2 on yklua. Although this
is the only pass sequence in the Table \ref{table:preandpostlink} which does not succeed all
of yk's tests, I still included it in the table for the next hypothesis.

Notice that the last four rows have statistically equivalent performance, this
indicates that there is a wall created by some yk specific passes which might be
limiting more performance gains. 

\begin{table}[H]
\centering
\begin{tabular}{@{}p{7.5cm}ll@{}}
\toprule
\textbf{Yklua binaries built using yk-config} & \textbf{Mean} & \textbf{95\% CI} \\ 
\midrule
NO PRELINK + POSTLINK  & 0.140 & 0.002 \\
using just PRELINK & 0.107 & 0.005 \\
using just POSTLINK & 0.105 & 0.002 \\
using PRELINK + POSTLINK (without handling arguments) & 0.101 & 0.002 \\
using PRELINK (with handling arguments) & 0.097 & 0.003 \\
using PRELINK + POSTLINK (with handling arguments) & 0.096 & 0.003 \\
-O2 PRELINK + POSTLINK & 0.097 & 0.004 \\
\bottomrule
\end{tabular}
\caption{The table shows the performance of yklua interpreter when it has been built
either using no pass sequence (i.e baseline) and after applying prelink, postlink,
or both prelink and postlink on yklua binary with 95\% confidence intervals (the rightmost column).
The interpreter is built without JIT enabled,
that is only yk is retrofited onto the interpreter but JIT is turned off. 
The first row shows performance when no passes are applied on yklua, and the
second and third rows indicate performance when only pre or postlink passes are
applied on the interpreter. Third row indicates both passes are being applied but
the arguments inside functions are not handled. In the fourth row, we handle all
combinations to receive a compatible prelink and postlink pass sequence in our
genetic algorithm. Finally, last row represents yklua's performance when the full
list of -O2 (pre and postlink) is applied on the interpreter. This list however
is not compatible with the yk meta-tracing system (i.e~ the list fails yk's tests)}
\label{table:preandpostlink}
\end{table}

For the first experiment we consider two ways to build the lua binary:
\begin{itemize}
    \item{With the help of yk-config, we call this yklua}
    \item{With the help of traditional compilers such as gcc or clang and llvm opt tool.}
\end{itemize}

yk-config is a tool to apply yk-specific compiler and linker flags on the
interpreter and other systems which use yk's meta-tracing system. yk-config
makes configuring/compiling C interpreters easy by having the support to add
`cflags` (compiler) and `ldflags` (linker), this can be used to add the prelink and
postlink pass sequences found via the genetic algorithm by appending them to cflags
and ldflags. Finally, I use ykllvm's clang to compile yklua binary.

Clang in -O0 by default adds an `optnone` attribute to all of the functions, which
makes any optimisation we add ineffective. For this, yk-config already has
`disable-O0-optnone` (which works with the `-O0` flag). 

Table \ref{table:preandpostlink} shows performance numbers for 30 runs of
${db.lua}$\footnote{link to the benchmark test: https://github.com/ykjit/yklua/blob/main/tests/db.lua} when built
with and without optimisations and JIT turned off, and with yk meta-tracing
system retrofitted into the interpreter. The first column is with no
optimisations and the rest have some or all optimisations turned on while
building yklua binary.


\textbf{H2} \hspace*{0.5cm} Enabling all optimisation passes, even those
which can interfere with correctness, improves performance further over the
subset found by the GA \\

Table \ref{table:preandpostlink} clearly disapproves H2, the intuition behind this hypothesis
was that yklua when using full -O2 should have more performance improvement than
using subset of -O2 passes. Let's dig further why this might have happened.

The rows with optimisations applied clearly have some performance
improvement (by 1.5-2 times) when compared with the baseline (Row I). However,
it is interesting that even when we apply full -O2 pass sequence on yklua binary
(in ROW VII) the performance does not improve further. 

In the subsections below, I show that when lua is built via clang/gcc and with
-O2 passes it is 3 times faster than yklua when it is built using -O2. The
reason for this is yk's pessimistic approach to block some backend optimisations
as they interfere with basic block mapping (as described in previous sections).
To solve this I, improve some yk specific flags(passes) and show that it is
possible to win back some of the lost performance, and present proposal for
improving other passes. 

\subsection{Apply full -O2 to yk retrofitted and traditionally compiled
interpreter}

Next, I ran $db.lua$ 30 times when lua binary is compiled with gcc in -O2 \& -O3
mode, and when lua is compiled with clang and optimisations were applied with
the help of llvm's opt tool. The latter was to verify what is the performance of lua
binary when optimisation pass sequences (pre+postlink) are applied with the help
of another tool instead of using clang's default -O2 mode. Even with using opt tool to apply
passes, the performance of lua binary remains the same as what we get with gcc's -O2
pass sequence. This indicates that yk-config might be including some flags
which results in a 3 times slowdown of yklua binary when compared to lua binary. 

\begin{table}[H]
\begin{tabular}{@{}p{7.5cm}ll@{}}
\toprule
\textbf{Yklua binaries built without using yk-config} & \textbf{Mean} & \textbf{95\% CI} \\ 
\midrule
lua binary build with gcc -O2 & 0.034 & 0.004 \\
lua binary build with gcc -O3 & 0.030 & 0.003 \\
lua binary build using clang and opt (-O2 mode) & 0.034 & 0.004 \\
\bottomrule
\end{tabular}
\caption{Performance of lua binary when compiled with gcc's -O2 and -O3 mode,
and when lua is compiled with clang and optimised using -O2 passes patched via
opt. Where patching means applying optimisation passes on the IR.}
\label{tab:b}
\end{table}

\subsection{Performance impact of yk-based flags with full -O2 optimisation
turned on}

To verify if yk-based passes contribute to a 3x slowdown to yklua I start to turn
off yk related flags. First, I turn them off individually and then in
combinations. The speedup of the interpreter when the passes are turned off
indicates that the flag (or combination of them) is contributing slowdown by
ratio of: $$\frac{\text{\small (performance of baseline)}}{\text{\small
(performance of row n)}}$$ The numbers in Table \ref{table:preandpostlink} and \ref{tab:b}
contradict H2. The majority of the overhead is caused when the yk flags are used in
combination with row 14 causing maximum of slowdown.

\begin{table}[H]
\begin{tabular}{@{}cllr@{}}
\toprule
\# & \multicolumn{1}{c}{Flags turned off (-O2 passes are set)} & \multicolumn{1}{c}{Execution Time} & \multicolumn{1}{c}{95\% CI} \\ 
\midrule
1 & baseline (all flags are turned on) & 89.9ms & 0.002 \\
2 & —yk-no-fallthrough & 76.1ms & 0.002 \\
3 & —yk-block-disambiguate & 78.7ms & 0.00189 \\
4 & —yk-patch-control-point & 84.4ms & 0.002 \\
5 & —yk-insert-stackmaps & 85.8ms & 0.002 \\
6 & —yk-split-blocks-after-calls & 82ms & 0.002 \\
7 & —yk-no-fallthrough + —yk-block-disambiguate & 68.4ms & 0.002 \\
8 & —yk-no-fallthrough + —yk-block-disambiguate + —yk-patch-control-point & 63.9ms & 0.002 \\
9 & —yk-no-fallthrough + —yk-block-disambiguate + —yk-insert-stackmaps & 58.4ms & 0.002 \\
10 & —yk-no-fallthrough + —yk-block-disambiguate + —yk-split-blocks-after-calls & 66.8ms & 0.00189 \\
11 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-patch-control-point + —yk-insert-stackmaps\end{tabular} & 55.1ms & 0.002 \\
12 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-patch-control-point + —yk-split-blocks-after-calls\end{tabular} & 64.2ms & 0.002 \\
13 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-insert-stackmaps + —yk-split-blocks-after-calls\end{tabular} & 58.2ms & 0.002 \\
14 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-patch-control-point + —yk-insert-stackmaps +\\ —yk-split-blocks-after-calls\end{tabular} & 54.5ms & 0.003 \\
\bottomrule
\end{tabular}
\caption{Shows performance win back when flags (i.e~passes) or combinations of
them are turned off. Row 1 is performance of baseline lua binary, when no flag
is turned off. Row 14 shows maximum retrieval of performance when the particular
combination of flags in the row are turned off.}
\label{table:c}
\end{table}

\subsection{Performance retrieval by improving yk based flags}

Now that I have shown in Table \ref{table:c} that yk-based flags contribute upto 1.5 times slowdown in
yklua, the next task is to figure out how much performance can be retrieved back
by optimising some (and if possible maybe all) flags above. 
The best targets to fix are yk-insert-stackmaps pass and yk-no-fallthrough,

Currently in yk-insert-stackmaps, stackmaps are inserted before every function
unless it's intrinsic. However, stackmaps are not needed before calls to
unmappable functions, as these are not traced and thus never need
de-optimisation. Example of unmappable functions are external calls and functions
with no condition(s) inside them. This might claw back a few `ms` for this flag. 

With code changes and by adding a test case in this\footnote{link to the PR
containing the change for --yk-insert-stackmaps and the test case:
https://github.com/ykjit/ykllvm/pull/114}PR, I show the performance improvement
of yklua binary from ${89.9ms}$ to ${86.3ms}$.

Similarly, stackmap is not needed for functions which do not have any conditions
inside them, this is because no condition means there are no gaurds which means
the function does not need to be traced or de-optimised. This is a bit more
difficult to implement because we will need to store which functions have no
condition inside them. This can be targetted as a future work.

As for the yk-split-blocks-after-calls pass, if we adjust jitmodbuilder a bit,
we could also refrain from splitting blocks after calls to unmappable functions
such as external functions. But that could turn out to be a bit fiddly in
jitmodbuilder as the splitting helps us to know when to start/stop outlining;
outline is the opposite of inlining (we undo some inlined functions).
Jitmodbuilder is a trace builder for yk which splits basic blocks after multiple
calls, this is to understand efficiently what's going on in the trace because
LLVM does not by default split the blocks after the call. 

Another pass which can be optimised (or maybe not needed) is --yk-no-fallthrough
pass, this pass stops fallthrough optimisations which again interferes with
basic block mapping. However, there is a possible solution to not disable this
optimisation. LLVM has a special code which provides information about how blocks
are merged. If we can store the information about how many blocks are
merged and transfer this information to the hardware tracer then we can tell which
blocks were single basic blocks and which have been merged. Encoding
extra information about merged basic blocks can help us figure back the mapping
for the merged basic blocks.

\section{Related Work}
\label{section:7}
The optimisation phase ordering problem for compiler optimisations has been solved
before~\cite{guidedGA} However, the solution for the phase-ordering problem for one
system may not work for another system. This is because the oracle (the
testing conditions and target goal) may vary for different systems.
There exist other meta-tracing systems, such as RPython~\cite{rpython} that
do not face the same problem as yk does. This is because rpython traces a version of
the interpreter compiled in its own format: it actually interprets an
interpreter. With software tracing you can inject code into the AOT-compiled
binary that directly records the IR blocks that were executed, and this entirely
bypasses the need for mapping

\section{Conclusion and Future Work}
\label{section:8}
Conclusion: In this report, I prove that we can win back some of the lost
performance when we retrofit a meta-tracing system such as yk to a C-based
interpreter. Although the experiments above are run only on a couple of
benchmarks, they show the possibility of improving yk-based interpreters. We can
expand the oracle suite of the genetic algorithm to include more comprehensive
benchmarks and extend the above experiments.

Future work: includes improving yk-based flags such as
yk-insert-stackmaps, yk-split-blocks-after-calls, and yk-no-fallthrough.

Another possible hypothesis which might be worth testing is whether we can
create a partial version of the disabled optimisations, which were disabled
because they interfere with basic block mappings. This would require change(s)
in llvm or to write custom optimisation passes which perform most of the
optimisations but do not mess with the mapping. One way could be to tweak the
passes so that only optimisations within the basic blocks are allowed (and those
which work across the basic blocks are not allowed).

\nocite{*}
\bibliographystyle{plain}
\bibliography{local.bib}

\end{document}
