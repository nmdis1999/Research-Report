%&main_preamble
\endofdump
\begin{document}

\maketitle

\begin{abstract}
    \noindent
    Meta-tracing systems such as yk turn existing C interpreters into tracing
    just-in-time(JIT) compilers, making it easier for developers to use the JIT
    without implementing one from scratch for their languages. However, yk at the
    current stage does not support higher optimisation levels and
    pessimistically disables all compiler optimisations. The cost of retrofitting
    yk onto the interpreter without any optimisation is a 5x slowdown as
    compared to non-yk interpreter. My work reduces this overhead to 3x by:
    using a genetic-algorithm to find yk-compatible compiler optimisation passes,
    and performing macro-optimisations on
    both yk's compiler and the interpreter using yk. Additionally, I propose
    solutions to modify yk and non-compatible optimisation passes which could
    further reduce this overhead of retrofitting yk onto interpreters,
    starting with the lua interpreter.
\end{abstract}
\section{Introduction}
\subsection{Overview}

The popularity of dynamically typed languages stems from their ability to ease
writing software, and the rich library framework they provide. In recent years,
dynamically typed languages such as Python and JavaScript have gained popularity not only
for writing scripts, but also complex server side applications.
The most commonly used implementation of these languages are interpreters as
they are quite flexible to use, and the developer does not need to incur the cost of
compilation. However, the same flexibility that makes these
languages popular among software developers also brings challenges to
optimisations and code generations. As these languages see an increasing adoption, they demand
improvement to their execution speed. An argument can be made that developers can pick
a compiler for these languages instead of using an interpreter. But there is a catch:
dynamic languages tend to be quite slow to run when compiled~\cite{pypy}. In
statically typed languages, compilers rely on the type information for generating
machine code. However, for dynamic languages such as Python, the type
information is not known at compile-time. This means that the compiler has to emit
generalised machine code to deal with all possible type combinations during runtime,
which can be very expensive in terms of space and time complexity.

It has been stated that implementations of dynamically typed languages are often
slower than the implementation of statically typed languages\cite{ltratt09}. 
There has been continuous research on making
interpreters faster which dates back to the Self project\cite{HU94} but they
are not used as frequently as one would want. Most dynamically typed languages
use a bytecode interpreter without any advanced optimisations such as just-in-time
compilations. The reason for limited use of JIT compilers is the difficulty in their
implementations\cite{bolz2014meta}. A JIT compiler for a dynamically typed
language would need to handle all the complex interactions of the language while
simultaneously generating efficient code for the common cases. 

One solution to this problem is using a meta-tracing system. With meta-tracing
systems such as yk, language implementers do not need to implement their own
tracing JITs; instead, they can use yk meta-tracing system, and the tracing JIT
compilation will automatically be embedded in the language implementation(s). 

One of the newer approaches of implementing just-in-time compilers is tracing
JIT. In tracing JIT compilation, the Virtual Machine(VM) identifies
the frequently executed ``hot'' program paths at runtime. Once a hot path is
detected, the VM records all instructions in this path into a trace until
execution loops back to the first instruction of this path. The trace is then
compiled, and the subsequent iterations of this path are executed using the
compiled trace instead of the original code. But developing custom tracing
JITs is a hard undertaking. The concept of tracing optimisations was initially
explored by the Dynamo Project~\cite{dynamo}, which was successful in using tracing
information at runtime to optimise most executed paths. This technique is also
used in Mozilla's TraceMonkey JavaScript VM~\cite{tracemonkey}, and in Chrome's V8
through their tracing V8 system~\cite{v8jit}.

yk is a meta-tracing system which traces the execution of the interpreter, instead of tracing
the user program being executed by the interpreter. At the current stage, yk uses modified LLVM
and does not support compilation of the interpreter using any other level than -O0. Even at that
level, it disables all backend optimisations. When JIT is turned on, and we use compiler optimisation
passes at different levels, some passes either interfere with or undo the 
work done by some of yk's passes by changing the intermediate
representation. In yk, when the yk interpreter switches from JIT compilation to
interpretation, the yk system tries to map back machine basic blocks to LLVM IR
basic blocks. The yk passes makes sure that the mapping is accurate else the
correctness and efficiency of the meta-tracing system will be compromised. We
discuss this further in \ref{background}.

The result of yk pessimistically turning off all of the optimisation passes is
that the performance of yk compiled interpreter is significantly slow even when
JIT is turned off. What this means is simply embedding yk onto the interpreter
has a conspicuous cost.

\begin{figure}[ht!]
    \label{figure:multilang-jit}
    \centering
    \scalebox{0.8}{
    \includegraphics[scale=0.5, width=\linewidth]{images/multilang-jit.pdf} 
    }
    \caption{An example of how dynamic languages, without a
    meta-tracing system need individual JITs, while with a meta-tracing system, multiple languages can
    use the same JIT.}
\end{figure}

In this report, I discuss improvements I made which reduced the performance overhead
when retrofitting yk onto an existing interpreter written in C. The aim of the report is to reduce the
gap between performance of interpreter which is compiled via yk's modified
implementation of LLVM called ykllvm and the interpreter which is compiled with
GCC or LLVM's clang.

The main contributions of this report are:
\begin{itemize}
    \item{A parallel genetic algorithm to find LLVM optimisation passes that do not
        break yk's tests and improve the performance of yklua, an interpreter written in C.}
    \item{Reduced overhead of embedding yk on lua interpreter from 5x to 3x when compared to the non-yk lua interpreter.}
    \item{Optimised the yk-based pass for inserting stackmaps.}
    \item{Idenfied macro-optimisation opportunities in yk's compiler passes to guide future work.}
\end{itemize}

\subsection{Defining the problem}
The first hypothesis I test in this report is:

\textbf{H1} \hspace*{0.5cm} There exists a subset of compiler
optimisation passes (drawn from the current list of optimisations available in
LLVM) that is compatible with the meta-tracing system and meaningfully improves
the interpreter's performance.
\\
To validate this hypothesis, I start out with a divide-and-conquer algorithm
to find a pass sequence compatible with the meta-tracing system. Compatibility
is defined as the successful passing of the meta-tracer system's test cases and
the pass sequence's ability to build and run the C interpreter. However, while
this method found a compatible pass sequence (which did not break yk's test
cases), it did not find a pass sequence which improved the interpreter's
performance, and in some cases, even made it worse. Later, I present a
better approach using a stochastic genetic algorithm, which not only found
multiple compatible pass sequences but also yielded sequences that began to
improve the interpreter's performance.

There are a total of 3 hypothesis in this report, the second and third hypothesis
are discussed in Section \ref{results}.

\section{Background}
\label{background}

This section provides background on dynamically typed languages, just-in-time
(JIT) compilation, the tracing JIT compilation technique, and meta-tracing. The
highlight of this section is meta-tracing systems, as they are an important
component of this report. Additionally, this section provides details on
RPython, a production-grade meta-tracing system, and yk meta-tracing, a
hardware-based meta-tracing system on which my experiments have been carried
out.

\subsection{Dynamic typed languages and their implementation}

Interpreters are often the preferred method for implementing dynamically typed
languages such as Python, Ruby, and JavaScript. Their appeal often lies in the
ease of development and flexibility they offer. However, interpreters 
inherently suffer from slower execution speeds.

While dynamically typed languages can be compiled, they are often slower to
run when compiled as compilers will have to generate more general and slower
code to accommodate various type. \iti{rephrase}

Optimising implementation for dynamic languages is not as easy task as they are
not usually designed for speed. This has resulted in years of research to
bring the performance of dynamically typed languages to an
acceptable performance level, JIT compiler\cite{Ayc03} being a common theme in them.
The early results varying from Lisp \cite{SJG93} to Smalltalk \cite{DS84} revolved around 
the SELF language \cite{HUrs, HU94, CUE89} and were applied to the Hotspot VM
\cite{PVCO1}. These results showed that JIT compilation can give the advantages of
interpretation with the performance benefits of compiled code without
compromising the flexibility that dynamic languages require.

However, the challenge with Just-in-time compilation is that it is not an easy
implementation technique, especially when the language it targets is
itself a complex one. The challenge is to maintain the development of such
JIT compilers with the pace of the language development, this requires hundreds
of engineers effort spanning over multiple years.

Many dynamic languages use bytecode interpreters without advanced techniques like
JIT compilation. The reason for the limited use of JIT compilers is the complexity
of their implementation.  Languages such as JavaScript and Python have very
complex core semantics, with many corner cases.  So how can these languages use
JIT to improve their performance without having to writing their own JITs?
One solution is meta-tracing. Meta-tracing is a technique to efficiently execute
dynamic languages without having to rewrite a dedicated JIT for each of them,

\subsection{JIT and Meta-compilation}

There are different methods to implement just-in-time compilers, one of them
being tracing JIT. A tracing JIT records and compiles traces of the program being
interpreted. One of the important topics to be discussed in this report is
insertion of a MetaJIT in the interpreter. MetaJIT is a kind of tracing JIT
compiler. In this section we will cover what tracing JIT is and how they relate
to MetaJIT.

The first project to explore tracing JIT was Dynamo, in which tracing
optimisations were used to dynamically optimise machine code at runtime. Later,
the optimisation techniques were used by Java Virtual Machines \cite{GPF06,
GF06, BCW+10, IHWN11}.
Due to the ease of their implementation, tracing JITs became popular and were later
used in projects like Mozilla's TraceMonkey and LuaJIT. 

A trace is a sequence of instructions which are executed during a 
program's execution. Tracing refers to the process of capturing the information
about the execution of a program i.e~traces of the program. Tracing JITs use the
technique where they collect/record information about most executed (or ``hot'')
paths via traces of the program, and later at runtime, use the information to
perform optimisations and make inlining decisions\cite{tracing-jit}. 

\begin{figure}[ht!]
    \label{figure:program_and_trace}
    \centering
    \scalebox{0.8}{
    \includegraphics[scale=0.5, width=\linewidth]{images/program_and_trace.pdf} 
    }
    \caption{An example of the trace of a program (inspired from
    Wikipedia). Left hand side is a python program and right hand side is an image
    of the program's trace.}
\end{figure}

A tracing JIT makes assumptions such as:
\begin{itemize}
    \item{Programs spend most of their time inside the loops}
    \item{Several iterations of the same loop are likely to take the same
        execution path}
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{images/trace.pdf}
    \caption{\small In tracing JIT (which is the core of meta-tracing system), conditions are turned
    into guards. The guards are responsible for checking whether the conditions are
    being met (i.e~they are true) or not. If the conditions are true, and a
    trace of the execution path is
    already compiled then instead of interpreting the condition again, the compiled trace
    is used by the interpreter.}
    \label{tracing-jit}
\end{figure}

Meta-tracing (also often called MetaJIT) is a form of meta-compilation which
can be used to build JIT virtual machines for dynamic languages by feeding some
hints into the existing interpreter for the language. This makes it much easier
for developers to use a tracing JIT for their languages without writing one
themselves.

The main difference between a tracing JIT and meta-tracing JIT is that the
former is used to record and compile traces of a user program as it is being
executed, while the latter works one level down and is used to record and compile
traces of interpreter as it executes the user program\cite{bolz2014meta}.

\subsection{Retrofitting tracing JIT to an interpreter}
\label{meta-tracing}

yk is a meta-tracing system in which the tracing JIT is applied to the bytecode
interpreter instead of it being applied to directly to the user program.

There are two mechanisms for collecting traces:
software tracing and hardware tracing. yk uses Intel-PT to collect the trace of the
interpreter running the user program. Intel Processor Trace (Intel PT) is a feature
available in some Intel processors that provides a hardware-based tracing
mechanism, enabling low overhead collection of program execution traces.

The high-level view of how hardware tracing in used in yk when it is retrofitted
on a C interpreter:

\begin{itemize}
        \item{Add hints to the interpreter written in C to make it compatible
            with yk.}
        \item{User programs starts running in the interpreter written in C
            , which is compiled with ykllvm (currently with -O0).}
        \item{When a hot loop is detected, run the user program with unoptimised
            interpreter, start Intel PT, execute the loop and collect the trace.} %This will remove the 150x overhead of software meta-tracing!
    \item{Map the jump locations from PT trace back to C IR, giving us a
        usable trace for the interpreter.}
    \item{After collecting the trace IR, optimise it into machine code.}
    \item{The next iteration of the loop will now use the (fast) optimised
        machine code rather than the (slow) interpreter.}
\end{itemize}


% -----------------------
% \iti{rewrite this explaining what is loop unrolling and why we do it? :
% https://dl.acm.org/doi/pdf/10.1145/1565824.1565827 -> 3.1}
%
% ------------------------- 
\textbf{Adding hints to the interpreter written in C}

In a simple bytecode interpreter, the dispatch loop is sometimes the only hot
loop present in the whole program. This loop is responsible for executing a
opcode with each iteration. As it is unlikely that same opcode will run multiple
times in a row the guards fail very often resulting in no performance
difference\cite{pypy}, contradicting the tracing JIT's assumptions. So,
to optimise the interpreter's performance, yk uses loop unrolling. With loop
unrolling, we trace the execution of multiple opcodes, effectively unrolling so
much so that we end up with one user program loop. A user program loop is a hot
loop in the user program; a loop is called hot when the interpreter's program
counter reaches the same value multiple times indicating multiple iterations of
the same loop.

\iti{rewrite?}
The program counter is stored in one of the several variables in the interpreter. To
figure out which variable holds value for the program counter, the MetaJIT needs
hints (via a dummy control point). Since LLVM is Static-Single-Assignment (SSA),
it also adds temporary variables while generating the Intermediate
Representation(IR). The dummy control point is later(at runtime) replaced by the
original control-point. The loop is considered closed only when all the
variables that make up the program counter \iti{all the variables?} are same in the second iteration. This is
how the interpreter traces a single user loop.

\begin{figure}[htbp]
    \centering
    \scalebox{0.5}{
        \includegraphics[scale=0.01, width=\linewidth]{images/yk_metatracer.pdf} 
    }
    \caption{Figure shows how ykllvm introduces control-point and compiles
        (yk)lua binary. The control-point is necessary so that the meta-tracing
        system can identify at what point to start recording the trace. ykllvm
        also adds some additional data such as builtin tracer and LLVM IR into
        the yklua binary, as they are required to collect the trace and map the
        collected trace to LLVM IR. The yk-interpreter binary then makes calls to yk when
        it performs JIT compilations.}
    \label{meta-tracer}
\end{figure}

\begin{samepage}
\textbf{Mapping jump locations from PT trace to LLVM IR}

yk uses Intel PT as its hardware tracer. However, Intel PT can only store a
stream of basic blocks and give their virtual addresses~\cite{intel-pt}. So,
block mapping is used in hardware tracing to get from a traced virtual address
to a file offset in the binary, to the LLVM IR block. The trace IR we get can
also be called the JIT IR. The yk meta-tracing system depends on loader API and a
special LLVM section to do this. This section is emitted just before backend
optimisation.
\end{samepage}

As trace represents one of many possible paths a program can take, to ensure
correctness of the trace, guards are inserted at every point where the trace could
have diverged to different route. At machine code level these guards are
converted into checks. If a check in fails very often we quit executing the
machine code and goes back to the interpretation. 

At JIT compilation time, the tracing interpreter records the basic blocks
(jumps). When the tracing interpreter switches back to interpretation (when a
guard fails), it needs to map the information back at the AOT basic block; this
is where LLVM IR is used. Meta-tracers such as yk need to do this with a hundred
per cent accuracy. The JIT compiler relies on the traces to identify hot paths
in the program for optimisation\cite{hot-trace}. If the mapping from the traced
addresses to the LLVM IR blocks is inaccurate, the JIT compiler might optimise
the wrong code paths or apply optimisations based on incorrect assumptions. This
can lead to the generation of incorrect or inefficient JIT-compiled code,
affecting the program's correctness and performance.

\begin{figure}[htbp]
    \centering
    \scalebox{0.8}{
        \includegraphics[scale=0.5, width=\linewidth]{images/aot_and_jit.pdf} 
    }
    \caption{\small The yk meta-tracing system uses ykllvm
    to compile the interpreter's source code into a binary. This binary has a built-in tracer, a jit
    compiler and LLVM IR. The LLVM IR is then used to get the tracing information
    from the binary. The binary is generated via ykllvm and is used to interpret the
    user program. If the program has a loop (loop meaning the tracer comes
    back to the point it started collecting trace from), then the builtin tacer
    records this information. A trace starts getting collected when the meta-tracing
    system detects the control-point in the interpreter, which can be inserted by the
    user initially. This control-point is later converted into a yk specific control
    point. After collecting the full trace, the trace is compiled which is called
    just-in-time compilation (or JIT compilation) and then executed as a part of the
    meta-tracing system. If, while executing the user program, both a loop and
    complete traces are detected (which can be a later iteration of the loop), then
    instead of recording and compiling the trace again, the already compiled trace
    is used for execution.
    }
\label{yk-flow}
\end{figure}

However, some LLVM optimisation passes mess with the basic-block mapping (that
is the JIT IR). One such example is when the basic blocks are merged, some blocks
may be left unaccounted for in the trace. Additionally, LLVM applies some passes
at different stages of the pipeline, even after the final IR has been generated
(target specific optimisations). This again interferes with the mapping and creates
problems for the meta-tracer. To avoid this problem, the yk meta-tracing system
turns off all the optimisations by default, which slows down the interpreter
significantly even without the JIT being enabled.


\begin{figure}[htbp] \centering
    \includegraphics[scale=0.2]{images/block-merge.pdf} \caption{\small High
        Level IR and Machine IR side by side after backend optimisation. After
        enabling backend optimisations, some basic blocks may get merged into
        one. The previous basic block which contained operations are either
        empty or deleted. This results in a wrong run-time behaviour of the
        yk-optimised interpreter.} \label{bb-merge}
\end{figure}

\subsection{Compiler Optimisation Passes}

Modern compilers have a huge number of optimisation passes available. These
optimisation passes target unique segments of a program, such as transforming a
function, basic block(s) or the whole program\cite{llvm-pass}. Further, these optimisations can be
applied at different stages of the compilation pipelines. The sequence of passes tries
to retain the semantics of the program while potentially improving the program's
performance. There are two types of passes: Analysis and Transformation. Both
of these passes work together: the analysis passes collect information about the
program and transformation passes are responsible for using the collected
information and changing the program. Examples of optimisation levels provided by
modern compilers are: -O0, -O1, -O2, -O3, and -Og. The two major compilers gcc and
Clang have 200 and 100+ optimisation passes respectively; their sequence is
called an optimisation sequence. By default, without an optimisation level, all of
the passes are turned off and an expert can choose to turn on and off passes
accordingly~\cite{kulkarni}. 

Choosing a right optimisation level for meta-tracing systems such as yk is not
an easy task. As mentioned earlier some optimisation passes change the JIT IR resulting in
wrong run-time behaviour of a program. Identifying the problematic passes either to
disable them or fix the problem is an important step in making yk faster. LLVM
has a number of compiler optimisation levels, and each level has an optimisation
pipeline. In the next section I'll shed some light on this and how we can find the
right optimisation passes for our meta-tracing system.

\subsection{LLVM Project}
LLVM is a set of compiler and toolchain technologies and has its own
Intermediate Representation (LLVM IR). This language-independent IR helps LLVM
compilation to be target and source independent. The LLVM IR is optimised
through a sequence of LLVM passes. Passes are an operational unit of the IR which
can:
    \begin{itemize}
        \item{mutate the IR}
        \item{perform some computation about the IR} 
    \end{itemize}

The scope of each pass is a unit of an IR; it is what the pass is going to operate on. This
can be :
\begin{itemize}
    \item{a module: LLVM module is a top-level structure that represents a
        single  unit of code which is processed together. It includes global
        variable(s), function declaration(s), and implementation(s).}
    \item{a function: LLVM function is a self-contained unit of execution within
          a module. It corresponds to a function in the source code and contains
          a list of arguments, a basic block, and a symbol table.}
    \item{a basic block: LLVM basic block is a linear sequence of instructions
          within a function, and has a single entry and exit point. They are
          used to represent the straight-line code sequence that makes up a function's
          body.}
    \item{an instruction: an LLVM instruction is the smallest unit of execution
         in the LLVM IR, representing a single operation within a basic block.
         Instructions can perform a wide range of operations, including arithmetic,
         memory access, logical operations, and control flow changes. Each instruction
         produces a result and can have zero or more operands, which are either constants
         or the results of other instructions.}
\end{itemize}
These passes are categorised into analysis, transformation and utility passes
depending on whether they perform computation on the IR, mutate the IR, or do
some other non-categorised task.

\subsection{LLVM's optimisation pipeline}
\label{lto-opt}
Classically, compilers are split into three parts: frontend, middle-end and the
backend. The frontend deals with language-specific analysis i.e~operations such as
parsing, type checking and so on. The target-independent optimisations are
performed in the middle-end. And finally, the backend applies the target-specific
optimisations and emits the machine code.

\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, 
    fonttitle=\bfseries,
    boxrule=2pt, 
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]
Frontend                   LLVM
/---------\               /-----------------------------------\
|  clang   |              |                                   |
|  rustc   | --(LLVM IR)->| Middle end --(LLVM IR)--> Backend | --> Machine code
|   ...    |              |                                   |
\---------/               \-----------------------------------/
\end{Verbatim}
\captionof{figure}{Figure shows the optimisations done at different level of
compilation. Image credit: \cite{lto-prepost}}
\end{tcolorbox}


\subsubsection{LTO Pipeline}
The middle-end comprises LTO pipelines, which are split into a
pre-link,  post-link optimisation and backend optimisation
pipeline\cite{lto-prepost}.

\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, 
    fonttitle=\bfseries,
    boxrule=2pt, 
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]

|------------- pre-link --------------|
|------------------------- post-link ---------------------------|

/-------------------------------------\
|       module simplification         |
|-------------------------------------|   /---------------------\
|                                     |   | module optimisation |
|                                     |   |---------------------|   /---------\
| /-------\ /-----------------------\ |   |                     |   | backend |
| | early | |       inlining        | |   \---------------------/   \---------/
| |cleanup| |function simplification| |                                        
| \-------/ \-----------------------/ |   
|                                     |
\-------------------------------------/
\end{Verbatim}
\captionof{figure}{Figure shows optimisation pipeline in middle-end optimisation
(i.e~ LTO). Prelink and Postlink optimisation pipeline share some of the
optimisation passes. Image credit: \cite{lto-prepost}}
\end{tcolorbox}

\iti{rewrite with stephen's comments}
In the LLVM LTO pipeline, module simplification includes pre-link optimisations
such as initial cleanup, inlining of functions, and call graph optimisation to
refine the modules for additional optimisation processes. Following
this, module optimisation during the post-link phase implements optimisations
like vectorization and dynamic loop unrolling to improve the program's overall
performance. Finally, the backend phase compiles the machine code,
integrating register allocation and optimisations tailored to the specific
architecture, resulting in a faster executable code.

Now that we know about the optimisation pipelines, the question rises: what can we do
to figure out which pass to keep and which to disable for yk? In this report
I have used two algorithms to explore the search space for yk-compatible pass
sequence: a divide-and-conquer algorithm and a genetic-algorithm. I will be 
explaining both of the algorithm in Section\ref{experiments}. However, I would still
like to introduce some terminologies for genetic-algorithm in the next
subsection.

\subsection{Genetic Algorithms}
A genetic algorithm is a stochastic search algorithm which borrows the concept
of evolution from nature. This algorithm starts with an initial list of entities
(which are often represented in a bitwise form) and explores different
combinations of such entities. This is done by combining elements from two
entities by the way of crossover and mutation into a third entity, which may
represent a new point in the vast search space, that otherwise would have been
left unexplored by search methods like divide-and-conquer.

Terminology for a genetic algorithm:

\begin{itemize}
\item{Entity: In genetic algorithms, an entity (often also called an
``individual''' or ``chromosome'') represents a possible solution to the
problem being addressed. An entity is typically encoded as a string or
array, where the structure and content of this encoding depends on the
specific problem. Each entity has a set of properties (or ``genes'')
that can be altered and combined with properties of other entities
during the algorithm's processes. The fitness of each entity, determined
by a fitness function, indicates how good a solution it is with respect
to the problem.}

\item{Crossover: Crossover (also known as ``recombination'') is another genetic operator
used to combine the genetic information of two parent entities to generate new
offspring entities. The process involves swapping sections of the parents'
encoding to produce one or more children. The rationale behind crossover is to
take the good characteristics of each parent and combine them to produce better
offsprings, potentially leading to better solutions in subsequent generations.
The general way of doing a crossover is to `swap' sections of the genome of 2 parents.
The section being swapped may comprise from a single property, to all properties.
The choice of properties being exchanged may be randomised or guided by a predefined heuristic.
The simplest heuristic is a single-point crossover wherein a property in the genome sequence
is picked at random (called point), and all the properties on one side of the 'point' are swapped.}

\item{Mutation: Mutation is a genetic algorithm operator used to maintain genetic
diversity from one generation of a population of entities to the next. It is
analogous to biological mutation. The process involves making random changes to
individual genes of an entity. This can involve flipping bits in a binary
string, changing numbers in an array, or altering any part of the entity's
encoding. Mutation introduces new genetic structures in the population by
creating variants of existing entities, which helps in exploring new solutions
and prevents the algorithm from getting stuck in local optima.}
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.5]{images/genetic_algo.pdf}
    \caption{\small The logical flow of a general parallel genetic algorithm. Figure taken from~\cite{guidedGA}.}
    \label{genetic-algo}
\end{figure}

\section{Addressing interpreter performance in a meta-tracing system}
\label{interpret-perf}
yk cannot enable all the optimisation passes currently, as they
reorganise the basic-blocks and thus mess the mapping when switching from JIT
compilation back to AOT compiled interpreter. Therefore, we need to carefully choose an
optimisation pass sequence which does not interfere with basic block mapping,
while also improving the interpreter's performance.

This problem is an instance of an optimising phase ordering problem, where the
order is fixed. Optimising phase ordering is a long-standing problem with
substantial work done on it~\cite{guidedGA, kulkarni, genetic-algo}. However,
the optimisation pass sequence which might work for one system may not perform
well or even work at all for another. Finding the right optimisation pass
sequence depends on the algorithm and the oracle used. The algorithm can be
either recursive or stochastic, while an oracle is a test set which establishes
an invariant constraint. Unless the pass sequence satisfies all the constraints,
it will not be selected in the algorithm. The oracle can contain constraints
such as tests as well as the execution time of a user program. I have considered
both yk's test case and execution of a user program with yklua as the oracle.

Since some optimisation passes depend on others to enable a program's
transformation, the relative order of passes is known and fixed. Now choosing
the right subset of passes is important for a program's performance either with
respect to execution time, code size or both. A pass sequence is a subset of
passes from the larger set, in a certain order, which means the position in
which the passes are placed with respect to one another. However, with such a
high number of passes (around 90) the search space becomes large: considering
all possible subsets with a fixed ordering, the search space has $2^{90} - 1$
points.

In this report, I start with a divide-and-conquer algorithm to find a
meta-tracing compatible pass sequence and later move on to a genetic algorithm
for the same. Genetic algorithm is a well-known class of stochastic algorithms
which is adopted based on the theory of evolution. This algorithm has been
used by several researchers to solve phase-ordering problems~\cite{guidedGA,
kulkarni, genetic-algo}.

Using a genetic algorithm also allows us to find a pass sequence for our oracle in
acceptable time, where acceptable time is finding a compatible pass sequence
within a couple of hours if not minutes because of the vast search space. Further,
the genetic algorithm is not only able to find a pass sequence that is
compatible with the meta-tracing system, but the pass sequence also ends up
improving the C interpreter's performance.

However, there is scope for further improvement of the yk retrofitted
interpreter. First, the genetic algorithm currently only finds a pass sequence
for pre and postlink optimisation pipeline, and by default, all backend
optimisations are disabled both in -O0 and -O2. Second, some of yk's passes
incur significant overhead. After finding the pass sequence which satisfies the
oracle as well as improves the yklua interpreter, I proceed to investigate other
blockers which inhibit yklua's efficiency. The following experiments start from
understanding the slowdown incurred by yk's passes and later, I proceed to find
the impact of disabling all of the backend optimisations and whether it is
possible to enable them back in both -O0 and -O2 optimisation levels.

\section{Evaluation}

\subsection{Methodology}
\label{methodology}

I had to use two separate machines for the work done, one for finding optimisation
passes and the other to run the yk and non-yk lua's test
suites for benchmarking. The first machine has an x86\_64 architecture with 72
physical cores while the second machine has an x86\_64 architecture with 8
physical cores.  

As the search space of compiler optimisation passes is huge, running the genetic
algorithm on an 8 core processor was quite slow. Using two machines made it faster
to conduct the experiments and verify my hypothesis.

\textbf{Machine I}: Intel Xeon, x86\_64 architecture, 72 physical cores,
                      Debian 5.10 \\

\textbf{Machine II}: Intel Xeon Platinum, x86\_64 architecture, 8 physical cores,
                        Debian 6.1 \\ 

Machine I is used to run the algorithms discussed below and search for yk-compatible optimisation pass
sequences.
Machine II is used to compile yklua with the pass sequences obtained from the said
algorithms and gather the benchmark data.

To both collect pass sequences, and benchmark yklua's test suite I used
ykllvm (version 18).

\subsection{Experiments}
\label{experiments}

The initial goal was to find `some' subset of passes that satisfies the oracle. 
As it would be very expensive to try all the combinations linearly, it is easier
to reduce the search space every time our search in the bigger space fails. The
simplest metric is to divide the search space into half.

Therefore, to verify H1, I implemented a divide-and-conquer algorithm and tested
it against yk's test suite. I compared the performance of yklua compiled with
the pass sequence retrieved from the algorithm against the default yklua binary
(i.e~ one compiled without any optimisations enabled). The result showed that
the algorithm could not find a pass sequence which satisfied both constraints
i.e~ passing yk's test suite and improving the yklua's interpreter's
performance. Next, I wrote a
genetic\-algorithm\footnote{https://github.com/ykjit/yk/pull/900} for the same.
The genetic algorithm successfully found multiple pass sequences which satisfied
both the constraints, and emitted the most performant sequence at the end. 

In all of the experiments done below, we use yk as the meta-tracing system and yklua as
the C interpreter being optimised.

\subsubsection{Divide-and-conquer algorithm for phase ordering problem}
\scalebox{0.65}{
\label{dac-algo}
\begin{tcolorbox}
    \begin{algorithmic}[1] % The [1] argument enables line numbering
    \small
    \State Define structures for \textbf{Pass} and \textbf{PipelineConfig}
    \State Fetch all available optimisation passes

    \Function{test\_pipeline}{PipelineConfig}
        \State Execute the pipeline with the specified configuration
        \State Log the results
    \EndFunction

    \Function{attempt1}{passes}
        \State Test each pass in isolation
        \State Log which passes succeed and which fail
    \EndFunction

    \Function{attempt2}{passes}
        \State Start with an empty set of okay passes
        \State Recursively test combinations of passes
        \If{a combination is successful}
            \State Add it to the set of okay passes
        \EndIf
        \If{a combination with more than one pass fails}
            \State Split and test subsets
        \EndIf
    \EndFunction

    \Function{main}{}
        \State Fetch all passes
        \State Perform initial testing of passes in isolation (\textbf{attempt1})
        \State Perform combined testing of passes (\textbf{attempt2})
    \EndFunction
    \end{algorithmic} 
\end{tcolorbox}}

This algorithm has two functions: attempt1 tries each pass
individually to see which passes succeed and which ones fail in isolation. It
takes a total of 36 hours to run attempt1. In attempt2, the algorithm starts with
an empty list of ok passes. It then recursively tests combinations of passes starting with
a list of all passes, and then splits the list in two if the initial list fails. The function only
adds the subsets which succeed to the ok passes list. The run time for attempt2
is between 9-10 hours.

The initial version of the algorithm used to reshuffle
the list as well to make it stochastic, however, I noticed that we received no
pass sequence which was compatible with yk and improved yklua's
performance. Removing stochastic elements helped me to get at least one pass
sequence which was compatible with yk, but failed to provide any performance
improvement to yklua. This is because some LLVM optimisation passes depend on others
to provide them required information. So if the order of pass sequence is not correct, some 
optimisations might not work since their dependencies are not satisfied before
they are processed.\footnote{https://github.com/ykjit/yk/pull/829}

\subsubsection{Genetic Algorithm for phase-ordering problem}
The divide-and-conquer algorithm gives incompatible passes with reshuffle on,
and if turned off, it does not explore the search space well enough to give a
pass sequence that improves performance. The algorithm does not introduce any
randomisation in the choice of passes, and therefore, limits the number of
combinations created and tested. Thus comes the requirement of a stochastic
algorithm that can explore more pass sequences in the search space, while
keeping the order of passes fixed.

The pseudocode in Figure \ref{pseudocode-genetic} details the genetic algorithm used
to find an optimisation pass sequence compatible with yk and yklua. Initially, it randomly
generates a population where each entity is a binary array indicating
whether a specific pass is active (1) or inactive (0). The binary array maps
to a predefined order of passes, and allows to select a subset of it. The main parallel
component is the fitness function, which is a value computed for
every entity, indicating how ``optimal'' a pass sequence is.

Optimal means a pass sequence which succeeds all yk's tests while also minimising
execution time of the interpreter. Each entity's performance is tested
in parallel, speeding up the process significantly. To pick each new parent, the algorithm uses
tournament selection (reproduction operator) between two randomly picked entities from the
population, which selects the entity with a better fitness score of the two.
The selected parents are then recombined and mutated to form new entities, resulting in the
next generation of population. The process iterates through generations to improve pass
sequence combinations (evolve) until the optimal set of passes, leading to the best
interpreter performance is identified.

\laurie{this is a fairly weird way to define an algorithm: why not use more
conventional pseudocode? see e.g. Fig 6 in the ``don't panic!'' paper as an
example}

\scalebox{0.65}{
\label{pseudocode-genetic}
\begin{tcolorbox}
    \begin{algorithm}[H]
        \begin{algorithmic}[1] % The [1] argument enables line numbering
        \small
        \State Define structures for \textbf{Pass} and \textbf{PipelineConfig}
        \State Fetch all available optimisation passes
        \State \iti{how do we maintain the static ordered list of passes?}

        \Function{test\_pipeline}{PipelineConfig $pl$}
            \State Execute the pipeline with configuration $pl$
            \State Log the results
        \EndFunction

        \Function{encode\_passes\_randomly}{Pass $root\_passes$}
            \State Recursively set `on' flag of each pass to 0 or 1 randomly
            \State Ensure at least one subpass is enabled if parent is enabled
            \State Return modified $root\_passes$
        \EndFunction

        \Function{decode\_passes\_to\_string}{Pass $passes$}
            \State Generate string representation of enabled passes and hierarchy
            \State Exclude passes with `on' set to 0
            \State Return the string representation
        \EndFunction

        \Function{evaluate\_fitness}{Pass $passes$, ...}
            \State Decode passes string into PipelineConfig $pl$
            \State Call $test\_pipeline(pl)$ to get execution time of a user program
            \State Add fitness score (execution time of user program) to shared list
        \EndFunction

        \Function{tournament}{list $population$, list $fitness$, float $sp$}
            \State Select two individuals randomly
            \State Choose fitter one with probability $sp$ as parent
        \EndFunction

        \Function{crossover}{Pass $parent1$, Pass $parent2$}
            \State Create child passes by swapping random passes between parents
            \State Return child passes
        \EndFunction

        \Function{mutate}{Pass $entity$, float $mutation\_rate$}
            \State Recursively flip `on' flag of each pass with probability $mutation\_rate$
            \State Return mutated pass
        \EndFunction

        \Function{genetic\_algorithm}{string $parsed\_str$, ...}
            \State Create initial population of randomly encoded passes
            \For{$generation$ in $1$ to $generations$}
            \State Evaluate fitness of each entity in parallel
            \State Select elites (good fitness)
            \State Apply crossover and mutation to create new generation
            \EndFor
            \State Return best entity (set of passes)
        \EndFunction

        \Function{main}{}
            \State Set up temporary directories
            \State Get available passes using `get\_all\_passes'
            \State Run genetic algorithm to find best passes
            \State Decode best entity and print final passes
        \EndFunction
        \end{algorithmic}
    \end{algorithm}
\end{tcolorbox}}

\subsection{Algorithm's speed up after parallelisation}

The first version of the genetic-algorithm was not parallelised and it took 4-5
hours to run just 10 generations. Computation of the fitness function for each
entity is independent of other entities in the population, and is the most
expensive operation in the algorithm. Therefore, I optimised the algorithm to
compute the fitness of a generation in parallel which reduced the run-time for
10 generations from 4-5 hours to 15 minutes. This improved my efficiency in
running experiments and finding patterns in successful and non-successful pass
sequence\footnote{https://github.com/ykjit/yk/pull/900/commits/e597774190c2856cd19328c94401f44ea8bf3a8b}.

\section{Results}
\label{results}
\subsection{Overview}

When I started the experiments, yklua without JIT turned on performed ~5x worse
than the non-yk lua interpreter. The aim of the optimisations performed in
the upcoming section was to reduce this gap caused by embedding yk onto the lua
interpreter. There is expected cost for profiling the interpreter without
turning on the JIT. Ideally we would want to bring the difference between the
two variants to 2x or lesser.

Results at a glance:
\begin{itemize}
    \item{the genetic algorithm is able to find pass sequences which satisfies our
        oracle's constraints as well as improves yklua interpreter's performance
    by ~1.70x.}
    \item{After optimisation performed above, 50\% of the total overhead caused by retrofitting yk onto the lua
        interpreter is contributed by yk-based passes.}
    \item{Optimised stackmap insert pass resulting in about 7\% performance
        improvement.}
    \item{Macro optimisation done on yklua's code improving performance of
        some benchmark by 200x (I'll be elaborating this in upcoming subsections.)}
\end{itemize}

In addition to the above, I explore the possibility of making some, if not all,
backend optimisations work with the oracle. As a prerequisite to designing and
executing experiments for hypotheses related to backend optimisations, I needed
to fix the compilation errors that occurred when enabling backend optimisations
with both -O0's and -O2's optimisation passes.

\subsection{Defining the Baselines}
There are two binaries which can be considered as potential baseline. We shall
refer to them as baseline 1 and baseline 2.

\textbf{Baseline 1:} yklua compiled with -O0 (i.e~ all optimisations are
turned off).

\textbf{Baseline 2:} non-yk lua compiled with -O2.

My initial goal is to reduce the overhead of retrofitting yk onto the
interpreter. To achieve this, I apply yk-compatible optimisation passes onto the
-O0 compiled yklua interpreter. The  optimisation passes that satisfy the oracle
are considered to be yk-compatible. Once I reduce the performance overhead when
compared to Baseline 1, I shift my aim to bring the performance of the already
optimised yklua closer to that of the -O2 compiled non-yk lua interpreter (i.e~
Baseline 2).

\subsection{Effects of applying a meta-tracing compatible pass sequence on the yklua
interpreter}

In this section I'll be using a pass sequence obtained through genetic algorithm
to improve performance of the yklua interpreter. Further, I will show that even
after applying full -O2 optimisation level we are still 3x slower than non-yk
lua partially due to yk-based flags interfering with optimisations which use the
lto backend.

\subsubsection{Adding Passes at -O0 Level}
As discussed in Section \ref{lto-opt}, there are three levels of LLVM's 
optimisation pipeline: prelink, postlink, and backend optimisation. I modified
yk-config to accept prelink and postlink optimisation passes, however
overwriting backend optimisation is rather challenging and will be discussed
later in this section. 

For the first set of performance numbers, I added a pass sequence (both
prelink and postlink) to the -O0 optimisation in yklua binary (i.e~ turning on the
passes found via genetic algorithm) with the help of yk-config.

Finally, I use ykllvm's clang to compile yklua binary, as
described in Figure \ref{yk-flow}.

Table \ref{table:preandpostlink} shows performance numbers of adding prelink,
postlink or both to yklua -O0 optimisation level.
\begin{table}[H]
\centering
\begin{tabular}{@{}p{4.0cm}lllcc@{}}
\toprule
\textbf{binary} & \textbf{prelink passes} & \textbf{postlink passes} & \textbf{optimisation level} & \textbf{Mean} & \textbf{95\% CI} \\ 
\midrule
yklua (Baseline 1) & no & no & -O0 & 0.140 & 0.002 \\
yklua & yes & no & -O0 & 0.097 & 0.003 \\
yklua & no & yes & -O0 & 0.104 & 0.001 \\
yklua & yes & yes & -O0 & 0.096 & 0.003 \\
yklua & no & no & -O2 & 0.093 & 0.003 \\
lua (Baseline 2) & no & no & -O2 & 0.034 & 0.004 \\
\bottomrule
\end{tabular}
\caption{The table shows the performance of the yklua interpreter built with and
without a pass sequence obtained from the genetic algorithm, with a 95\% confidence
interval (CI). The performance numbers are for 30 runs of \textit{db.lua} when the JIT is
turned off. \textit{db.lua} is a lua program which tests lua's debug library.
All the performance numbers represent the cost of retrofitting yk
onto the interpreter. The first row is yklua built without any pass sequence,
and consecutive rows with adding prelink, postlink, or both pass sequences to
-O0. The last two rows are yklua and non-yk lua compiled lua with -O2
optimisation level.}
\label{table:preandpostlink}
\end{table}

\subsubsection{Compiling yklua with -O2}
yklua blocked all backend optimisation to run by adding a pass in ykllvm called
YkAfterIRPasses, which would patch the `optnone' attribute on all of the
functions in the backend optimisations. However, this pass assumed that this
would be enough to block backend optimisation for all the optimisation levels
while still not causing problems when compiling with the optimisation level.
Contradictory to the assumption made by the pass, this resulted in a compilation
error in yklua when using the -O2 optimisation level. The obvious question is,
why does compiling yklua only fails with -O2 and not with -O0? The answer
turned out to be quite simple; unlike -O0, to block backend optimisation
functions in -O2, we have to apply both `optnone'  and `noinline' attribute. 

The reason for using both noinline and optnone attributes together when blocking
optimisations at higher optimisation levels like -O2 is related to how the LLVM
optimiser handles these attributes and the inlining optimisation. At -O0
optimisation level, the LLVM optimiser performs minimal optimisations, and the
optnone attribute alone is sufficient to prevent optimisations for a specific
function. The optnone attribute tells the optimiser to skip all optimisations
for the function, including inlining. However, at higher optimisation levels
like O2, the LLVM optimiser becomes more aggressive and performs various
optimisations, including function inlining. The optnone attribute alone may not
be enough to completely prevent optimisations in this case. When a function is
marked with the optnone attribute, the optimiser will skip most optimisations
for that function. However, if the function is called from other functions that
are being optimised, the optimiser may still decide to inline the optnone
function into the calling function.

After adding the fix, yklua can be compiled using both -O0 and -O2. However,
the problem remains that we cannot use the full set of passes for yklua's
compilation as they are incompatible with yk's test suite. This is due to wrong
runtime behaviour caused by interaction between yk-bases passes and the backend
optimisations. The goal is to make a single optimisation level which works for
all of our oracles (in this case they are the yk test suite and yklua's
compilation and test suite).

\subsubsection{Improvements with -O2}
The prelink and postlink passes obtained from the genetic algorithm are a subset
of -O2 optimisation pass sequences. Observing Table \ref{table:preandpostlink}
it is curious that applying a subset of -O2 pass sequences results in similar
performance as that of directly compiling with -O2 optimisation level. There is
a possibility that genetic algorithm found all the passes in the sequence which
are sole contributors to the performance. The other possibility
is that yk based flags interfere with some of non-backend optimisations in -O2
limiting -O2's full potential for optimisations.
This brings me to my second hypothesis.

\textbf{H2} \hspace*{0.5cm} compiling yk retrofitted interpreter with -O2 should
result in fastest performing interpreter by significant margin. \\

Table \ref{table:preandpostlink} clearly disapproves H2, the intuition behind
this hypothesis was that yklua when using full -O2 should have further
performance improvement over using subset of -O2 passes. However, the
performance of -O2 compiled yklua lies in the range of performance of yklua when
compiled with subset of -O2 passes. Let's dig further why this might have
happened.

The reason -O2 is not optimising yklua interpreter to it's full potential is
that some of the middle-end optimisation passes are getting blocked by yk-based pass.
Postlink and backend optimisation share same the LTO backend code (in
LTOBackend.cpp). Since YkAfterIRPasses (a ykllvm pass) applies optnone attribute
via LTO backend, it ends up blocking some of the postlink optimisations along
with intended backend optimisations. 

This prevents us from exploring the performance gains we could have gotten from
the blocked passes. However, we cannot remove the `optnone'
and `noinline' attributes from the lto backend, as removing them allows all
backend optimisations to run which change the JIT IR such that compiling the IR
results in a wrong runtime behaviour.

The easiest way to remove these attributes from lto backend while still
maintaining the soundness of the meta-tracing system is disabling only
problematic backend optimisation passes. However, before designing experiments
to find problematic backend optimisation passes I'll focus on one other
macro-optimisation opportunity.

In next subsection I will show the performance overhead of yk specific passes,
this will help us to figure out which pass is problematic and then proceed to
investigate whether it is necessary to block all of the backend optimisations.

\subsubsection{Performance impact of yk-based flags when compiling yklua with -O2} 
\textbf{a) Overhead introduced by yk passes} \\ 
Table \ref{table:c} shows performance
overhead of individual yk passes.  The objective of the following experiment is
to understand what portion of total slowdown in yklua is contributed by yk-based
passes (either individual pass or a combination of them). I started by turning
off yk specific passes individually and then in combinations, and recorded the
execution time for each case. The speed-up of the interpreter when the passes
are turned off indicates that the pass (or combination of them) is contributing
a slowdown by the ratio of: $$\frac{\text{\small (performance of
baseline)}}{\text{\small (performance of row n)}}$$ 

\begin{table}[H]
\begin{tabular}{@{}cllr@{}}
\toprule
\# & \multicolumn{1}{c}{Flags turned off (-O2 passes are set)} & \multicolumn{1}{c}{Execution Time} & \multicolumn{1}{c}{95\% CI} \\ 
\midrule
1 & baseline (yklua compiled with -O2) & 89.9ms & 0.002 \\
2 & —yk-no-fallthrough & 76.1ms & 0.002 \\
3 & —yk-block-disambiguate & 78.7ms & 0.00189 \\
4 & —yk-patch-control-point & 84.4ms & 0.002 \\
5 & —yk-insert-stackmaps & 85.8ms & 0.002 \\
6 & —yk-split-blocks-after-calls & 82ms & 0.002 \\
7 & —yk-no-fallthrough + —yk-block-disambiguate & 68.4ms & 0.002 \\
8 & —yk-no-fallthrough + —yk-block-disambiguate + —yk-patch-control-point & 63.9ms & 0.002 \\
9 & —yk-no-fallthrough + —yk-block-disambiguate + —yk-insert-stackmaps & 58.4ms & 0.002 \\
10 & —yk-no-fallthrough + —yk-block-disambiguate + —yk-split-blocks-after-calls & 66.8ms & 0.00189 \\
11 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-patch-control-point + —yk-insert-stackmaps\end{tabular} & 55.1ms & 0.002 \\
12 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-patch-control-point + —yk-split-blocks-after-calls\end{tabular} & 64.2ms & 0.002 \\
13 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-insert-stackmaps + —yk-split-blocks-after-calls\end{tabular} & 58.2ms & 0.002 \\
14 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-patch-control-point + —yk-insert-stackmaps +\\ —yk-split-blocks-after-calls\end{tabular} & 54.5ms & 0.003 \\
\bottomrule
\end{tabular}
\caption{Shows the performance we can win back when individual passes or
    combinations of them are turned off with a 95\% confidence interval (CI). Row
    1 is the performance of the baseline yklua binary compiled with -O2. Row 14 shows
    maximum retrieval of performance when the passes in
    the row are turned off.}
\label{table:c}
\end{table}

From Table \ref{table:c} we can see that yklua after applying -O2 optimisations
is 3x slower than Baseline 2. It is clear that yk-based passes contribute upto
50\% of total slowdown in yklua. Some passes cannot be further optimised as they
change the AOT IR making them compatible for meta-tracing system (e.g adding
patch points and block-disambiguate pass). In the next subsection I'll elaborate
on how much performance can be retrieved back by optimising some of the most
expensive yk passes shown in table above. 

\textbf{b) Optimising yk-based Passes} \\

\textbf{yk-insert-stackmaps} \\

Stackmaps describe how the stack looks like i.e~ where the variables are stored
i.e~ either in the register or the stack slot. During JIT compilation when the
guard fails very often (i.e~ when the failure hits threshold set by yk) the
meta-tracer starts de-optimising the trace. De-optimisation is process of going
back to the start of the loop and rebuilding the trace with new guards. During
JIT compilation, the trace builder inlines functions into a trace which results
in the changed structure of stack. With the help of Stackmaps, yk is able to
get back the exact trace that it saw at the beginning of loop.

The `yk-insert-stackmaps' pass is responsible for adding stackmaps to the JIT
IR.

Currently in yk-insert-stackmaps, stackmaps are inserted before every function
unless it is intrinsic. However, stackmaps are not needed before calls to
unmappable functions expect from the control point\iti{reference to appendix}
and yk\_promote\iti{reference to appendix}, as these are not traced and thus
never need de-optimisation. Example of unmappable functions are external calls
and functions with no condition(s) inside them. This might claw back a few `ms'
for this pass. 

After adding a fix to make sure that stackmap is not inserted after unmappable
functions, except for control point and yk\_promote calls, we can see around
10\% speedup when compared to yklua binary without this fix\footnote{
https://github.com/ykjit/ykllvm/pull/98}. Similarly, another opportunity for
macro optimisation is to not add stackmaps for the functions which do not have
conditions in them (i.e~ no guards) as they have no need for de-optimisations.
This however is more complex to fix as currently there is no direct way to
figure whether a function has a condition in it or not. For now I'll leave this
fix as a part of future work.

\textbf{yk-split-blocks-after-calls pass}

SplitBlocksAfterCalls pass in yk makes sure that function calls terminate by
splitting blocks after every call. This pass is needed so that outlining is
performed effectively. Outlining is reverse of inlining. The tracer (in our case
yk's JITmodbuilder) automatically inlines functions during tracing. This means
that for a function with big loops each iteration of the loop will be inlined in
the trace. This makes the trace huge, resulting in occupying much more memory and
making compilation slower with no additional performance benefits. Thus yk
removes the inlined function with its call instruction.

Currently this pass also splits blocks after calls to unmappable functions (such as
external functions). Another macro optimisation is to refrain from
splitting blocks after calls to unmappable functions. But this could turn out to
be rather fiddly as currently, we depend on splitting to identify when to
start/stop outlining.

\textbf{yk-no-fallthrough pass}

The fallthrough optimisation in both SelectionDAG and FastISel works by
analysing the control flow graph (CFG) of the function being compiled. It
identifies patterns where a basic block has a single successor and there is no
intervening code between the basic block and its successor. When such patterns
are found, the optimisation eliminates the branch instruction at the end of the
basic block and merges the two blocks into a single block. This allows the
execution to fall through from one block to the next without the need for an
explicit branch.
This becomes problem for the JIT as it distorts the basic block mapping as we
don't have information about which blocks were merged. One solution which
doesn't need us to disable fallthrough optimisation is by encoding extra
information about merged basic blocks. LLVM has a special code which provides
information about how blocks are merged (such as ``merged\_bb\!''). If we can
store the information about how many blocks are merged and transfer this
information to the hardware tracer, then we can tell which blocks were single
basic blocks and which have been merged when mapping back from JIT IR to AOT IR.

\subsection{An Optimisation to yklua}
One of the macro optimisations I made while benchmarking yklua was directly to
yklua's source code. Earlier yklua would perform reallocation of yk's location
array for every instruction. Yk's location array indicates the point from which
the loop starts and is then sent to the control point.
However, this was done for every instruction instead of performing reallocation
only for those instructions whose bytecode size was larger than the current
location array size. This turned out to be quite performance-intensive and made
some programs exponentially slower as they grew in size. The
fix for this was checking and reallocating the location array only when the
resize was necessary\footnote{https://github.com/ykjit/yklua/pull/84}.

\subsection{Where to begin: -O0 or -O2?}

After adding `noinline' attribute to lto backend we can now easily use -O2 to
compile yklua. This gives us an opportunity to consider whether we want to start
with -O0 and add passes to it or start from -O2 and remove passes from it.
One would expect both should result in same performance when benchmarking with
lua tests. 

\textbf{H3} \hspace*{0.5cm} The performance for yklua binary should be the same when adding a list of
passes to -O0 to that of removing passes from -O2. \\

\begin{table}[h]
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
Benchmarks & Compiled yklua & Compiled yklua & Compiled yklua & Compiled yklua & Non-yk lua & Non-yk lua \\
& with -O0 (Baseline 1) & with -O2 & with -O0 and adding passes & with -O2 and
removing passes & with -O0) &  with -O2 (Baseline 2) \\
\hline
db.lua & 0.120 & 0.068 & 0.125 & 0.069 & 0.061 & 0.028 \\
verybig.lua & 0.597 & 0.335 & 0.666 & 0.342 & 0.261 & 0.141 \\
constructs.lua & 4.693 & 2.506 & 4.614 & 2.645 & 2.753 & 1.072 \\
errors.lua & 5.353 & 3.853 & 5.726 & 3.862 & N/A & 2.919 \\
files.lua & 0.214 & 0.194 & 0.223 & 0.198 & 0.162 & N/A \\
gc.lua & 0.435 & 0.280 & 0.449 & 0.297 & 0.245 & 0.138 \\
math.lua & 0.229 & 0.123 & 0.291 & 0.156 & 0.065 & 0.019 \\
pm.lua & 0.050 & 0.050 & 0.047 & 0.036 & 0.022 & 0.009 \\
\hline
\end{tabular}
}
\caption{The table shows that the performance of yklua binary varies when we start
    compilation from -O0 versus when we start from -O2, and add/remove the same
    pass sequence. The last two columns are lua (non yk retrofitted) binary
    compiled in -O0 and -O2 respectively, latter being the Baseline 2.
    `N/A' means that program did not run with the said version of the
    interpreter.}
\end{table}

Depending upon the optimisation level we start with, clang perform different
optimisations as well as expand preprocessor macros differently during IR
generation. Which means even though the passes we add/remove have the same name
they may perform the optimisations differently. This explains why we see
different performance even when we apply same passes to overwrite these two
optimisation levels.

Although we have verified that compiling with -O2 and removing passes gives us
better performance for the interpreter we still need to make sure that our
oracle passes with -O2(i.e~ all yk and yklua test suite are successful) before
making it the default compilation level. 

\subsection{Backend optimisations and runtime behaviour}

The oracle we use has broadly two parts. The first part of the oracle checks whether we are able to find compiler
optimisation passes which result in successful compilation and run-time
behaviour of yklua's test suite when the JIT was turned off. This helped us
analyse the cost of embedding yk onto the interpreter. My work reduced the
overhead of retrofitting yk onto the lua interpreter from 5x to 3x. However, to further 
reduce this overhead I will need to figure out what component in yk is preventing the optimisation
passes from producing correct run-time behaviour for the second part of the oracle.

The second part of the oracle expects yk's test suite to be successful when we
enable optimisation passes at different levels with JIT turned on.
However, for some optimisations (particularly that uses LTO's backend), this
reveals a wrong run-time behaviour for a good portion of the test cases.

Some test-cases fail because they expect the IR to look a certain way,
which we can discard as it is expected that some optimisations will re-arrange
the parts of the IR without resulting in wrong runtime behaviour. Other
test-cases fail with much more complicated problems. One example is that with
some optimisations, the loop is optimised in a way that control point is moved
outside of the loop, this however is not expected by yk and thus results in test
cases failing. Others result in wrong behaviour such as re-entering the loop
after de-optimisation, that will require an in-depth investigation to figure out
which optimisation is responsible for changing the IR in a way that yk (or
particularly ykllvm) does not expect.

My earlier work assumed that only backend optimisation passes interfere with
the IR in a manner which results in wrong mapping from JIT IR back to the AOT IR,
as we switched from JIT compilation back to AOT compiled interpreter. However, it
turns out that some postlink optimisations also result in changing the AOT IR
in a way that is not compatible with yk-based passes. Which essentially means
that when we are adjusting the IR at code generation stage (e.g adding Stackmap passes,
splitting blocks after calls etc) we might be changing the IR in a way that is
not expected by LLVM at that stage. Some backend optimisation passes
may be optimising away/re-arranging the instructions that are needed either by
Stackmap or are expected to be presented in the IR. Thus, although we are able to
compile the test cases, they fail with the wrong run-time behaviour.

The next set of experiments will include me studying the failing test cases when
we enable prelink, postlink and backend optimisation, first individually and
then in combination. The primary target is to be able to generate correct IR
even after inserting yk-specific opcodes and running yk-specifc passes, so
that LLVM optimisations at backend generate IR which results in correct run-time
behaviour.

The second experiment, which I believe will need to be conducted in parallel, is
to figure out the problematic passes which result in these test cases to fail. This
can be done by fixing LLVM to overwrite backend optimisations. I can adjust
TargetPassConfig in LLVM Codgen to do this with the help of llc. llc has an
option to over-write backend passes with run-passes. After adjusting the
TargetPassConfig, I will need to make adjustments to yk-config so that we
overwrite the backend optimisations at the right stage. Once it is possible to
overwrite backend optimisations, we can use the genetic algorithm described in
Section\ref{experiments} to find the problematic passes and study their fixes (or
if we need to completely disable them).

\section{Related Work}
\label{related-work}
The optimisation phase ordering problem for compiler optimisations has been solved
before~\cite{guidedGA}. However, the solution for the phase-ordering problem for one
system may not work for another. This is because the oracle (the
testing conditions and target goal) may vary for different systems.

There exist other meta-tracing systems, such as RPython~\cite{rpython}, that do
not face the same problems as yk does. RPython requires the language
implementation to be written in its own format and interprets the interpreter.
Further, in RPython the trace is collected only when the software tracer is sure it is a hot loop. This
means the performance overhead of collecting traces in RPython is expected to be
much more than that of a hardware-based tracer. However, it also means that
RPython does not need to map back JIT IR back to the interpreter's IR, and thus,
can use the compiler optimisation passes much more freely.

With software tracing, we can inject code into the AOT-compiled binary that directly
records the IR blocks that are executed, and this entirely bypasses the need
for mapping. On the other hand, yk uses a hardware tracing mechanism to record the
traces with the interpreter's first iteration. Then, for the consecutive
iterations, the compiled trace is used instead. 

Since yk requires to select optimisation passes which would not
re-arrange/change the machine basic block incorrectly, solving the problem of
optimisation pass selection as an instance of the phase ordering problem
becomes much more helpful in yk's context.

\section{Conclusion and Future Work}
\label{CaF}
In this report, I have shown that with the help of a genetic algorithm, we can
use middle-end and possibly backend optimisation passes with the C interpreter,
which not only maintains the correctness of the meta-tracer, but simultaneously
improves the interpreter's performance when compiled with the meta-tracer
retrofitted into it.

As shown in the Table \ref{table:preandpostlink}, I can win back around 1.75x
performance after applying middle-end passes and macro-optimisations to the
meta-tracer's passes. Although I am yet to catch problematic backend
optimisation passes, it is not a very far-fetched goal. The next immediate
experiment would be to change LLVM's legacy pass manager so that we can easily
overwrite the backend optimisation pipeline at any optimisation level. 

After the change mentioned above, we can use the genetic algorithm to find a combination of
pass sequences which does not modify the JIT IR in a non-compatible
way. Meaning when we are mapping back the JIT IR to the AOT IR, the compiled IR results
in the correct run-time behaviour of the code.
Alternatively, we can turn off the parts of the optimisations which interfere with the
IR in an unwanted way for the meta-tracer. For example, one way could be to tweak the passes
so that only optimisations within the basic blocks are allowed (and those which
work across the basic blocks are not allowed as they change the structure of the
IR).

Other future work includes enabling back fallthrough optimisation (i.e.~
retiring yk-no-fallthrough). Currently, yk uses -O0 without backend optimisations
enabled to compile the lua interpreter. With my changes, we can
move in the direction of making a custom optimisation pass in LLVM for yk, which not
only blocks non-compatible optimisation, but can also use as much of the higher
optimisation level passes as possible. The result will be a 50\%, if not more,
reduction in the overhead of embedding yk onto the C interpreters.

\section{Timeline}
\noindent
\resizebox{1.1\textwidth}{!}{  % Increase scaling to 1.5
    \begin{tabular}{lp{10cm}}
        \toprule
        \textbf{Year} & \textbf{Goals} \\
        \midrule
        \textbf{2024 June - August} & \begin{itemize} \item{Adjust LLVM's Legacy Pass Manager to overwrite the backend optimisation pipeline.} \item{Design a custom pipeline with yk-compatible optimisations.} \item{Perform macro-optimisation on yk-based passes in ykllvm, reducing their overhead.} \end{itemize} \\
        \textbf{2024 September - November} & \begin{itemize} \item{Profile and benchmark yk-retrofitted C interpreters when compiled with a yk-compatible custom optimisation level. This may introduce opportunities for further optimisations.} \end{itemize} \\
        \textbf{2024 December - 2025 February} & \begin{itemize} \item{Conduct an in-depth literature review on optimization passes constructed for meta-tracing systems.} \item{Design experiments to explore introducing a new optimisation pass for the yk meta-tracing system, performing optimisations at JIT compilation time.} \end{itemize} \\
        \bottomrule
    \end{tabular}
}

\nocite{*}
\bibliographystyle{plain}
\bibliography{local.bib}

\section{Appendix}
\iti{APPENDIX}yk-config is a tool to apply yk-specific compiler and linker flags on the interpreter and other
systems which use yk's meta-tracing system. yk-config makes
configuring/compiling C interpreters easy by having the support to add `cflags'
(compiler) and `ldflags' (linker), this can be used to add the prelink and
postlink pass sequences found via the genetic algorithm by appending them to
cflags and ldflags.

\iti{APPENDIX}ykllvm is a modified version of LLVM
which applied yk specific flags to the final binary.

\iti{add what
is control point and yk\_promote in the background section}

\iti{add about LLC}

\end{document}
