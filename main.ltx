% Main Document
\input{preamble.ltx}

\begin{document}

\maketitle

\section{Introduction}
LLVM-BOLT is a post-link static binary optimizer that enhances code layout
utilizing sample-based profiling data at the binary level.  This study
investigates whether providing more accurate data can improve LLVM-BOLT's
performance in optimizing binaries. We conduct experiments on building and
optimizing Clang binaries using LLVM-BOLT with two data sources: perf-captured
sample data and Intel-PT captured data. The analysis aims to determine if
supplying finer-grained and more branching information to LLVM-BOLT results in
further optimization of Clang binaries.

\section{Background}
Modern compilers have integrated a lot of compiler optimization techniques, such
as using basic block frequencies and branch target information to perform
function inlining.  But they don’t have the capability of identifying hot or
cold blocks of code which can help in function splitting to improve code
locality or optimize instruction cache, fortunately this can be achieved using
feedback-driven (FDO), or profile-guided optimizations (PGO). Traditional
methods such as using instrumentation-based profilers have significant memory
and performance costs for collecting profiles, and they might not be able to
capture accurate profiling information. With FDO, we use unmodified binaries to
capture profiling data and have only the overhead of collecting the data with
much finer and accurate information. It is also easily adoptable in production
environments, as one can do continuous optimization by regularly feeding dynamic
profiling data to rebuild the binary. Facebook’s BOLT (which was moved to
llvm-project) uses sampled profile using perf (with and without LBR) to perform
post-link FDO on binaries.

\subsection{Profiling Techniques}
\subsection*{Perf}
Perf, also known as perf\_event, is a lightweight Linux-based tool that offers
an abstraction over hardware-specific capabilities, primarily for performance
measurements.  Commonly referred to as Performance Counters for Linux, it
provides per-task, per-CPU, and per-workload counters, along with the ability to
sample on top of these.  Perf has four type of measurable events. Such as from
pure kernel counters, in this case they are called \textbf{software events}.
Examples include: context-switches, minor-faults.  Another source of events is
the processor itself and its Performance Monitoring Unit (PMU). It provides a
list of events to measure micro-architectural events such as the number of
cycles, instructions retired, L1 cache misses and so on. Those events are called
\textbf{PMU hardware events} or hardware events. They vary with each processor
type and model.  In \textbf{hardware cache events}, each processor gets mapped
onto an actual events provided by the CPU, if they exists, otherwise the event
cannot be used.  And \textbf{tracepoint events} which are implemented by the
kernel ftrace infrastructure.

The perf tool is in the linux-tools-common package and can also build from the
Linux kernel source.  perf or perf\_events can instrument in following ways:
\begin{itemize}
    \item \textbf{counting} events in-kernel context. In counting modes, the
        occurrences of events are simply aggregated and presented on standard
        output at the end of an application run. This mode does not generate a
        perf.data file. We can use perf stat to generate statistics.
    \item \textbf{sampling} events write event data to a kernel buffer, which is
       asynchronously read by the perf to write in perf.data file. Sampling periods are
       based on event occurrences and not timer ticks. perf\_event emulates 64-bit
       counter. And every time counter overflows (i.e wraps $2^64$ to $0$) a sample is
       recorded, including instruction pointer when the program was interrupted.
\end{itemize}

The generated file can be read by report and script commands.

In Sampling mode with perf record , care needs to be taken about the overheads,
as the capture files can quickly become hundreds of megabytes. It depends on the
rate of the event you are tracing: the more frequent, the higher the overhead
and larger the perf.data size.

Another way to instrument is by using BPF program on events which can execute
custom user-defined programs in kernel space, and perform filters and summaries
of the data. But we won’t be covering that in our experiments.

By default perf records sample at \textbf{both} user and kernel levels using
cycles events with average rate of $1000$ samples/sec or $4000$ events / sec of
thread execution.  With Sampling `perf record` asks PMU to count some events,
for example cpu-cycles and generate an overflow interrupt after every $n$th
event (e.g a million). On every interrupt perf\_event records such as : PID/TID
(Process/Thread ID), timestamp, command being executed, Instruction Pointer to
the ring buffer and reset the counter to a new value.

There is a limit to maximum frequency that can be set but we can adjust both
frequency of collection samples and period i.e collecting sample every Nth
occurance of an event by passing -F and -c flag respectively to perf record.
Once the ring buffer is filled, perf will dump the data to `perf.data` file
which can be of several megabyte in size.  

\subsection*{LBR}
Intel’s Last Branch Record (LBR) are hardware registers used to records
information about branch instructions that a CPU takes, it logs “From” and “To”
addresses of each branch along with some additional metadata. The register has a
ring buffer with most recent branch information, and is usually have stack depth
of 8, 16, or 32 frames.  

\subsection*{Intel-PT}
Intel-PT is a hardware based tracing technique integrated into CPU hardware,
generating data for every control flow instruction which can be later used to
resconstruct the whole control flow of a program. The frequency of capturing
data is same as number of calls, branches and returns executed per second by the
program. This means that per CPU, Intel-PT can capture hundreds of megabytes of
trace data.  

\subsubsection*{Buffer Handling:}
In Intel-PT mode, perf\_events saves log to different ring buffer and perf’s
user space tool will periodically save data from ring buffer for offline
parsing, filtering and decoding.

The default rate is set at 4MiB/page size for privileged user and 128KiB/page
size for unprivileged users. Passing the larger buffer size is possible, but it
means that multiple buffers will be logically concatenated. To switch between
these buffers, an interrupt is made; sometimes these interrupts are not made in
time and buffer overflows which results in trace data lost. Sometimes passing
larger auxtrace mmap size helps in resolving the trace data loss, which can be
done by passing `-m` flag.So, passing `-m, 64M` means we can set trace buffer
size to 64MiB . However, buffer size is set per CPU and we need to be careful
with choosing auxtrace mmap size. So if we have 8 CPUs we are essentially
setting total 512MiB of trace buffer size.

Another method which sometimes seems to be helpful in addition of setting AUX
mmap size is re-running the workload to resolve overflow packets error. This
will capture slightly less data(a couple of MiB) but you will either see no or
fewer “Instruction errors” in captured data.

\subsubsection*{Background on Bolt}
LLVM-BOLT is a static post link binary optimizer built on top of LLVM compiler
infrastructure. It uses sample based profiling data to reorder function blocks.

BOLT requires profiles to be collected with relocation mode (by passing
`—emit-relocs` flags) this helps BOLT to reorder basic blocks and split hot/cold
blocks for improving code locality. Function discovery is initial step in order
to modify the binaries. BOLT relies on ELF symbol table and function frame
information (which contains functions boundaries) for function discovery.

BOLT relies on using sample based profiling methods(i.e `perf` ) to get data,
the major advantage of sample based method in comparision to instrumentation one
is better (or more accurate) information with minimal operational complexity.

\section {Optimizing Clang with Bolt}
We decided to replicate optimizing Clang binary with llvm-bolt with some
additional steps.We build clang( version 16.0.3) and bootstrapped it with PGO
and LTO. Finally, we use the bootstraped version to collect profile for a new
clang build in three modes NO LBR, LBR and Intel PT. llvm-bolt only supports
first two, and to make Intel PT work we had to make few tweaks to llvm-bolt’s
source code.

In LBR mode, llvm-bolt uses perf to fetch PID/TID, Instruction Pointer (IP) and
Branch Stack (brstack). llvm-bolt expects branch flags to either predict or
mispredict, however branch flags are not supported by Intel PT (why?)

To make llvm-bolt work we adjusted configuration option in perfconfig file
which sets mispred flag on all branches. Simply adding following lines to
~/.perfconfig lets us get brstack information out of sampled data
collected with Intel PT

\begin{lstlisting}[language=bash]

[intel-pt]

mispred-all = on

\end{lstlisting}


Next adjustment was modifying llvm-bolt’s source code for data aggregation,
adding \texttt{—itrace=i100usle} option in conjuction with perf scripts lets us
extract brstack field in PT mode. Throughout the report we have used multitime
to benchmark runs, this helps us to collect sample mean and standard
deviations.

\subsection*{Single run comparisions}
In this experiment, we established two distinct sets: a training set to
generate the profiles for constructing the new version of clang, and a testing
set to procure the mean wall-clock time and the 99\% confidence interval. The
latter was used to compare the performances of various clangs, each operating
the testing set file(s) one hundred times.

The experiment was conducted in three distinct modes: NO LBR, LBR, and Intel
PT. The term 'mode' here refers to the specific flags employed, if any, during
the profile capture of the training set file compilation.

Within the table, the first column presents the testing set files, while the
succeeding columns represent clangs, each built with the assistance of training
set files and consequently named with the corresponding filename extension.

Moving on to Table I, we will analyze the performance of the best clang build
across the NO LBR, LBR, and Intel PT mode profiles. The following table
represents the difference:

\begin{table}[htbp]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
    \toprule
    Testing file & Baseline & \multicolumn{1}{c|}{NO LBR} & \multicolumn{1}{c|}{LBR} & \multicolumn{1}{c|}{Intel PT} \\
    \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
    format.c & & 0.735 ± 0.0005 & 0.723 ± 0.0005 & 0.701 ± 0.0008 \\
    tty.c & & 0.449 ± 0.0005 & 0.442 ± 0.0005 & 0.428 ± 0.0005 \\
    window-copy.c & & 0.715 ± 0.0008 & 0.704 ± 0.0008 & 0.680 ± 0.0008 \\
    extsmaild.c & & 0.209 ± 0.0005 & 0.205 ± 0.0005 & 0.201 ± 0.0005 \\
    \bottomrule
  \end{tabular}
\end{table}

It can be observed that, on average, the Intel PT mode performs 4.76\% better
than the NO LBR mode and 2.89\% better than the LBR mode. To probe into why the
Intel PT mode clang outperforms the others, we will examine the number of
functions that llvm-bolt aims to reorder and the actual percentage of functions
reordered in the three modes.

\subsubsection*{Analysis of number of function recorded during compilation,
number of function perf captures in sampling mode and number of functions Bolt
reorders}

For training we used clang build with extsmaild.c file in all thre modes, and 
for analysis we ran clang binaries on extsmaild.c to capture perf profiles 
and analysize number of functions recorded duing compilation.

\begin{table}
    \centering
    \caption{BOLT Reordering Data}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Method} & \textbf{Perf Captured} & \textbf{BOLT Reorder Aim} & \textbf{BOLT Reordered \%} \\
        \midrule
        NO LBR          & x                     & 311                       & 73.95\%                   \\
        LBR             & y                     & 1501                      & 71.95\%                   \\
        Intel PT        & z                     & 4177                      & 69.43\%                   \\
        \bottomrule
    \end{tabular}
\end{table}

Out of 175120 functions in Clang binary, llvm-bolt reorders 230
functions in NO LBR mode, 1080 functions in LBR mode and 2900 functions in Intel
PT mode. The unordered functions are ...

Functions captured during compilation (using valgrind) versus function 
captured by perf during sampling.

We will be using two tools, Callgrind, a profiling tool included in Valgrind
suite and perf which we use to capture profile and for profile guided optimization.

Callgrind tracks the callgraph (in simple terms function call hierarchy) of a
program during its execution. It dumps information about number of times each
function is called, number of instructions being executed and number of cycles
consumed by each functions in a callgrind.out.PID file. We are only interested
in number of functions being recorded. We used QCachegrind to study the
dumped data (MacOS version of KCachegrind).

The next question we aim to answer is the percentage of functions that perf
successfully captures. Is there a correlation between a higher number of
captured functions in a profile and a superior clang build (Hypothesis III)?

In order to delve further, we will formulate some hypotheses:

\end{document}
