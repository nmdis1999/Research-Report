%&main_preamble
\endofdump
\begin{document}

\maketitle

\begin{abstract}
    \noindent
    JIT compilation gives dynamically typed languages speed of compiled code along
    with the flexibility of interpretation. One of the variants of JIT is tracing
    just-in-time compiler, which observes the program at the runtime and collects
    the most frequently executed parts in a linear execution trace.  and optimises it
    into machine code. However, building a tracing JIT for a language can take up
    significant engineering hours. Meta-tracing systems such as yk can easily turn
    existing C interpreters into tracing JIT compilers. At the current stage,
    however, yk pessimistically turns off all of the optimisations. 
    The result is a significant slowdown of the
    interpreter even when JIT is not enabled. My work aims to regain the lost
    performance. In this report, I show that by enabling performance critical
    passes at all levels of LLVM's optimisation pipeline, we can bridge the gap
    between current performance and the target performance of the interpreter.
\end{abstract}
\section{Introduction}
\subsection{Overview}

The popularity of dynamically typed languages stems from their ability to ease
writing software and rich library framework they provide. In recent years,
dynamically typed languages such as Python and JavaScript are not just popular
for writing scripts but also complex server side applications\cite{dyn-server}.
The most commonly used implementation of these languages are interpreters, as
they are quite flexible to use and the developer don't need to incur cost of
compilation\footnote{example: }. However, the same flexibility that makes these
languages popular among software developers also bring challenges to
optimisations and code generations. As these languages have become one of the
key tools in software development kit they demand improvement to their execution
speed\iti{rewrite this line}. An argument can be made that developers can pick
compiler for these languages instead of using interpreter. But there is a catch,
dynamic languages tend to be quite slow to run when compiled~\cite{pypy}. In
statically typed languages compilers rely on the type information for generating
machine code. However, for dynamic languages such as that of Python, the type
information may vary at the runtime. This means that compiler will have to emit
generalised machine code to deal with all the possible type combinations every
time we run the program, which can be very expensive in terms of space and time
complexity.

It has been stated that implementations of dynamically typed languages are often
slower than the implementation of statically typed languages\cite{ltratt
dynamically typed}. {why?} There has been continuous research on making
interpreters faster which dates back to Self project\cite{add here} but they
aren't used as frequently as one would want. Most dynamically typed languages
uses bytecode interpreter without any advance optimisations such as just-in-time
compilations. The reason for limited use of JIT compilers is difficulty of their
implementations\cite{bolz thesis}. A JIT compiler for the dynamically typed
language would need to handle all the complex interactions of the language while
simultaneously generating efficient code for the common cases. 

One solution to this problem is using meta-tracing system. With meta-tracing
systems such as yk, language implementers do not need to implement their own
tracing JITs; instead, they can use yk meta-tracing system and the tracing JIT
compilation will automatically be embedded in the language implementation(s). 

One of the newer approaches of implementing just-in-time compilers is tracing
JIT. In tracing JIT compilation, the Virtual Machine\iti{introduce} identifies
frequently executed, ``hot'' program paths at runtime. Once a hot path is
detected, the VM records all instructions in this path into a trace until
execution loops back to the first instruction of this path. The trace is then
compiled and the subsequent iterations of this path are executed using the
compiled trace instead of the original code. But developing custom tracing
JITs is a hard undertaking. The concept of tracing optimisations was initially
explored by Dynamo Project~\cite{dynamo}, which was successful in using tracing
information at runtime to optimise most executed paths. This technique is also
used in Mozilla's TraceMonkey JavaScript VM~\cite{tracemonkey}and in Chrome's V8
through their tracing V8 system~\cite{v8jit}.
 

YK is a meta-tracing system which traces the execution of the interpreter instead of tracing
the user program executed by the interpreter. At current stage, yk does not
support interpreter compilation using any other level than -O0, and even at that
level it disables all of the backend optimisations. When we enable optimisation
passes at different level of optimisation passes they either interfere or undo
work done by some of the yk's passes by changing the intermediate
representation. In yk, when the \iti{add word} switches from JIT compilation to
interpretation the yk system tries to map back machine basic blocks to LLVM IR
basic block. The yk passes makes sure that the mapping is accurate else
correctness and efficiency of the meta-tracing system will be compromised. We
discuss this further in \label{blah}.

The result of yk pessimistically turning off all of the optimisation passes is
that the performance of yk compiled interpreter is significantly slow even when
JIT is turned off. What this means is simply embedding yk onto the interpreter
has \iti{add word} cost.

\begin{figure}[ht!]
    \label{figure:example}
    \centering
    \scalebox{0.8}{
    \includegraphics[scale=0.5, width=\linewidth]{images/multilang.pdf} 
    }
    \caption{Example of how dynamic languages need individual JITs without
    meta-tracing system, while with meta-tracing system multiple languages can
    use the same JIT without having to write their own tracing JITs}
\end{figure}

In this report, I show improvements I made which reduced the performance overhead
when retrofitting yk onto existing C interpreter. The aim of the report is to lessen the
gap between performance of interpreter which is compiled via yk's modified
implementation of llvm called ykllvm and the interpreter which is compiled with
gcc or llvm's clang.

The main contributions of this report are:
\begin{itemize}
    \item{A parallel genetic algorithm to find llvm optimisation passes that do not
        break yk's tests and improve the performance of yklua, a C interpreter.}
    \item{}
        {Identfying and fixing some of the causes of overhead when using yk to build
        yklua}
\end{itemize}

\subsection{Defining the problem}
The first hypothesis I test in this report is:

\textbf{H1} \hspace*{0.5cm} There exists a subset of existing compiler
optimization passes (drawn from the current list of optimizations available in
LLVM) that is compatible with the meta-tracing system and meaningfully improves
the interpreter's performance.
\\
To validate this hypothesis, I started out with a divide-and-conquer algorithm
to find a pass sequence compatible with the meta-tracing system. Compatibility
is defined as the successful passing of the meta-tracer system's test cases and
the pass sequence's ability to build and run the C interpreter. However, while
this method found a compatible pass sequence (which didn't break yk's test
cases), it did not find a pass sequence which improved the interpreter's
performance and, in some cases, even made it worse. Later, I will present a
better approach using a stochastic genetic algorithm, which not only found
multiple compatible pass sequences but also yielded sequences that began to
improve the interpreter's performance.

There are total of 3 hypothesis in this report, the second and third hypothesis
are discussed in Section x.

\iti{write structure in better way}
The report is structured as follows: Section \ref{sec:background} is the background,
the next section i.e~Section \ref{section:3} addresses interpreter's performance in
meta-tracing system. This is followed by Section \ref{section:4} and
\ref{section:5} describing the experimental setup and what methodology is used to
conduct the experiments. Section \ref{section:6} discusses results, and finally I
present Section \ref{section:7} for addressing related works and wrapping the
report with Section \ref{section:8} i.e~conclusions and future Work.

\section{Background}
\label{sec:background}

This section provides background on dynamically typed languages, just-in-time
(JIT) compilation, the tracing JIT compilation technique, and meta-tracing. The
highlight of this section is meta-tracing systems, as they are an important
component of this report. Additionally, this section provides details on
RPython, a production-grade meta-tracing system, and yk meta-tracing, a
hardware-based meta-tracing system on which my experiments have been carried
out.

\subsection{Dynamic typed languages and their implementation}

Dynamic typed languages have seen a rapid increase in their use for developing
software not just for the web but also for domains that previously only used
statically typed languages\iti{https://dl.acm.org/doi/pdf/10.1145/1543135.1542528}. The appeal
often lies in their ease of development and the flexibility they offer. Dynamic
languages defer type-safety to runtime, unlike statically typed languages which
handle it at compile-time, which affects their performance negatively.

Although dynamically typed languages can be compiled ahead-of-time (AOT), they
often run slower when compiled. This is partly because, in statically typed
languages, compilers can utilize type information to create optimized machine
code. However, in dynamically typed languages, like JavaScript, types can
change, leading to optimization challenges for compilers. Consequently, they
must generate more general, slower code to accommodate various types. Attempts
to get interesting static information for languages like Python have produced
limited results [Can05] or are quite complex and don’t scale well [Sal04]
because of the nature of dynamically typed languages.

Techniques like static type inference can improve performance by predicting
types, but these approaches are complex and not suited for environments that
require rapid updates, such as web pages in real time. For example, optimizing a
web app for quick loading and interaction becomes more challenging with dynamic
typing due to the additional analysis required.

Implementation of dynamically typed languages is not an easy task as they are
not usually designed for speed. Some of the challenges when implementing
dynamically typed languages are:
\begin{itemize}
    \item{Late Binding}
    \item{Dispatching}
    \item{Boxing}
\end{itemize}

However, the categorisation of dynamic and statically typed languages are not
always very clear. A statically typed language can have dynamic feature, one
such example is Jave which often use dynamic method dispatch, dynamic code
execution, type inference etc. All the dynamic feature results in limited
performance gain from ahead-of-time (AOT) compilation. This is because to generate efficient
and type-specialised code the compiler knowledge of variable type.

Interpreters are often preferred method for implementing dynamically typed
languages such as Python, Ruby, and JavaScript. This preference stems from their
ability to evaluate code line-by-line, which aligns better with the dynamic
nature of these languages. This method is easier to manage with runtime
type changes, making interpreters more suitable for dynamically typed languages
than AOT compilationi\iti{[Can05]}.

However, while interpreters provide convenience and simplicity, they inherently
suffer from slower execution speeds.

These challenges has resulted in years of research to bring performance of
dynamically typed languages to an acceptable performance levels.Early results
for Lisp [SJG93] and Smalltalk [DS84] involved extensive research around the
Self language [CUE89, HU94, Hö94], which was later applied to the Hotspot VM
[PVC01]. The common theme in all of these research was they used a JIT compiler
[Ayc03] to compile and recompile parts of the program at runtime, this was done
for most executed part of the program. JIT compilation gives advantages of 
interpretation with the performance benefits of compiled code, optimizing
execution speed without compromising the flexibility dynamic languages require.

However, the challenge with Just-in-time compilation is that it is not an easy
implementation technique, especially when the language it is targeting is 
itself a complex one. The challenge is to maintain the development of such
JIT compilers with the pace of the language development, this requires hundreds
of engineers effort spanning over multiple years.

Executing a dynamic programming language on interpreter has large overhead when
compared to running same program using statically typed language like C. In next
subsection we will be looking at how JIT techniques such as tracing JIT can help
us to overcome some of these overheads.

Figure shows typical working on JIT. 
\subsection{PyPy and RPython}

PyPy is a framework to write efficient implementation for dynamic languages such
as Python. The necessary condition to use pypy is that the interpreter's implementation
has to be written in RPython (``Restricted Python'')[AACM07]. RPython is a
subset of python. First the language interpreter is written in RPython and then
later translated into target environment (usually C/Posix). One of the low-level
detail that is added while translation is MetaJIT, which is language independent
tracing just-in-time compiler. In label{meta-tracing} I explain more in detail
about what it is and how it works.

Many dynamic languages use bytecode interpreters without advanced technique like
JIT compilation. The reason for the limited use of JIT compiler is complexities
of their implementation.  Languages such as JavaScript and Python have very
complex core semantics with many corner case.  So how does these languages use
JIT to improve their performance without dealing with writing their own JITs?
One solution is meta-tracing. Meta-tracing is a technique to efficiently execute
dynamic languages without having to rewrite a dedicated JIT for each of them,

\subsection{JIT and Meta-compilation}

There are different ways to implement just-in-time compilers, one way is by
using tracing JIT. A tracing JIT records and compiles traces of program being
interpreted. One of the important topic to be discussed in this report is
insertion of MetaJIT in the interpreter. MetaJIT is a kind of tracing JIT
compiler, in this section we will cover what tracing JIT is and how they relate
to MetaJIT.

The first project to explore tracing JIT was Dynamo, in which tracing
optimisation were used to dynamically optimise machine code at runtime. Later
the optimisation technique were used by Java Virtual Machines [GPF06, GF06, BCW+10, IHWN11].
Due to ease of their implementation tracing JIT became popular and were later
used in projects like Mozilla's TraceMonkey and LuaJIT. 

A trace is a sequential instructions which can be executed consecutively during
program's execution. Tracing refers to the process of capturing the information
about the execution of a program i.e~traces of the program. Tracing JITs use the
technique where they collect/record information about most executed (or ``hot'')
paths via traces of the program and later at runtime, uses the information to
perform optimisations and make inlining decisions\cite{tracing-jit}. 

Tracing JIT makes assumptions such as:
\begin{itemize}
    \item{Programs spend most of their time inside the loops}
    \item{Several iteration of the same loop are likely to take the same
        execution path}
\end{itemize}

\iti{add figure about tracing jit from ipad}

\subsection{Retrofitting tracing JIT to an interpreter}
\label{meta-tracing}
The tracing JIT in yk is applied to the interpreter running the user program
instead of it being applied directly to the user program. A tracing JIT
finds the hot loops in the program to compile. Since in yk's case that program is
an interpreter the meta-tracing system care about interpreter's loop which is
bytecode dispatch loop(i.e~if we consider bytecode interpreter) as well as
the hot loop in the user program.

In a simple bytecode interpreter the dispatch loop is sometimes the only hot
loop present in the whole program. This loop is responsible for executing a
opcode with each iteration. As it is unlikely that same opcode will run multiple
times in a row the guards fails very often resulting in no performance
difference\iti{add bolz paper} as the tracing JIT's assumptions fails. So, to
optimise interpreter's performance yk uses loop unrolling. With loop unrolling
we trace the execution of multiple opcodes effectively unrolling so much so that
we end up with one user loop. A user loop is a hot loop in the user program, a
loop is called hot when the interpreter's program counter reaches the same value
multiple times.

The program counter is stored in one of several variables in the interpreter.
To figure out which variable holds value for the program counter, the MetaJIT
needs hints (via a dummy control point). We need a dummy control point as LLVM
also adds its temporary while generating the IR. The dummy control point is
later(at runtime) replaced by original control-point. The loop is considered
closed only when all the variables that make up the program counter are same in
second iteration. This is how the interpreter traces a single user loop.
\iti{draw a diagram}
\iti{add example for adding hints}
\iti{Figure 4 from Bolz example of trace in tracing and meta-tracing jit from Bolz paper}

\begin{figure}[ht!]
    \label{figure:example}
    \centering
    \scalebox{0.8}{
    \includegraphics[scale=0.5, width=\linewidth]{images/program_and_trace.pdf} 
    }
    \caption{Example of how trace of a program looks like(inspired from
    Wikipedia). Left hand side is a python program and right hand side is image
    of the program's trace}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{images/trace.pdf}
    \caption{\small In tracing JIT (which is the core of meta-tracing system), conditions are turned
    into guards. The guards are responsible for checking whether the conditions are
    being met (i.e~they are true) or not. If the conditions are true, and a trace of it is
    already compiled then instead of interpreting the condition again, the compiled trace
    is used by the interpreter.}
    \label{traces}
\end{figure}

The main difference between a tracing JIT and meta-tracing JIT is that, the
former is used to record and compile traces of a user program as it is being
executed, while the later works one level down and is used to record and compile
traces of interpreter as it executes the user program\cite{meta-tracing}.

Meta-tracing (also often called MetaJIT) is a form of meta-compilation which
can be used to build JIT virtual machines for dynamic languages by feeding some
hints in the existing interpreter for the language. This makes it much easier
for developers to use tracing JIT for their languages without writing one
themselves.

As MetaJIT is used to trace the operation of the interpreter, the tracing needs
to record a full loop in the user program. Tracing the behaviour of the
interpreter helps in reusing the MetaJIT for different language interpreters, as
it would work independent of the language semantics.

\iti{add diagram normal tracing JIT and Meta-JIT}

\begin{figure}[htbp]
    \centering
    \scalebox{0.5}{
        \includegraphics[scale=0.01, width=\linewidth]{images/yk_metatracer.pdf} 
    }
    \caption{Figure shows how ykllvm introduces control-point and compile
    (yk)lua binary. The control-point is necessary so that the meta-tracing
    system can identify at what point to start recording tracing. ykllvm also add
    some additional data such as builtin tracer and LLVM IR into the yklua binary}
    \label{traces}
\end{figure}

\begin{figure}[htbp]
    \centering
    \scalebox{0.8}{
        \includegraphics[scale=0.5, width=\linewidth]{images/aot_and_jit.pdf} 
    }
    \caption{\small The yk meta-tracing system uses ykllvm
    to compile the interpreter's source code into a binary, which is called ahead of
    time compilation (or AOT compilation). This binary has a built-in tracer, a jit
    compiler and LLVM IR. The LLVM IR is then used to get the tracing information
    from the binary. The binary is generated via ykllvm and is used to interpret the
    user program. If the program has a loop, where a loop means the tracer comes
    back to the point it started collecting trace from, then the builtin tacer
    records this information. A trace starts getting collected when the meta-tracing
    system detects control-point in the interpreter, which can be inserted by the
    user initially. This control-point is later converted into a yk specific control
    point. After collecting the full trace, the trace is compiled which is called
    just-in-time compilation (or JIT compilation) and then executed as a part of the
    meta-tracing system. If, while executing the user program, both loop and
    complete traces are detected (which can be a later iteration of the loop) then
    instead of recording and compiling the trace again, the already compiled trace
    is used for execution.
    }
\label{traces}
\end{figure}

\begin{samepage}
\subsection{High level view of yk meta-tracing system}

yk uses hardware tracer\footnote{The hardware tracer is Intel PT based, Intel
Processor Trace (Intel PT) is a feature available in some Intel processors that
provides a hardware-based tracing mechanism, enabling low overhead collection of
program execution traces.}.And Intel PT can only store a stream of basic blocks
and give their virtual addresses~\cite{intel-pt}. So, block mapping is used in
hardware tracing to get from a traced virtual address to a file offset in the
binary, to a LLVM IR block. The yk meta-tracing system depends on loader API and
a special LLVM section to do this. This section is emitted just before backend
optimisation.
\end{samepage}

At JIT time, the tracing interpreter records the basic blocks (jumps). When
the tracing interpreter switches back to interpretation (when a guard fails), it
needs to map the information back at the AOT basic block; this is where LLVM IR is
used. Meta-tracers such as yk need to do this with a hundred per cent accuracy.
The JIT compiler relies on the traces to identify hot paths in the program for
optimisation\cite{hot-trace}. If the mapping from the traced addresses to the LLVM IR blocks is
inaccurate, the JIT compiler might optimise the wrong code paths or apply
optimisations based on incorrect assumptions. This can lead to the generation of
incorrect or inefficient JIT-compiled code, affecting the program's correctness
and performance.

However, some llvm optimisation passes mess with the basic-block mapping. One
such example is when basic blocks are merged, some blocks may be left
unaccounted for in the trace. Additionally, llvm applies some passes at different
stages of the pipeline, even after the final IR has been generated. This again
interferes with mapping and creates problems for the meta-tracer. To avoid this
problem, the yk meta-tracing system turns off all the optimisations by default,
which slows down the interpreter significantly even without the JIT being
enabled.


\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{images/block-merge.pdf}
    \caption{\small High Level IR and Machine IR side by side after backend
optimisation. After some backend optimisations some basic blocks get merged into
one. And the previous basic block which contained some operations are either
empty or deleted. This creates mapping issue in meta-tracing systems.}

    \label{traces}
\end{figure}

Modern compilers have a huge number of optimisation passes available. These
optimisation passes target unique segments of a program, such as transforming a
function, basic block(s) or whole program\cite{llvm-pass}. Further, these optimisations can be
applied at different stages of compilation pipelines. The sequence of passes tries
to retain the semantics of the program while potentially improving the program's
performance. There are two types of passes: Analysis and Transformation. Both
of these passes work together: the analysis passes collect information about the
program and transformation passes are responsible for using the collected
information and changing the program. Examples of optimisation levels provided by
modern compilers are: -O0, -O1, -O2, -O3, and -Og. The two major compilers gcc and
Clang have 200 and 100+ optimisation passes respectively; their sequence is
called an optimisation sequence. By default, without an optimisation level, all of
the passes are turned off and an expert can choose to turn on and off passes
accordingly~\cite{kulkarni}.

\subsection{LLVM Project}
LLVM is a set of compiler and toolchain technologies and has its own
Intermediate Representation (LLVM IR). This language-independent IR helps LLVM
compilation to be target and source independent. The LLVM IR is optimised
through a sequence of LLVM passes. Passes are an operational unit of the IR which
can:
    \begin{itemize}
        \item{mutate the IR}
        \item{perform some computation about the IR}
        \item{or just print something}
    \end{itemize}

The scope of each pass is a unit of an IR; it is what the pass is going to operate on. This
can be :
\begin{itemize}
    \item{a module: LLVM module is a top-level structure that represents a
        single  unit of code which is processed together. It includes global
        variable(s), function declaration(s), and implementation(s).}
    \item{a function: LLVM function is a self-contained unit of execution within
          a module. It corresponds to a function in the source code and contains
          a list of aruguments, a basic block, and a symbol table.}
    \item{a basic block: LLVM basic block is a linear sequence of instructions
          within a function, and has a single entry and exit point. They are
          used to represent the straight-line code sequence that makes up a function's
          body.}
    \item{an instruction: an LLVM instruction is the smallest unit of execution
         in the LLVM IR, representing a single operation within a basic block.
         Instructions can perform a wide range of operations, including arithmetic,
         memory access, logical operations, and control flow changes. Each instruction
         produces a result and can have zero or more operands, which are either constants
         or the results of other instructions.}
\end{itemize}
These passes are categorised into analysis, transformation and utility passes
depending on whether they perform computation on the IR, mutate the IR, or do
some other non-categorised task.

\subsection{LLVM's optimisation pipeline}
Classically, compilers are split into three parts: frontend, middle-end and the
backend. The frontend deals with language-specific analysis i.e~operations such as
parsing, type checking and so on. The target-independent optimisations are
performed in the middle-end. And finally, the backend applies the target-specific
optimisations and emits the machine code.

Although languages such as C, C++, and Rust etc. have their own frontends,
but they rely on LLVM for middle and backend optimisations.

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, 
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]
Frontend                   LLVM
/---------\               /-----------------------------------\
|  clang   |              |                                   |
|  rustc   | --(LLVM IR)->| Middle end --(LLVM IR)--> Backend | --> Machine code
|   ...    |              |                                   |
\---------/               \-----------------------------------/
\end{Verbatim}
\end{tcolorbox}

\subsubsection{Type of optimisation pipelines}
For H1, we are mostly concerned with LLVM's middle-end optimisation
pipeline. LLVM has three kinds of optimisation pipelines: the default (non-LTO)
pipeline, ThinLTO pipeline, and FatLTO pipeline.

\subsubsection{Default Pipeline}
The default pipeline optimises each module separately, without any special
knowledge about other modules. ``Module'' here refers to ``single source file''
in C/C++ and a single ``codegen unit'''in Rust language.

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, 
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]
Module 1 --(Optimise)--> Module 1'
Module 2 --(Optimise)--> Module 2'
Module 3 --(Optimise)--> Module 3'
\end{Verbatim}
\end{tcolorbox}

\subsubsection{ThinLTO Pipeline}

The LTO pipelines are split into a pre-link and a post-link optimisation
pipeline\cite{lto-prepost}. After the pre-link pipeline, ThinLTO performs some
lightweight cross-module analysis, such as importing certain functions from
other modules to make them ready for inlining. The post-link optimisation
however, works only on individual modules. Pre-link optimisations simplify the
modules for optimisations, while contrary to the name, post-link optimisations
happen during the link-time stage and perform both module simplification and
optimisations through passes.

\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, 
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]
                              cross-import
Module 1 --(ThinLTO pre-link)------\-/-----(ThinLTO post-link)--> Module 1'
Module 2 --(ThinLTO pre-link)-------X------(ThinLTO post-link)--> Module 2'
Module 3 --(ThinLTO pre-link)------/-\-----(ThinLTO post-link)--> Module 3'
\end{Verbatim}
\end{tcolorbox}

\subsubsection{FatLTO Pipeline}
Finally, after the pre-link step, FatLTO merges all modules together
and then runs the post-link optimisations on a single large module:

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, % Thickness of the box border
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]
             begin{Verbatim}[fontsize=\scriptsize,formatcom=\bfseries]
                                 merge
Module 1 --(FatLTO pre-link)-------\
Module 2 --(FatLTO pre-link)---------------(FatLTO post-link)--> Module'
Module 3 --(FatLTO pre-link)-------/
\end{Verbatim}
\end{tcolorbox}

While LTO stands for ``link-time optimisation'', it is not necessarily performed
by the linker. For example, rustc performs LTO as part of its
compilation process. 
LLVM’s optimisation pipeline looks like:

% Todo: add caption and citation to the diagram
\begin{tcolorbox}[
    enhanced,
    title=LLVM Compilation Process, % Caption
    fonttitle=\bfseries,
    boxrule=2pt, % Thickness of the box border
    sharp corners,
    colback=white,
    colframe=gray,
    coltitle=black,
    attach title to upper,
    before upper={\parindent15pt}
]
\begin{Verbatim}[fontsize=\scriptsize, formatcom=\bfseries]

|-------------------------- default ----------------------------|

|------------- pre-link --------------|
|------------------------- post-link ---------------------------|

/-------------------------------------\
|       module simplification         |
|-------------------------------------|   /---------------------\
|                                     |   | module optimisation |
|                    cgscc            |   |---------------------|   /---------\
| /-------\ /-----------------------\ |   |                     |   | backend |
| | early | |       inlining        | |   | vectorization       |   \---------/
| |cleanup| |function simplification| |   | runtime unrolling   |
| \-------/ \-----------------------/ |   \---------------------/
|                                     |
\-------------------------------------/
\end{Verbatim}
\end{tcolorbox}


In the LLVM LTO pipeline, module simplification includes pre-link optimizations
such as initial cleanup, inlining of functions, and call graph optimisation to
refine the modules for additional optimization processes. Following
this, module optimization during the post-link phase implements optimizations
like vectorization and dynamic loop unrolling to improve the program's overall
performance. Finally, the backend phase compiles the machine code,
integrating register allocation and optimizations tailored to the specific
architecture, resulting in a faster executable code.

For the yk meta-tracer, we work with FatLTO because it merges multiple modules into
one giving us full IR. yk can potentially work with ThinLTO and the choice of
going with FatLTO is just for the sake of convenience.

\section{Addressing interpreter performance in a meta-tracing system}
\label{section:3}
\laurie{don't give labels numbers, as that misses the point: you want textual
labels, so that if you move sections around the numbers shown in the PDF
automatically change. but if you use labels in the latex file itself, you'll
just confuse yourself!}

As mentioned earlier, meta-tracing systems which use hardware tracing mechanism
such as yk cannot enable all the optimisation passes by default, as they
reorganise the basic-blocks and thus mess the mapping when switching from JIT
mode to AOT compiled interpreter mode. Therefore, we need to carefully choose an
optimisation pass sequence which does not interfere with basic block mapping
while also improving the interpreter's performance.

Such a problem is called an optimising phase ordering problem. Optimising phase
ordering is a long-standing problem with substantial work done on
it~\cite{guidedGA, kulkarni, genetic-algo}. However, the optimisation pass
sequence which might work for one system may not perform well or even work at
all for another. Finding the right optimisation pass sequence depends on the
algorithm and the oracle used. The algorithm can be a recursive or stochastic
one, while an oracle is a test set which establishes an initial constraint.
Unless the pass sequence satisfies all the constraints, it will not be selected
in the algorithm. The oracle can contain constraints such as tests as well as
the execution time of the program. I have considered both yk's test case and
execution of user program with yklua as the oracle.

Since some optimisation passes depend on others to enable a program's
transformation, choosing the right sequence (and order of passes) is important for
a program's performance either with respect to execution time, code size or both.
The pass sequence is a subset of passes from the larger set, while the order of
passes means in which position the passes are placed with respect to
one another. However, with such a high number of passes (around 90) the search
space becomes large (if we try all sequences irrespective of ordering we would
have to try $2^{90} - 1$ possible combinations!).

In this report, I start with a divide-and-conquer algorithm to find a
meta-tracing compatible pass sequence and later move on to a parallel genetic algorithm
for the same. Genetic algorithm is a well-known class of stochastic algorithms
which is adopted based on the theory of evolution. This algorithm has been
used by several researchers to solve phase-ordering problems~\cite{guidedGA,
kulkarni, genetic-algo}.

Using a genetic algorithm also allows us to find a pass sequence for our oracle in
acceptable time, where acceptable time is finding a compatible pass sequence
within a couple of hours if not minutes because of the vast search space. Further,
the genetic algorithm is not only able to find a pass sequence that is
compatible with the meta-tracing system, but the pass sequence also ends up
improving the C interpreter's performance. 

However, there is scope for further improvement of yk retrofitted interpreter.
First, the genetic algorithm currently only find pass sequence for pre and postlink
optimisation pipeline and by default all backend optimisations are disabled both
in `-O0` and `-O2`. Second, some of yk's passes incurs significant overhead.
After finding the pass sequence which satisfies my oracle as well as improves
the yklua interpreter I proceed to investigate the other blockers which stops
yklua to be more efficient. The proceeding experiments start from understanding
slowdown incurred by yk's passes and later I move to find impact of disabling
all of the backend optimisations and whether it is possible to enable them
back in both `-O0` and `-O2` optimisation level.

\section{Evaluation}

\subsection{Methodology}
\label{section:4}
The experiments were conducted on a dual-socket Intel Xeon Platinum 8452Y
system, featuring an x86\_64 architecture with 72 total cores distributed across
two NUMA nodes, operating between 800 MHz and 3200 MHz \laurie{you'd better
hope they didn't vary across that, or the numbers will be bonkers :p you want
to do the benchmarking on whichever bencher you have exclusive access to. you
will need to stick it in `performance' mode so that (as far as possible) its
clock speed doesn't vary}.
\iti{mention experiments which ran on b9 (for performance gathering) and  the GA in b16 along with section
names.}
\laurie{list all versions of software, llvm, operating system, etc etc}

\subsection{Experiments}
\label{section:5}

\laurie{linking to prs isn't really the right thing to do here: you want a more abstract description, in general}

To verify H1, I wrote a divide-and-conquer
algorithm, tested it against yk's
test suite and compared performance of yklua compiled with pass sequence
retrieved from the algorithm against the default yklua binary (i.e~ one compiled
without any optimisations enabled). The result showed that the algorithm
couldn't find a pass sequence which satisfied both constraints i.e~ passing yk's
test suite and improving the yklua's interpreter's performance. Next, I wrote a
genetic\-algorithm\footnote{https://github.com/ykjit/yk/pull/900}. The genetic algorithm
successfully found multiple pass sequence which satisfied both constraint and
emitted the most performant one at the end. 

In all of the experiments done below, we use yk as the meta-tracing system and yklua as
the C interpreter being optimised.

\iti{add a section defining the oracle}
\subsubsection{Divide-and-conquer algorithm for phase ordering problem}
\scalebox{0.65}{
\label{dac-algo}
\begin{tcolorbox}
    \begin{algorithmic}[1] % The [1] argument enables line numbering
    \small
    \State Define structures for \textbf{Pass} and \textbf{PipelineConfig}
    \State Fetch all available optimisation passes

    \Function{test\_pipeline}{PipelineConfig}
        \State Execute the pipeline with the specified configuration
        \State Log the results
    \EndFunction

    \Function{attempt1}{passes}
        \State Test each pass in isolation
        \State Log which passes succeed and which fail
    \EndFunction

    \Function{attempt2}{passes}
        \State Start with an empty set of okay passes
        \State Recursively test combinations of passes
        \If{a combination is successful}
            \State Add it to the set of okay passes
        \EndIf
        \If{a combination with more than one pass fails}
            \State Split and test subsets
        \EndIf
    \EndFunction

    \Function{main}{}
        \State Fetch all passes
        \State Perform initial testing of passes in isolation (\textbf{attempt1})
        \State Perform combined testing of passes (\textbf{attempt2})
    \EndFunction
    \end{algorithmic} 
\end{tcolorbox}}

This algorithm has two functions, attempt1 tries each pass
individually to see which passes succeeds and which ones fail in isolation. It
takes a total of 36 hours to run attempt1. In attempt2, the algorithm starts with
an empty list of ok passes, recursively tests combinations of passes starting with
a full list and then splits the list in two if the initial list fails and only
adds the subsets which succeed to the okay pass list. The run time for attempt2
is between 9-10 hours. The initial version of the algorithm used to reshuffle
the list as well to make it stochastic, however, I noticed that we received no
pass sequence which was compatible with yk as well as improved yklua's
performance. Removing stochastic elements helped me to get at least a pass
sequence which was compatible with yk, but failed to provide performance
improvement to yklua. This is because some LLVM optimisations depend on others
to give them information. So if the order of pass sequence is not right some 
optimisation might not work because their dependencies are not present before
they are processed.\footnote{https://github.com/ykjit/yk/pull/829}

\subsubsection{Genetic Algorithm for phase-ordering problem}
A genetic algorithm is a stochastic search algorithm which borrows the concept of
evolution from nature. This algorithm starts from an initial list (often
represented in a bitwise form) called entities and explores different
combinations. This is done by combining elements from two entities by way of
crossover and mutation into a third entity, which may represent a new point in
the vast search space, that otherwise would have been left unexplored by search
methods like divide-and-conquer.

Some terminology for the genetic algorithm: \laurie{a general description of GAs would be better off in the background section}

\begin{itemize}
\item{Entity: In genetic algorithms, an entity (often also called an
``individual''' or ``chromosome'') represents a possible solution to the
problem being addressed. An entity is typically encoded as a string or
array, where the structure and content of this encoding depends on the
specific problem. Each entity has a set of properties (or ``genes'')
that can be altered and combined with properties of other entities
during the algorithm's processes. The fitness of each entity, determined
by a fitness function, indicates how good a solution it is with respect
to the problem.}

\item{Crossover: Crossover (also known as ``recombination'') is another genetic operator
used to combine the genetic information of two parent entities to generate new
offspring entities. The process involves swapping sections of the parents'
encoding to produce one or more children. The rationale behind crossover is to
take the good characteristics of each parent and combine them to produce better
offsprings, potentially leading to better solutions in subsequent generations.
There are various crossover techniques, such as single-point crossover,
multi-point crossover, and uniform crossover, each differing in how and which
parts of the parent entities' genes are exchanged.}

\item{Mutation: Mutation is a genetic algorithm operator used to maintain genetic
diversity from one generation of a population of entities to the next. It is
analogous to biological mutation. The process involves making random changes to
individual genes of an entity. This can involve flipping bits in a binary
string, changing numbers in an array, or altering any part of the entity's
encoding. Mutation introduces new genetic structures in the population by
creating variants of existing entities, which helps in exploring new solutions
and prevents the algorithm from getting stuck in local optima.}
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.5]{images/genetic_algo.pdf}
    \caption{\small Figure retrieved \laurie{what does `retrieved' mean?} from~\cite{guidedGA}, a parallel genetic algorithm used to find an optimisation pass
        sequence compatible with yk and yklua. Initially, it randomly generates
        a population of passes, where each entity is a binary array indicating
        whether a specific pass is active (1) or inactive (0). The main parallel
        component is the fitness function, which is a value associated with
        every entity indicating how ``optimal'' a pass sequence is. Optimal means
        a pass sequence which succeeds all yk's tests while also minimising
        execution time of the interpreter. Each entity's performance is tested
        in parallel, speeding up the process significantly. The algorithm uses
        tournament selection (reproduction operator) to pick parents, which are
        then recombined and mutated to form new entities, iterating through
        generations to improve pass sequence combinations. This evolutionary
        process continues until the optimal set of passes, leading to the best
        interpreter performance, is identified.}
    \label{genetic-algo}
\end{figure}

\laurie{this is a fairly weird way to define an algorithm: why not use more
conventional pseudocode? see e.g. Fig 6 in the ``don't panic!'' paper as an
example}
\scalebox{0.80}{
\label{algorithm:}
\begin{tcolorbox}
    \begin{algorithm}[H]
        \begin{algorithmic}[1] % The [1] argument enables line numbering
        \small
        \State Define structures for \textbf{Pass} and \textbf{PipelineConfig}
        \State Fetch all available optimisation passes

        \Function{test\_pipeline}{PipelineConfig $pl$}
            \State Execute the pipeline with configuration $pl$
            \State Log the results
        \EndFunction

        \Function{encode\_passes\_randomly}{Pass $root\_passes$}
            \State Recursively set `on` flag of each pass to 0 or 1 randomly
            \State Ensure at least one subpass is enabled if parent is enabled
            \State Return modified $root\_passes$
        \EndFunction

        \Function{decode\_passes\_to\_string}{Pass $passes$}
            \State Generate string representation of enabled passes and hierarchy
            \State Exclude passes with `on` set to 0
            \State Return the string representation
        \EndFunction

        \Function{evaluate\_fitness}{Pass $passes$, ...}
            \State Decode passes string into PipelineConfig $pl$
            \State Call $test\_pipeline(pl)$ to get execution time
            \State Add fitness score (execution time) to shared list
        \EndFunction

        \Function{tournament}{list $population$, list $fitness$, float $sp$}
            \State Select two individuals randomly
            \State Choose fitter one with probability $sp$ as parent
        \EndFunction

        \Function{crossover}{Pass $parent1$, Pass $parent2$}
            \State Create child passes by swapping random passes between parents
            \State Return child passes
        \EndFunction

        \Function{mutate}{Pass $entity$, float $mutation\_rate$}
            \State Recursively flip `on` flag of each pass with probability $mutation\_rate$
            \State Return mutated pass
        \EndFunction

        \Function{genetic\_algorithm}{string $parsed\_str$, ...}
            \State Create initial population of randomly encoded passes
            \For{$generation$ in $1$ to $generations$}
            \State Evaluate fitness of each entity in parallel
            \State Select elites (good fitness)
            \State Apply crossover and mutation to create new generation
            \EndFor
            \State Return best entity (set of passes)
        \EndFunction

        \Function{main}{}
            \State Set up temporary directories
            \State Get available passes using `get\_all\_passes`
            \State Run genetic algorithm to find best passes
            \State Decode best entity and print final passes
        \EndFunction
        \end{algorithmic}
    \end{algorithm}
\end{tcolorbox}}

\subsection{Algorithm's speed up after parallising}

The first version of the genetic-algorithm was not parallised and it took 4-5
hours to run 10 generations. I optimised the algorithm to calculate the fitness
function for each entity in parallel which reduced the run-time for 10
generation from 4-5 hours to 15 minutes \laurie{does that mean this is an ``embarrassingly parallel'' problem?}. This improved my efficiency in running
experiments and finding patterns in successful and non-successful pass sequence.
\footnote{commit with this change:
https://github.com/ykjit/yk/pull/900/commits/e597774190c2856cd19328c94401f44ea8bf3a8b}

\section{Results}
\label{section:6}
\subsection{Overview}

\laurie{an overview is a good idea, but this is very hard to understand,
because different baselines are being used at different places --- but that's
never made clear. it's vital to be clear what the baseline is at each point,
otherwise the numbers seem nonsensical.}

When I started the experiments, yklua without JIT turned on performed ~5x worse
than the non-yk lua interpreter. The aim of the optimisations performed in
the upcoming section was to reduce this gap caused by embedding yk onto the lua
interpreter. There is expected cost for profiling the interpreter without
turning on the JIT. Ideally we would want to bring the difference between the
two variants to 2x or lesser.

Results at a glance: \iti{format this list}
\begin{itemize}
    \item{the genetic algorithm is able to find pass sequences which satisfies our
        oracle's constraints as well as improves yklua interpreter's performance by 1.75x.}
    \item{yk-based passes have around 1.5x overhead \laurie{if we improved things by 1.75x but there's only 1.5x overhead, that means yklua must now be faster than normal lua! so something doesn't make sense with one of these numbers} and we can reduce this
        overhead by optimising some of the passes.}
    \item{Optimising stackmap insert pass resulting in about 7\% performance
        improvement.}
    \item{Macro optimisation done on yklua's code improving performance of
        some benchmark by 200x (I'll be elaborating this in results sections.)}
\end{itemize}

In addition to the above, I explore how we may be able to make some if not all
backend optimisations work with the oracle.

\iti{I also fixed...} Fixing errors in -O0 optimisation level, which were not letting us compile the interpreter and
the test programs with backend optimisations enabled \laurie{huh?}. This fix is
important as it will help us figuring out which backend optimisation
is problematic when combined with yk's passes. This is covered in \iti{add
section label here.}

\iti{add this section in introduction?}
Adding passes obtained through genetic algorithm to -O0 and by applying my macro
optimisations, I am able to show average performance improvement of \%. These
optimisations (both pipeline and macro-optimisation) respects the constraints
laid out by the oracle indicating they are sound to use.

\iti{move the table to appropriate section}
\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Benchmark       & Non-optimized yklua & Optimized yklua \\
                & (compiled in -O0)    & (compiled in -O0) \\
\midrule
db.lua          & 0.140 ±              & 0.130 ± \\
verybig.lua     & ±                    & 0.628 ± \\
constructs.lua  & ±                    & 4.576 ± \\
gc.lua          & ±                    & 0.471 ± \\
sort.lua        & ±                    & 1.288 ± \\
tpack.lua       & ±                    & 0.018 ± \\
varg.lua        & ±                    & ± \\
calls.lua       & ±                    & 0.198 ± \\
erros.lua       & ±                    & ± \\
files.lua       & ±                    & 0.201 ± \\
\bottomrule
\end{tabular}
\caption{Comparison of yklua benchmarks}
\label{tab:yk_lua_benchmarks}
\end{table}

\iti{add this either in the introduction of future work}
The bigger goal is to be able to make a custom optimisation with selected passes
from the middle and backend optimisation pipeline, thus decreasing the overhead
induced by retrofitting yk onto the interpreter (starting with the Lua
interpreter). Working on this goal also means that along the way, I'll have to
make macro-optimisation to yk and sometimes to the interpreter itself (i.e.~ the
way we introduce hints for JIT in the interpreter).

\subsection{Defining the Baselines}
There are two binaries which can be considered as potential baseline. We shall
refer to them as baseline 1 and baseline 2.

\textbf{Baseline 1:} yklua compiled with `-O0` (i.e~ all optimisations are turned off).

\textbf{Baseline 2:} non-yk lua compiled with `-O2` \laurie{find out what the right ticks are in latex -- you can't use ` for start and end ticks}.

\iti{make sure to put baseline 1/2 in further tables and references}

My initial goal is to reduce the overhead of retrofitting yk onto the
interpreter. To achieve this, I apply yk-compatible optimisation passes onto the
-O0 compiled yklua interpreter. The  optimisation passes that satisfy
the oracle are considered to be yk-compatible. Once I reduce the performance overhead when
compared to Baseline 1, I shift my aim to bring the performance of the already
optimised yklua closer to that of the -O2 compiled non-yk lua interpreter (i.e~
Baseline 2).

\subsection{Effects of applying a meta-tracing compatible pass sequence on the yklua
interpreter}

In this section I'll be using a pass sequence obtained through genetic algorithm
to improve performance of the yklua interpreter. Further, I will show that even
after applying full -O2 optimisation level we are still 3x slower than non-yk
lua partially due to yk-based flags interfering with optimisations which use the
lto backend.

\textbf{Adding Passes at -O0 Level} \\
As discussed in Section \iti{add lto section}, there are three levels of llvm
optimisation pipeline: prelink, postlink, and backend optimisation. I modified
yk-config to accept prelink and postlink optimisation passes, however
overwriting backend optimisation is rather challenging and will be discussed
later in this section. For the first set of performance numbers, I added a pass sequence (both
prelink and postlink) to the -O0 optimisation in yklua binary (i.e~ turning on the
passes found via genetic algorithm) with the help of yk-config.
\iti{APPENDIX}yk-config is a tool to apply yk-specific compiler and linker flags on the interpreter and other
systems which use yk's meta-tracing system. yk-config makes
configuring/compiling C interpreters easy by having the support to add `cflags`
(compiler) and `ldflags` (linker), this can be used to add the prelink and
postlink pass sequences found via the genetic algorithm by appending them to
cflags and ldflags.

Finally, I use ykllvm's clang to compile yklua binary, as
described in Section \iti{add about ykllvm}.
\iti{APPENDIX}ykllvm is a modified version of llvm
which applied yk specific flags to the final binary.

Table \ref{table:preandpostlink} shows performance numbers 
of adding prelink, postlink or both to yklua -O0 optimisation level. \iti{Row 1 and row 4 function as our reference points/baseline}.

\textbf{Compiling yklua with -O2} \\
yklua currently does not support compilation with a `-O2` optimisation level.
With the help of yk pass YkAfterIRPasses `optnone` attribute is applied on all
of the backend optimisation functions, which results in blocking all of the
backend optimisation. \iti{the previous statement is ambiguous. Tell that it works for O0 and you assumed it should work for O2 but does not.} The obvious question is, why does compiling only with
`-O2` fails and not with `-O0`? The answer turned out to be quite simple; unlike
`-O0`, to block backend optimisation functions in `-O2` we have to apply both
`optnone` and `noinline` attribute. 

The reason for using both noinline and optnone attributes together when blocking
optimizations at higher optimization levels like -O2 is related to how the LLVM
optimizer handles these attributes and the inlining optimization. At -O0
optimization level, the LLVM optimizer performs minimal optimizations, and the
optnone attribute alone is sufficient to prevent optimizations for a specific
function. The optnone attribute tells the optimizer to skip all optimizations
for the function, including inlining. However, at higher optimization levels
like O2, the LLVM optimizer becomes more aggressive and performs various
optimizations, including function inlining. The optnone attribute alone may not
be enough to completely prevent optimizations in this case. When a function is
marked with the optnone attribute, the optimizer will skip most optimizations
for that function. However, if the function is called from other functions that
are being optimized, the optimizer may still decide to inline the optnone
function into the calling function.

After adding the fix, yklua can be compiled using both `-O0` and
`-O2`. However, the problem remains that we cannot use the full set of passes
for yklua's compilation as they are incompatible with yk's test suite. The goal
is to make a single optimisation level which works for all of our oracles (in
this case they are the yk test suite and yklua's compilation and test suite).

\begin{table}[H]
\centering
\begin{tabular}{@{}p{4.0cm}lllcc@{}}
\toprule
\textbf{binary} & \textbf{prelink passes} & \textbf{postlink passes} & \textbf{optimisation level} & \textbf{Mean} & \textbf{95\% CI} \\ 
\midrule
yklua & no & no & `-O0` & 0.140 & 0.002 \\
yklua & yes & no & `-O0` & 0.097 & 0.003 \\
yklua & no & yes & `-O0` & 0.104 & 0.001 \\
yklua & yes & yes & `-O0` & 0.096 & 0.003 \\
yklua & no & no & `-O2` & 0.093 & 0.003 \\
lua & no & no & `-O2` & 0.034 & 0.004 \\
\bottomrule
\end{tabular}
\caption{The table shows the performance of the yklua interpreter built with and
without a pass sequence obtained from a genetic algorithm with 95\% confidence
intervals. The performance numbers are for 30 runs of `db.lua` when the JIT is
turned off. All of the performance numbers represent the cost of retrofitting yk
onto the interpreter. The first row is yklua built without any pass sequence,
and consecutive rows with adding prelink, postlink, or both pass sequences to
`-O0`. The last two rows are ykllvm and llvm clang compiled lua with `-O2`
optimisation level.}
\label{table:preandpostlink}
\end{table}

\textbf{Improvements with -O2} \\

The prelink and postlink passes obtained from the genetic algorithm are a subset
of -O2 optimisation pass sequences. Observing Table \ref{table:preandpostlink}
it is curious that applying a subset of -O2 pass sequences results in similar
performance as that of directly compiling with -O2 optimisation level. There is
a possibility that genetic algorithm found all the passes in the sequence which
are sole contributors to the performance. While the other possibility is that yk
based flags interfere with some of non-backend optimisations in -O2. This brings
me to my second hypothesis.

\iti{maybe write the complete word "Hypothesis 2"? H2 does not grab attention. Possibly put it in a box or like a quote or something.}
\textbf{H2} \hspace*{0.5cm} Enabling all optimisation passes, even those
which can interfere with correctness, improves performance further over the
subset found by the GA \iti{rewrite hypothesis}\\

Table \ref{table:preandpostlink} clearly disapproves H2, the intuition behind
this hypothesis was that yklua when using full -O2 should have further performance
improvement over using subset of -O2 passes. Let's dig further why this might
have happened.

Postlink and backend optimisation share same LTO backend code (in
LTOBackend.cpp), which means that when YkAfterIRPasses (a ykllvm pass) applies
optnone attribute to backend optimisation functions, it ends up blocking some of
the postlink optimisations as well.
This results in the interpreter loosing significant performance opportunities. In
the next section I'll show the performance overhead of yk specific passes, and then
proceed to investigate whether it is necessary to block all of the backend
optimisations.

\subsection{Performance impact of yk-based flags with full -O2 optimisation
turned on} \iti{language!}

\textbf{Overhead introduced by yk passes} \\
\label{table:c} shows performance overhead of individual yk passes.  

The objective of the following experiment is to understand what portion of total slowdown
in yklua is contributed by yk-based passes (either individual pass or a
combination of them). I start to turn off yk specific passes individually and
then in combinations, and record the execution times. The speedup of the interpreter when the passes are turned
off indicates that the flag (or combination of them) is contributing a slowdown by
the ratio of: $$\frac{\text{\small (performance of baseline)}}{\text{\small
(performance of row n)}}$$ 

\begin{table}[H]
\begin{tabular}{@{}cllr@{}}
\toprule
\# & \multicolumn{1}{c}{Flags turned off (-O2 passes are set)} & \multicolumn{1}{c}{Execution Time} & \multicolumn{1}{c}{95\% CI} \\ 
\midrule
1 & baseline (all flags are turned on) & 89.9ms & 0.002 \\
2 & —yk-no-fallthrough & 76.1ms & 0.002 \\
3 & —yk-block-disambiguate & 78.7ms & 0.00189 \\
4 & —yk-patch-control-point & 84.4ms & 0.002 \\
5 & —yk-insert-stackmaps & 85.8ms & 0.002 \\
6 & —yk-split-blocks-after-calls & 82ms & 0.002 \\
7 & —yk-no-fallthrough + —yk-block-disambiguate & 68.4ms & 0.002 \\
8 & —yk-no-fallthrough + —yk-block-disambiguate + —yk-patch-control-point & 63.9ms & 0.002 \\
9 & —yk-no-fallthrough + —yk-block-disambiguate + —yk-insert-stackmaps & 58.4ms & 0.002 \\
10 & —yk-no-fallthrough + —yk-block-disambiguate + —yk-split-blocks-after-calls & 66.8ms & 0.00189 \\
11 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-patch-control-point + —yk-insert-stackmaps\end{tabular} & 55.1ms & 0.002 \\
12 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-patch-control-point + —yk-split-blocks-after-calls\end{tabular} & 64.2ms & 0.002 \\
13 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-insert-stackmaps + —yk-split-blocks-after-calls\end{tabular} & 58.2ms & 0.002 \\
14 & \begin{tabular}[c]{@{}l@{}}—yk-no-fallthrough + —yk-block-disambiguate +\\ —yk-patch-control-point + —yk-insert-stackmaps +\\ —yk-split-blocks-after-calls\end{tabular} & 54.5ms & 0.003 \\
\bottomrule
\end{tabular}
\caption{Shows performance we can win back when flags (i.e~passes) or combinations of
them are turned off. Row 1 is performance of baseline lua binary, when no flag
is turned off. Row 14 shows maximum retrieval of performance when the particular
combination of flags in the row are turned off.}
\label{table:c}
\end{table}

From Table \ref{table:c}  yk-based flags contribute upto 1.5 times slowdown in
yklua. As I aim to figure out how much performance can be retrieved back by
optimising some (and possibly all of the expensive passes) in the table shown
above, I'll  be elaborating on how to achieve this in the following text. 

\textbf{Optimising yk-based Passes} \\
\iti{in the next 4 points, you discuss 'possible' solutions. Pitch your ideas with more force?}

\iti{Do you want to make the topic textbfs subsection? Like 7.3.1 so that you can use textbf for these point headers?}
1) yk-insert-stackmaps

Stackmaps describe how the stack looks like i.e~ where the variables are stored
either in the register or the stack slot. During JIT compilation when the gaurd
fails very often (i.e~ when the failure hits threshold set by yk) the
meta-tracer starts de-optimising the trace. De-optimisation is process of
getting back to the control point \iti{a little unclear} to rebuild the trace with new gaurds. During
JIT compilation, the trace builder inlines functions into a trace which results
in the changed structure of stack. With the help of Stackmaps, yk is able to get
back the exact trace that it saw at the beginning of loop.

The `yk-insert-stackmaps` pass is responsible for adding stackmaps to the JIT
IR.

Currently in yk-insert-stackmaps, stackmaps are inserted before every function
unless it is intrinsic. However, stackmaps are not needed before calls to
unmappable functions expect from the control point and yk\_promote,\iti{add what
is control point and yk\_promote in the background section} as these are not traced and thus
never need de-optimisation. Example of unmappable functions are external calls
and functions with no condition(s) inside them. This might claw back a few `ms`
for this pass. 

After adding a fix to make sure that stackmap is not inserted
after unmappable functions, except for control point and
yk\_promote calls, we can see \iti{add performance number} \% improvement over the
baseline (yklua compiled with `-O0`). \footnote{link to the PR
containing the change for --yk-insert-stackmaps and the test case:
https://github.com/ykjit/ykllvm/pull/98}PR, \iti{old performance: of yklua
binary from ${89.9ms}$ to ${86.3ms}$}.

Similarly, another opportunity of macro optimisation is to not add stackmaps for
the functions which do not have conditions in them (i.e~ no gaurds) as they have
no need for de-optimisations. This however is more complex to fix as currently
there is no direct way to figure whether a function has a condition in it or
not. For now I'll leave this fix as a part of future work.

2) yk-split-blocks-after-calls pass

SplitBlocksAfterCalls pass in yk makes sure that function calls terminate by
splitting blocks after every call. This pass is needed so that outlining is
performed effectively. Outlining is reverse of inlining. The tracer (in our case
yk's JITmodbuilder) automatically inlines functions during tracing. This means
that for a function with big loops each iteration of the loop will be inlined in
the trace. This makes the trace huge, resulting in occupying much more memory and
making compilation slower with no additional performance benefits. Thus yk
removes the inlined function with its call instruction.

Currently this pass also splits blocks after calls to unmappable functions (such as
external functions). Another macro optimisation is to refrain from
splitting blocks after calls to unmappable functions. But this could turn out to
be rather fiddly as currently, we depend on splitting to identify when to
start/stop outlining.

3) yk-no-fallthrough pass

The fallthrough optimization in both SelectionDAG and FastISel works by
analyzing the control flow graph (CFG) of the function being compiled. It
identifies patterns where a basic block has a single successor and there is no
intervening code between the basic block and its successor. When such patterns
are found, the optimization eliminates the branch instruction at the end of the
basic block and merges the two blocks into a single block. This allows the
execution to fall through from one block to the next without the need for an
explicit branch.
This becomes problem for the JIT as it distorts the basic block mapping as we
don't have information about which blocks were merged. One solution which
doesn't need us to disable fallthrough optimisation is by encoding extra
information about merged basic blocks. LLVM has a special code which provides
information about how blocks are merged (such as ''merged\_bb\!``). If we can
store the information about how many blocks are merged and transfer this
information to the hardware tracer, then we can tell which blocks were single
basic blocks and which have been merged when mapping back from JIT IR to AOT IR.

textbf{An Optimisation to yk} \\
One of the macro
optimizations I made while benchmarking yklua was directly to yklua's source
code. Earlier yklua would perform reallocation of yk's location
array for every instruction. Yk's location array indicates the point from
which the loop starts and is then sent to the control point \textit{confirm
this}. However, this was done for every instruction instead of
performing reallocation only for those instructions whose bytecode size was
larger than the current location array size. This turned out to be quite
performance-intensive and made some programs (such as `verybig.lua` in lua
benchmark) almost 10x slower. The fix for this was checking and reallocating
the location array only when the resize was 
necessary.\footnote{https://github.com/ykjit/yklua/pull/84}

\textbf{Where to begin: -O0 or -O2?} \\

After adding `noinline` attribute to lto backend we can now easily use `-O2` to
compile yklua. This gives us an opportunity to consider whether we want to start
with `-O0` and add passes to it or start from `-O2` and remove passes from it.
One would expect both should result in same performance when benchmarking with
lua tests. 

\textbf{H3} \hspace*{0.5cm} The performance for lua binary should be the same when adding a list of
passes to -O0 to that of removing passes from -O2. \\

\iti{note to self: update the benchmark similar to that of overview and format
it to look similar to other tables}
\begin{table}[h]
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
Benchmarks & Compiled yklua & Compiled yklua & Compiled yklua & Compiled yklua & Non-yk lua & Non-yk lua \\
& with -O0 & with -O2 & with -O0 and adding passes & with -O2 and removing passes & (compiled with -O0) & (compiled with -O2) \\
\hline
db.lua & 0.120 & 0.068 & 0.125 & 0.069 & 0.061 & 0.028 \\
verybig.lua & 0.597 & 0.335 & 0.666 & 0.342 & 0.261 & 0.141 \\
constructs.lua & 4.693 & 2.506 & 4.614 & 2.645 & 2.753 & 1.072 \\
errors.lua & 5.353 & 3.853 & 5.726 & 3.862 & N/A & 2.919 \\
files.lua & 0.214 & 0.194 & 0.223 & 0.198 & 0.162 & N/A \\
gc.lua & 0.435 & 0.280 & 0.449 & 0.297 & 0.245 & 0.138 \\
math.lua & 0.229 & 0.123 & 0.291 & 0.156 & 0.065 & 0.019 \\
pm.lua & 0.050 & 0.050 & 0.047 & 0.036 & 0.022 & 0.009 \\
\hline
\end{tabular}
}
\caption{The table shows that performance of yklua binary varies when we start
    compilation from -O0 versus that of -O2 and add/remove the same pass sequence.
    Depending upon the optimisation level we start with, clang and llvm perform
    different optimisations as well as expand macros different during IR
    generation. Which means even though the passes we add/remove although have
    same name they don't necessarily perform the same optimisations.}
\end{table}

Although we have verified that compiling with -O2 and removing passes gives us
better performance for the interpreter we still need to make sure that our
oracle passes with -O2(i.e~ all yk and yklua test suite are successful) before making
it the default compilation level. 

But before we proceed to investigate 

\subsection{Understanding yk's test suite}

\subsection{Compiling yklua with backend optimisations turned on}

Now, I'll show my finding for my final hypothesis for this report:

H4: The backend optimisations interfere with JIT IR such that mapping from JIT
IR to LLVM IR is incorrect.

another hypothesis? findings -> backend optimisation only interferes with basic block mappings because of
some of yk's passes.


Consequence of turning on backend optimisation passes on yk's test suite
\iti{discuss with lukas}


\section{Related Work}
\label{section:7}
The optimisation phase ordering problem for compiler optimisations has been solved
before~\cite{guidedGA}. However, the solution for the phase-ordering problem for one
system may not work for another system. This is because the oracle (the
testing conditions and target goal) may vary for different systems.
There exist other meta-tracing systems, such as RPython~\cite{rpython} that
do not face the same problem as yk does. This is because rpython traces a version of
the interpreter compiled in its own format: it actually interprets an
interpreter. With software tracing you can inject code into the AOT-compiled
binary that directly records the IR blocks that were executed, and this entirely
bypasses the need for mapping \iti{incomplete?}

\section{Conclusion and Future Work}
\label{section:8}
In this report, I have shown that with the help of a genetic algorithm, we can
use middle \iti{middle-end?} and possibly backend optimisation passes with the C interpreter,
which not only maintain the correctness of the meta-tracer but simultaneously
improves the interpreter's performance when compiled with meta-tracer
retrofitted into it. As shown in the \label{section:6} I can win back around 1.75x
performance by applying middle-end passes and macro-optimisations to
meta-tracer's passes. Although I am yet to catch problematic backend
optimisation passes, it is not a very far-fetched goal. The next immediate
experiment would be to change LLVM's legacy pass manager so that we can easily
overwrite the backend optimisation pipeline at any optimisation level. After
this change, we can use the genetic algorithm to find a combination of
pass sequences which does not change the AOT IR in non-compatible way.
Alternatively, turn off the part of the optimisation which interferes with the AOT
IR in an unwanted way for the meta-tracer. One way could be to tweak the passes
so that only optimisations within the basic blocks are allowed (and those which
work across the basic blocks are not allowed).

Other future work includes enabling back fallthrough optimisation (i.e.~
retiring yk-no-fallthrough). Currently, yk is disabling some backend
optimisation and compiling without any optimisations. With my changes, we can
move in a direction to make a custom optimisation pass in LLVM for yk, which not
only blocks non-compatible optimisation but can use as much of the higher
optimisation level passes as possible. The result will be 50\%, if not more, a
reduction in the overhead of embedding yk onto the C interpreters.

\section{Timeline}
\noindent
\resizebox{1.1\textwidth}{!}{  % Increase scaling to 1.5
    \begin{tabular}{lp{10cm}}
        \toprule
        \textbf{Year} & \textbf{Goals} \\
        \midrule
        \textbf{2024 June - August} & Adjust LLVM's Legacy Pass Manager to overwrite the backend optimization pipeline. Design a custom pipeline with yk-compatible optimizations. Perform macro-optimization on yk-based passes in ykllvm, reducing their overhead. \\
        \textbf{2024 September - November} & Profile and benchmark yk-retrofitted C interpreters when compiled with a yk-compatible custom optimization level. This may introduce opportunities for further optimizations. \\
        \textbf{2024 December - 2025 February} & Conduct a literature review on meta-tracing systems. Current design uses yk-based passes to embed additional data into the interpreter binary, ensuring the meta-tracer works correctly. Design hypotheses to explore introducing a new optimization pass for the yk meta-tracing system, performing optimizations at JIT compilation time. \\
        \bottomrule
    \end{tabular}
}

\nocite{*}
\bibliographystyle{plain}
\bibliography{local.bib}

\end{document}
