%&main_preamble
\endofdump
\begin{document}

\maketitle

\begin{abstract}
    \noindent
     \laurie{Maybe something like ``The yk system uses meta-tracing to turn
     existing C interpreters into JIT compilers. However, not all compiler
     optimisations are safe to use with yk specifically, or meta-tracing in
     general. yk pessimistically turns off all optimisations, slowing down the
     normal interpreter considerably. I am investigating regaining as much of
     the last performance as possible. This report shows progress to date and
     plans for the future.''}
     \\JIT compilation gives dynamic languages speed of compiled code with
     flexibility of interpretation. One variant of which is tracing JIT, which
     records instructions of most executed path in a trace and use it instead of
     original code in hot paths. But they are quite hard to implement. In
     Meta-tracing, language interpreters are built on top of a common tracing
     JIT compiler, removing the need to re-implement tracing JIT for every
     language. This separates language implementation concerns from JIT
     compilation. The yk meta-tracing system turns existing C interpreters into
     tracing JIT compilers. However, yk pessimistically turns off all
     optimisations, as not all compiler optimisations are safe to use 
     with yk or meta-tracing in general. This significantly slows down
     the normal interpreter. I am investigating regaining as much performance
     as possible for such interpreters. This reports covers progress for above
     problem till date and plans for the future. 


    \laurie{i think you need to try writing the abstract now, because it will
    guide you for the rest of the report. The advice in point 4 of
    https://plg.uwaterloo.ca/~migod/research/beckOOPSLA.html is an excellent
    starting point.}
\end{abstract}

\section{Introduction}

\laurie{this isn't exactly an introduction yet: it's some background, then a
solution. i would start by writing the abstract to help you crystalise what
you're trying to do. then you'll find it much easier to edit the introduction
into the right sort of shape.}

Dynamic languages such as Python and JavaScript are very popular because of
their ease of writing programs and expressiveness. They are used to write small
script as well as large and complex programs such as web-applications. However,
dynamic languages tend to be quite slow to compile. In dynamic languages such as
Python, the type information may vary at runtime, which means compiler cannot
rely on one type to transform operations into machine code and thus will have to
generate all possible combinations. This can be very slow! JIT compilers
overcome this problem by recording and using runtime informations to optimise
most executed paths (also known as "hot" paths). JIT compilation is combination
of two traditional method : ahead of time compilation (AOT) and interpretation.
Tracing JIT is a variation of JIT compilation. In tracing JIT compilation, the
VM identifies frequently executed, ‘hot’ program paths at runtime. Once a hot
path is detected, the VM records all instructions in this path into a trace
until execution loops back to the first instruction of this path. The trace is
then compiled and subsequent iterations of this path execute the compiled trace
instead of the original code. But developing custom tracing JITs is hard
undertaking. To solve this, Bolz et al \iti{add cite} mentions meta-tracing
approach in 2009. With meta-tracing system such as yk language implementers
don't need to implement their own tracing JITs instead they can use yk
meta-tracing system and the tracing JIT compilation will automatically be
embedded in the language implementations. yk traces the execution of the
interpreter instead of tracing the program executed by the interpreter. However,
not all compiler optimisations are safe to be used with meta-tracing systems
like yk. Therefore, yk pessimistically turns off all the optimisations by
default. This results in much slower interpreter than the baseline (i.e~
interpreter compiled without the meta-tracer.) My work focuses on minimising
performance overhead of the yk meta-tracer and thus speeding the C interpreter
to as much extent as possible. In later sections, I'll discuss work done so far
on this and future directions.

Since meta-tracing is retrofitted onto the interpreter and then traces the
interpreter's execution, it involves two interpreters: one that tracing JIT
uses called tracing interpreter and the other that AOT uses to run users
programs called language interpreter. The program that language interpreter
executes is called user program. At first when the program starts everything is
interpreted (i.e~the user program), however when the hot loop is identified the
meta-tracer enters into tracing mode. During tracing, the interpreter records a
history of all the operations it executes. It traces until it has recorded the
execution of one iteration of the hot loop. To decide when this is the case, the
trace is repeatedly checked during tracing as to whether the interpreter is at a
position in the program where it had been earlier. The history recorded by the
tracer is called a trace: it is a sequential list of operations, together with
their actual operands and results. Such a trace can be used to generate
efficient machine code. A trace is sequential path which only follows one of the
many possible directions through the code. To ensure correctness, the trace
contains a guard at every possible point where the path could have followed
another direction, such as at conditions and indirect or virtual calls. When
generating the machine code, every guard is turned into a quick check to
guarantee that the path we are executing is still valid. If a guard fails, the
tracing JIT immediately quit the machine code and continue the execution by
falling back to interpretation (i.e~AOT mode). Further, at JIT time, the tracing
interpreter records the basic blocks (i.e~jumps), when tracing interpreter 
switches back to interpretation (at AOT) it needs to map the information back
at AOT basic block. Meta-tracer such as yk need to do this with hundred percent
accuracy. \iti{(why?)}. However, some llvm optimisations such as branch folding
and block placement passes messes with the mapping. Additionally,
llvm applies some passes at different stages of the pipeline, even after the
final IR has been generated. This again interferes with mapping and creates 
problem for the meta-tracer. To avoid this problem the yk meta-tracing system
turns off all the optimisations by default, which slows down the interpreter 
significantly even without JIT being enabled.

In this report I show that it is possible to  minimize the performance overhead
introduced by meta-tracing in C interpreters, thereby enhancing execution speed
without sacrificing the integrity of the JIT compilation process. The main
contributions of this report are:
\begin{itemize}
    \item{Genetic algorithm to find llvm optimisation passes which doesn't
        break yk's tests and improves performance of yklua, a C interpreter.}
    \item{Identfying and fixing the cause of overhead when using yk to build
        yklua}
\end{itemize}
Through this work, I aim to bridge the gap between the theoretical performance of
meta-tracing JIT compilation and its practical performance for the  C
interpreter.

\section{Background}
\label{sec:background}
Modern compilers have huge number of optimisation passes available. These
optimisation passes target unique segments of program, such as transforming a
function, basic block or whole program. Further, these optimisations can be
applied at different stages of compilation pipelines. The sequence of passes try
to retain the semantic of the program while potentially improving program's
performance. There are are two type of passes: Analysis and Transformation. Both
of these passes work together, the analysis passes collect information about the
program and transformation passes are responsible for using the collected
information and change the program. Example of optimisation level provided by
modern compilers are: O0, O1, O2, O3, and Og. The two major compilers gcc and
clang have 200 and 100+ optimisation passes respectively, their sequence is
called as optimisation sequence. By default without optimisations level all of
the passes are turned off, and an expert can choose to turn on and off passes
accordingly.\iti{add reference}

As mentioned earlier, meta-tracing systems such as yk cannot enable all
the optimisation passes by default as it messes with basic-blocks mapping
when switching from JIT mode to AOT mode. \iti{add how mapping is done in JIT
mode?} Therefore, we need to carefully choose the optimisation pass sequence
which doesn't interferes with basic block mapping while also improving
the interpreters performance. Such problem is called optimising phase ordering problem.
Optimising phase ordering is a long standing problem with tons of work done on
it. However, the optimisation pass sequence which might work for one system may
not perform well or even work at all for the other. Finding right optimisation
pass sequence depends on the algorithm and oracle used. Algorithm can be
recursive or stochastic one while oracle means a test set which sets initial
constraint. Unless the pass sequence satisfied all the constraint it won't be
selected in the algorithm. The oracle can contain constraints such as tests as 
well as execution time of the program.

Since some optimisation passes depends on others to enable program's
transformation, choosing a right sequence (and order of passes) is important for
program's performance either in respect to execution time or code size.
\laurie{whoah, this is a big jump! you've gone from generic background straight
into your solution, without ever defining what problem you're trying to solve}
Genetic algorithm is used to solve this problem in this report, since the number
of passes are a lot (around 90) this makes the our search space huge (i.e~if we
try all sequence irrespective of ordering we would have to try $2^{90} - 1$
possible combinations!). Using genetic algorithm helps us to find an optimal
pass sequence for our oracle in acceptable time. Genetic algorithm is well-known
stochastic algorithm which is adopted on the basis of theory of evolution. This
algorithm has been used by several researchers to solve phase-ordering
problemi\iti{add citation}.

This report focuses on solving phase-ordering problem using genetic algorithm
for yk and yklua \laurie{yk and yklua are lower-cased}. yk is a meta-tracing
system and yklua is yk-enabled lua interpreter. First, I started with a
recursive based approach to find pass sequence which doesn't violate constraints
(where constraint is passing all of yk tests and building lua interpreter to run
it's tests successfully.) Later, in the report I moved to using genetic
algorithm due to it's nature to escape local optima very efficiently. \iti{add
citation} In this report I also address the modifications I had to make to make
our oracle suitable for the genetic algorithm. 

In later sections I show that we can use a parallelised genetic algorithm
for yk and yk-based interpreter to optimise  yklua by 1.5x-2x times 
from baseline (where baseline is pipeline with non O2 passes) and identify
what causes overhead and limit us to achieve theoretical performance for
the interpreter. 


\subsection*{Methodology}
Constraint Satisfaction Problems are mathematical problem with a finite set of
variable, each of which has a finite domain (i.e~set of possible values).
Since the variables can be related, this puts a limitation on what values
these variables can be instantiated with. To solve this problem, we have to find
possible values for these variables which lies within the domain and does
not violates any constraints.

In this report, we would be discussing about Constraint Satisfaction
Optimisation Problem (CSOP), which is a NP-hard problem\ref{add reference}. CSOP
is a variation of Constraint Satisfaction Problem which aims to find most
optimal solution (i.e~global optima) in the search space. One way to solve this
problem is systematic search algorithm, in this method we try all possible
combinations of variables for the oracle set, the variables can have any value
possible from their domains, the algorithm then exhausts the search space (if
necessary) to find a combination which satisfies our constraint set for the
oracle(s). The problem with this search is that since the number of combination
can be exponentially large, even though it might be finite to make sure the
solution found is optimal or near-optimal the algorithm will need to exhaust the
entire problem (or search) space, this might  take years.

Another way to solve CSOP is using stochastic search method, stochastic
search method involves randomising the search algorithm. Sometimes search
based problems contains multiple local optima, and deterministic or systematic
algorithm can get stuck in local optima because of combinatorial explosion
problem. 

The objective of this report is to solve constraint optimisation problem
for YK and YKLUA using stochastic search algorithm.

\begin{itemize}
    \item 
\end{itemize}

CSOP \laurie{?}
A finite (i.e~with a countable combination) constraint satisfaction problem can
be explained as a problem with a finite set of variable, in which each variable
can have values from the domain set (which is possible values or state for that
variable). Since variables can be related to each other, these variables can 
only be instantiated with those values from their domain which doesn't violate
any constraint from the constraint set.

For easier understanding, CSOP has three variable: $X$, $D$ and $C$:
\begin{itemize}
    \item $X$ is a set of variables, ${X_1, X_2, X_3, ..., X_n}$ 
    \item $D$ is a set of domain for each variable, ${D_1, D_2, ..., D_n}$
    \item $C$ is a set of constraints that dictates allowable combinations.
\end{itemize}


backtracking or heuristic search
local search
genetic algorithm

\subsection{The LLVM Project}
As Wikipedia describes, LLVM is a set of compiler and toolchain technologies.
LLVM very modular and reusable project which has its owns Intermediate
Representation (LLVM IR). This language-independent IR helps LLVM 
compilation to be target and source independent. The LLVM IR is
optimised through sequence of LLVM passes. Passes are operational
unit of the IR which can:
    \begin{itemize}
        \item{mutate the IR}
        \item{performs some computation about the IR}
        \item{or just print something}
    \end{itemize}

A unit of an IR is scope of the pass, it's what the pass is going 
to operate on. This can be :
\begin{itemize}
    \item{a module: LLVM module is a top-level structure that represents a
        single  unit of code which is processed together. It includes global
        variable, function declaration, and implementations.}
    \item{a function: LLVM function is a self-contained unit of execution within
          a module, it corresponds to function in the source code and contains
          a list of aruguments, a basic block, and a symbol table.}
    \item{a basic block: LLVM basic block is a linear sequence of instructions
          within a function, this has a single entry and exit point. They are
          used to represent straight-line code sequence that makes up function's
          body.}
    \item{an instruction: an LLVM instruction is the smallest unit of execution
         in the LLVM IR, representing a single operation within a basic block.
         Instructions can perform a wide range of operations, including arithmetic,
         memory access, logical operations, and control flow changes. Each instruction
         produces a result and can have zero or more operands, which are either constants
         or the results of other instructions.}
\end{itemize}
These passes are categorised into analysis, transformation and utility passes.
Depending on whether they perform computation on the IR, mutate the IR,
or do some other non-categorised task.

\subsection{LLVM's optimisation pipeline}
Classically, compilers are split into three parts, which is:
frontend, middle-end and the back end. The front end is responsible
for dealing with programming-language specific analysis, this can be
operations such as parsing, type checking and so. The middle-end is where
optimisations are performed. These optimisations are target independent \iti{add
how}. Next, the backend is responsible for applying target-specific optimisation
and emiting the machine code.

Different languages like C, C++, and Rust (and many more) have their own
frontends but rely on LLVM for middle and backends.

\subsubsection{Type of optimisation pipelines}
For this report we are mostly concerned with LLVM's middle-end optimisation
pipeline. LLVM has three kind of optimisation pipeline: default (non-LTO),
ThinLTO, and FatLTO pipeline.

\subsubsection{Default Pipeline}
\subsubsection{ThinLTO Pipeline}
QUESTION: what is prelink and postlink? does every optimisation level have
prelink and postlink? (answer yes, O2/O3 is made up of prelink and postlink
union flags)

\subsubsection{FatLTO Pipeline}
\iti  {stages diagram : https://www.npopov.com/2023/04/07/LLVM-middle-end-pipeline.html}.

Question: how does passes look? (from pass slides)
Question: how to fetch prelink and postlink passes for particular level?
example using opt
Yk

Ykllvm

Yklua

\subsection*{Optimisation passes sequence}
As we discussed types of pipeline, the next question is how and what passes are
decided to be sent in prelink and postlink time? how do we know what works 
for C++ and rust? (https://www.npopov.com/2023/04/07/LLVM-middle-end-pipeline.html)

Why phase-ordering is important? Why can't we just schedule extra pass runs? 
Question: why splitting the argument was important?
Question:

Add a section about oracle, describe the oracle.

Question: why can't we use custom pipeline on our oracle?
why yk or yklua oracle tests fail for some pass sequence?

\section{Experimental Setup}

\section{Experiments}

Hypothesis I: Maintaining the order of passes are not important for finding
successful pass sequence.

Parallising the program

write about binary split algorithm

Hypothesis II: Introducing randomisation can help us escape local optima (from the GA
thesis)

parallesing the algorithm for speed-up and faster testing. 
Speed up from parallesing

Hypothesis III: Deciding upon oracle

Hypothesis IV: aiming for performance

obstacles: aiming for correctness (yk's tests), why we disabled some tests

Obstacles: wall created by ykllvm and yk-config

Hypothesis V: ykllvm's clang is slower than llvm clang because ykllvm introduce
high penality passes.

why these passes are high penality?

is there any way to solve it?

exaplain yk-config in methodology section \iti{IMPORTANT!}

 % Dynamic languages are more expensive to compile than statically
 % typed languages. Since, traditional compilers don't have runtime
 % informations, they need to emit code that can handle 
 % all type of combinations that are possible at the runtime. This
 % makes performance of dynamic language much slower. JIT compilers
 % overcome this problem by recording and using runtime informations 
 % to optimise most executed paths. JIT compilation is 
 % combination of two traditional method : ahead of time compilation
 % (AOT) and interpretation. So
 % However, retrofitting meta-tracing system such as yk into
 % existing C-interpreters brings challenges to their performance. The aim of
 % my PhD is find ways to speed up retrofitting meta-tracing system (i.e~yk)
 % into existing interpreters, In this report, I first find optimisation pass
 % sequence (from tradition passes such as O2) using recursive and stochastic
 % method, the end result, when JIT is turned of is performance improvement by
 % 1.5-2 times for yklua, a C-based interpreter which uses yk. Next, I dig
 % deeper into reasons for runtime performance overhead introduce by yk in
 % C-based interpreters. 

% \bibliographystyle{plain}
% \bibliography{bib}

\end{document}
